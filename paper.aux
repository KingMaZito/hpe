\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{summary80s}
\citation{mocap}
\citation{vip}
\citation{mocap}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Benchmark Datasets}{1}{section.2}\protected@file@percent }
\newlabel{sec:benchmarks}{{II}{1}{Benchmark Datasets}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Max Planck Institute datasets}{1}{subsection.2.1}\protected@file@percent }
\citation{cape}
\citation{resynth}
\citation{vip}
\citation{dip}
\citation{prox}
\citation{grab}
\citation{tuch}
\citation{mano}
\citation{obman}
\citation{surreal}
\citation{ltsh}
\citation{amass}
\citation{coco}
\citation{caesar}
\citation{pcp}
\citation{pdj}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}1}Clothing}{2}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}2}Full-body}{2}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}3}Hand scans}{2}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}4}synthetic data}{2}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}5}Generalization of datasets}{2}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}COCO dataset}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Caesar dataset}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Benchmark metrics}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Percentage of Correct Parts PCP}{2}{subsection.3.1}\protected@file@percent }
\newlabel{sec:pcp}{{\mbox  {III-A}}{2}{Percentage of Correct Parts PCP}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Abstract visualisation of the values required for the PCP calculation. A solid line between two endpoints represents a bone in the ground truth of a dataset in green and the detected bone in black. The dashed lines show the distances $l_{1}, l_{2}$, which are evaluated with the true length $L$ of the bone.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:pcp}{{1}{2}{Abstract visualisation of the values required for the PCP calculation. A solid line between two endpoints represents a bone in the ground truth of a dataset in green and the detected bone in black. The dashed lines show the distances $\protect l_{1}, l_{2}$, which are evaluated with the true length $\protect L$ of the bone}{figure.1}{}}
\citation{pck}
\citation{mpii}
\citation{Zheng2020}
\citation{gower1975generalized}
\citation{Rhodin2018}
\citation{smpl}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Percentage of Detected Joints PDJ}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:pdj}{{\mbox  {III-B}}{3}{Percentage of Detected Joints PDJ}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Percentage of Correct Key-points PCK}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Head-normalized Probability of Correct Key-points PCKh}{3}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-E}}Object Key-point Similarity OKS}{3}{subsection.3.5}\protected@file@percent }
\newlabel{eq:OKS}{{1}{3}{Object Key-point Similarity OKS}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-F}}Mean Per Joint Position Error (MPJPE)}{3}{subsection.3.6}\protected@file@percent }
\newlabel{criteria:mpjpe}{{\mbox  {III-F}}{3}{Mean Per Joint Position Error (MPJPE)}{subsection.3.6}{}}
\newlabel{eq:MPJPE}{{2}{3}{Mean Per Joint Position Error (MPJPE)}{equation.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}models in human estimation}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Skinned Multi-Person Linear model SMPL}{3}{subsection.4.1}\protected@file@percent }
\newlabel{sec:SMPL}{{\mbox  {IV-A}}{3}{Skinned Multi-Person Linear model SMPL}{subsection.4.1}{}}
\newlabel{eq:model}{{3}{3}{Skinned Multi-Person Linear model SMPL}{equation.4.3}{}}
\citation{zuffi_pictorial_2012}
\citation{rafi_efficient_2016}
\citation{sun_deep_2019}
\citation{cheng_higherhrnet_2020}
\newlabel{eq:smpl}{{4}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.4}{}}
\newlabel{eq:bs}{{5}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.5}{}}
\newlabel{eq:Tf}{{6}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.6}{}}
\newlabel{eq:lbs}{{7}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.7}{}}
\newlabel{eq:lbs2}{{8}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}2D pose estimation}{4}{section.5}\protected@file@percent }
\citation{ren_faster_2016,micilotta_real-time_2006}
\citation{carreira_human_2016,fan_combining_2015,fieraru_learning_2018,li_heterogeneous_2014,qiu_peeking_2020,sun_compositional_2017,sun_integral_2018,toshev_deeppose_2014,wang_graph-pcnn_2020,z}
\citation{li_pose_2021}
\citation{toshev_deeppose_2014}
\citation{krizhevsky_imagenet_2012}
\citation{tompson_joint_2014,feng_ning_toward_2005}
\citation{li_markov_1994}
\citation{carreira_human_2016}
\citation{szegedy_going_2014}
\citation{vaswani_attention_2017}
\citation{li_pose_2021}
\citation{carion_end--end_2020}
\citation{chen_articulated_2014,newell_stacked_2016,wei_convolutional_2016}
\citation{tompson_efficient_2015,tompson_joint_2014}
\citation{artacho_unipose_2020,bulat_human_2016,gkioxari_chained_2016,lifshitz_human_2016,newell_stacked_2016,tompson_efficient_2015}
\citation{carreira_human_2016,luo_lstm_2018,toshev_deeppose_2014,wei_convolutional_2016}
\citation{ramakrishna_pose_2014}
\citation{wei_convolutional_2016}
\citation{he_deep_2016}
\citation{wei_convolutional_2016}
\citation{cai_learning_2020,chen_cascaded_2018,chu_multi-context_2017,ke_multi-scale_2018,liu_cascaded_2018,newell_stacked_2016,su_multi-person_2019,sun_deep_2019,xiao_simple_2018-1,yang_learning_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Top-Down Framework}{5}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}1}Regression-based methods}{5}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}2}Heatmap-based Methods}{5}{subsubsection.5.1.2}\protected@file@percent }
\citation{newell_stacked_2016}
\citation{chu_multi-context_2017}
\citation{yang_learning_2017}
\citation{cai_learning_2020}
\citation{liu_cascaded_2018,eichner_human_2012,felzenszwalb_pictorial_2005,girdhar_detect-and-track_2018}
\citation{pfister_flowing_2015}
\citation{yixing_gao_user_2015}
\citation{micilotta_real-time_2006,cheng_higherhrnet_2020,fieraru_learning_2018,jin_differentiable_2020,kreiss_pifpaf_2019,nie_single-stage_2019,insafutdinov_arttrack_2017,insafutdinov_deepercut_2016,newell_associative_2017,pishchulin_deepcut_2016,tian_directpose_2019}
\citation{geng_bottom-up_2021,nie_single-stage_2019,nie_human_2018}
\citation{jin_differentiable_2020,kreiss_pifpaf_2019,martinez_single-network_2019,insafutdinov_deepercut_2016,li_simple_2019}
\citation{cheng_higherhrnet_2020,jin_multi-person_2019,luo_rethinking_2021,newell_associative_2017}
\citation{ferrari_pose_2018}
\citation{newell_stacked_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Bottom-Up Framework}{6}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}1}Human Center Regression}{6}{subsubsection.5.2.1}\protected@file@percent }
\citation{nie_single-stage_2019}
\citation{geng_bottom-up_2021}
\citation{cao_realtime_2017}
\citation{wei_convolutional_2016}
\citation{zhu_multi-person_nodate}
\citation{kreiss_pifpaf_2019}
\citation{cao_realtime_2017}
\citation{newell_stacked_2016}
\citation{newell_associative_2017}
\citation{cheng_higherhrnet_2020}
\citation{jin_multi-person_2019}
\citation{jin_differentiable_2020}
\citation{luo_rethinking_2021}
\citation{toshev_deeppose_2014}
\citation{newell_stacked_2016}
\citation{cao_realtime_2017}
\citation{chen_cascaded_2018}
\citation{cao_realtime_2017}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}2}Part Field}{7}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}3}Associate Embedding}{7}{subsubsection.5.2.3}\protected@file@percent }
\citation{sumi}
\citation{summary80s}
\citation{wang_deep_2021,Chen2016,Chen_2017_CVPR,Tome_2017_CVPR,Andrikula2010,Ye2011,Martinez_2017_ICCV}
\citation{Chen2016}
\citation{Rogez2016}
\citation{Tome_2017_CVPR,Martinez_2017_ICCV}
\citation{Chen_2017_CVPR}
\citation{Dong_2019_CVPR}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{park_3d_2016,zhou_hemlets_2019,habibie_wild_2019,tekin_learning_2017}
\citation{wang_deep_2021}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Newell2016}
\citation{srivastava2014}
\citation{HeZRS15}
\citation{H3.6M}
\citation{PavlakosZDD16}
\citation{andriluka14cvpr}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}2D Human Pose Estimation Summary}{8}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}3D pose estimation}{8}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-A}}Lifting from 2D to 3D pose}{8}{subsection.6.1}\protected@file@percent }
\newlabel{sec:simple-yet-effective-baseline}{{\mbox  {VI-A}1}{8}{A simple yet effective baseline for 3d human pose estimation}{subsubsection.6.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}1}A simple yet effective baseline for 3d human pose estimation}{8}{subsubsection.6.1.1}\protected@file@percent }
\citation{Deng2021}
\citation{Deng2021}
\citation{Deng2021}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neural network structure from \cite  {Martinez_2017_ICCV}. It consists of two inner blocks containing a linear layer followed by batch normalization and dropout. A residual connection is added from the input of the first inner block to the output of the second. This structure is then repeated another time. The networks inputs are 2d joint positions, which it outputs uplifted to three dimensions.}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:martinez-network}{{2}{9}{Neural network structure from \cite {Martinez_2017_ICCV}. It consists of two inner blocks containing a linear layer followed by batch normalization and dropout. A residual connection is added from the input of the first inner block to the output of the second. This structure is then repeated another time. The networks inputs are 2d joint positions, which it outputs uplifted to three dimensions}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Test cases from \cite  {Martinez_2017_ICCV}. The picture on the left is the original overlayed with the 2d pose estimation (shown in the middle), while the respective uplifted 3d pose is shown on the right. Most test cases yield good results, but there are some failures too as shown in the bottom row.}}{9}{figure.3}\protected@file@percent }
\newlabel{fig:martinez-test-cases}{{3}{9}{Test cases from \cite {Martinez_2017_ICCV}. The picture on the left is the original overlayed with the 2d pose estimation (shown in the middle), while the respective uplifted 3d pose is shown on the right. Most test cases yield good results, but there are some failures too as shown in the bottom row}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}2}SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistency}{9}{subsubsection.6.1.2}\protected@file@percent }
\citation{wang_deep_2021}
\citation{Luvizon2018}
\citation{H3.6M}
\citation{andriluka14cvpr}
\citation{Martinez_2017_ICCV}
\citation{Tekin2016}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces GAN structure from \cite  {Deng2021}. The input 2d pose gets uplifted to a 3d pose estimation and an estimated camera position by the generator. This allows rotation of the model to feed the discriminator with a reprojection from various angles, who is comparing them with real 2d poses. The reprojections are also used to uplift and reproject them again to ensure Single-view-multi-angle Consistency.}}{10}{figure.4}\protected@file@percent }
\newlabel{fig:deng-structure}{{4}{10}{GAN structure from \cite {Deng2021}. The input 2d pose gets uplifted to a 3d pose estimation and an estimated camera position by the generator. This allows rotation of the model to feed the discriminator with a reprojection from various angles, who is comparing them with real 2d poses. The reprojections are also used to uplift and reproject them again to ensure Single-view-multi-angle Consistency}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-B}}3d pose directly from 2d image}{10}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}1}2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning}{10}{subsubsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}2}Structured Prediction of 3D Human Pose with Deep Neural Networks}{10}{subsubsection.6.2.2}\protected@file@percent }
\citation{Ye2011}
\citation{Haralick98}
\citation{spin}
\citation{simplify}
\citation{frankmocap}
\citation{frankmocap}
\citation{frankmocap}
\citation{hmr}
\citation{smplx}
\citation{spin}
\citation{spin}
\citation{spin}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-C}}Direct 3d pose estimation}{11}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-D}}use of models in pose estimation}{11}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-D}1}hand estimation}{11}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Encoder-decoder architecture that takes a hand image as input, extracts the features and decodes them as sufficient parameters to construct a hand model.\cite  {frankmocap}}}{11}{figure.5}\protected@file@percent }
\newlabel{fig:frankmocap_hand}{{5}{11}{Encoder-decoder architecture that takes a hand image as input, extracts the features and decodes them as sufficient parameters to construct a hand model.\cite {frankmocap}}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Visualisation of the $SPIN$ loop, in which first the pose and shape parameters on the right side were regressed to initialize the $SMPLify$ algorithm on the left side. The optimized parameters from $SMPLify$ were used to improve the supervision of the regressor modul, resulting into better initial models for $SMPLify$. \cite  {spin}}}{11}{figure.6}\protected@file@percent }
\newlabel{fig:spin}{{6}{11}{Visualisation of the $\protect SPIN$ loop, in which first the pose and shape parameters on the right side were regressed to initialize the $\protect SMPLify$ algorithm on the left side. The optimized parameters from $\protect SMPLify$ were used to improve the supervision of the regressor modul, resulting into better initial models for $\protect SMPLify$. \cite {spin}}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-D}2}full-body pose estimation}{11}{figure.6}\protected@file@percent }
\citation{flame}
\citation{ringnet}
\citation{ringnet}
\citation{ringnet}
\citation{Martinez_2017_ICCV}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-D}3}face estimation}{12}{subsubsection.6.4.3}\protected@file@percent }
\newlabel{eq:flame}{{9}{12}{face estimation}{equation.6.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}conclusion}{12}{section.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Ring-shaped network architecture of the $RingNet$ in training with 4 elements that are encoder-decoder structures. Training is performed with image sources consisting of 2 subjects, with only one image coming from the second subject. Ring instances regress 3d information from 2d images, which are projected into 2d space for evaluation in the loss function. This concept ensures the learning of consistency.\cite  {ringnet}}}{12}{figure.7}\protected@file@percent }
\newlabel{fig:ringnet}{{7}{12}{Ring-shaped network architecture of the $\protect RingNet$ in training with 4 elements that are encoder-decoder structures. Training is performed with image sources consisting of 2 subjects, with only one image coming from the second subject. Ring instances regress 3d information from 2d images, which are projected into 2d space for evaluation in the loss function. This concept ensures the learning of consistency.\cite {ringnet}}{figure.7}{}}
\bibstyle{plain}
\bibdata{references/benchmarks,references/criteria,references/2d_estimation,references/3d_estimation}
\bibcite{mpii}{1}
\bibcite{andriluka14cvpr}{2}
\bibcite{Andrikula2010}{3}
\bibcite{artacho_unipose_2020}{4}
\bibcite{simplify}{5}
\bibcite{bulat_human_2016}{6}
\bibcite{cai_learning_2020}{7}
\bibcite{cao_realtime_2017}{8}
\bibcite{carion_end--end_2020}{9}
\bibcite{carreira_human_2016}{10}
\bibcite{Chen_2017_CVPR}{11}
\bibcite{Chen2016}{12}
\bibcite{chen_articulated_2014}{13}
\bibcite{chen_cascaded_2018}{14}
\bibcite{cheng_higherhrnet_2020}{15}
\bibcite{chu_multi-context_2017}{16}
\bibcite{Deng2021}{17}
\bibcite{Dong_2019_CVPR}{18}
\bibcite{eichner_human_2012}{19}
\bibcite{pcp}{20}
\bibcite{fan_combining_2015}{21}
\bibcite{felzenszwalb_pictorial_2005}{22}
\bibcite{feng_ning_toward_2005}{23}
\bibcite{fieraru_learning_2018}{24}
\bibcite{geng_bottom-up_2021}{25}
\bibcite{girdhar_detect-and-track_2018}{26}
\bibcite{gkioxari_chained_2016}{27}
\bibcite{gower1975generalized}{28}
\bibcite{habibie_wild_2019}{29}
\@writefile{toc}{\contentsline {section}{References}{13}{section*.4}\protected@file@percent }
\bibcite{Haralick98}{30}
\bibcite{prox}{31}
\bibcite{obman}{32}
\bibcite{HeZRS15}{33}
\bibcite{he_deep_2016}{34}
\bibcite{ltsh}{35}
\bibcite{dip}{36}
\bibcite{insafutdinov_arttrack_2017}{37}
\bibcite{insafutdinov_deepercut_2016}{38}
\bibcite{H3.6M}{39}
\bibcite{jin_multi-person_2019}{40}
\bibcite{jin_differentiable_2020}{41}
\bibcite{hmr}{42}
\bibcite{ke_multi-scale_2018}{43}
\bibcite{spin}{44}
\bibcite{kreiss_pifpaf_2019}{45}
\bibcite{krizhevsky_imagenet_2012}{46}
\bibcite{li_simple_2019}{47}
\bibcite{li_pose_2021}{48}
\bibcite{li_markov_1994}{49}
\bibcite{li_heterogeneous_2014}{50}
\bibcite{flame}{51}
\bibcite{lifshitz_human_2016}{52}
\bibcite{coco}{53}
\bibcite{liu_cascaded_2018}{54}
\bibcite{smpl}{55}
\bibcite{luo_lstm_2018}{56}
\bibcite{luo_rethinking_2021}{57}
\bibcite{Luvizon2018}{58}
\bibcite{cape}{59}
\bibcite{resynth}{60}
\bibcite{amass}{61}
\bibcite{martinez_single-network_2019}{62}
\bibcite{Martinez_2017_ICCV}{63}
\bibcite{micilotta_real-time_2006}{64}
\bibcite{mocap}{65}
\bibcite{summary80s}{66}
\bibcite{tuch}{67}
\bibcite{newell_associative_2017}{68}
\bibcite{newell_stacked_2016}{69}
\bibcite{Newell2016}{70}
\bibcite{ferrari_pose_2018}{71}
\bibcite{nie_single-stage_2019}{72}
\bibcite{nie_human_2018}{73}
\bibcite{park_3d_2016}{74}
\bibcite{smplx}{75}
\bibcite{PavlakosZDD16}{76}
\bibcite{pfister_flowing_2015}{77}
\bibcite{pishchulin_deepcut_2016}{78}
\bibcite{qiu_peeking_2020}{79}
\bibcite{rafi_efficient_2016}{80}
\bibcite{ramakrishna_pose_2014}{81}
\bibcite{ren_faster_2016}{82}
\bibcite{Rhodin2018}{83}
\bibcite{caesar}{84}
\bibcite{Rogez2016}{85}
\bibcite{mano}{86}
\bibcite{frankmocap}{87}
\bibcite{ringnet}{88}
\bibcite{srivastava2014}{89}
\bibcite{su_multi-person_2019}{90}
\bibcite{sumi}{91}
\bibcite{sun_deep_2019}{92}
\bibcite{sun_compositional_2017}{93}
\bibcite{sun_integral_2018}{94}
\bibcite{szegedy_going_2014}{95}
\bibcite{grab}{96}
\bibcite{Tekin2016}{97}
\bibcite{tekin_learning_2017}{98}
\bibcite{tian_directpose_2019}{99}
\bibcite{Tome_2017_CVPR}{100}
\bibcite{tompson_efficient_2015}{101}
\bibcite{tompson_joint_2014}{102}
\bibcite{pdj}{103}
\bibcite{toshev_deeppose_2014}{104}
\bibcite{surreal}{105}
\bibcite{vaswani_attention_2017}{106}
\bibcite{vip}{107}
\bibcite{wang_graph-pcnn_2020}{108}
\bibcite{wang_deep_2021}{109}
\bibcite{wei_convolutional_2016}{110}
\bibcite{yang_learning_2017}{111}
\bibcite{pck}{112}
\bibcite{Ye2011}{113}
\bibcite{yixing_gao_user_2015}{114}
\bibcite{Zheng2020}{115}
\bibcite{zhou_hemlets_2019}{116}
\bibcite{zhu_multi-person_nodate}{117}
\bibcite{zuffi_pictorial_2012}{118}
