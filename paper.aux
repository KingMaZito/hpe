\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{summary80s}
\citation{mocap}
\citation{vip}
\citation{mocap}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Benchmark Datasets}{1}{section.2}\protected@file@percent }
\newlabel{sec:benchmarks}{{II}{1}{Benchmark Datasets}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Max Planck Institute datasets}{1}{subsection.2.1}\protected@file@percent }
\citation{cape}
\citation{resynth}
\citation{vip}
\citation{dip}
\citation{prox}
\citation{grab}
\citation{tuch}
\citation{mano}
\citation{obman}
\citation{surreal}
\citation{ltsh}
\citation{amass}
\citation{coco}
\citation{caesar}
\citation{pcp}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}1}Clothing}{2}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}2}Full-body}{2}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}3}Hand scans}{2}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}4}synthetic data}{2}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}5}Generalization of datasets}{2}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}COCO dataset}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}CAESAR dataset}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Benchmark metrics}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Percentage of Correct Parts PCP}{2}{subsection.3.1}\protected@file@percent }
\newlabel{sec:pcp}{{\mbox  {III-A}}{2}{Percentage of Correct Parts PCP}{subsection.3.1}{}}
\citation{pdj}
\citation{pck}
\citation{mpii}
\citation{Zheng2020}
\citation{gower1975generalized}
\citation{Rhodin2018}
\citation{smpl}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Abstract visualisation of the values required for the PCP calculation. A solid line between two endpoints represents a bone in the ground truth of a dataset in green and the detected bone in black. The dashed lines show the distances $l_{1}, l_{2}$, which are evaluated with the true length $L$ of the bone.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:pcp}{{1}{3}{Abstract visualisation of the values required for the PCP calculation. A solid line between two endpoints represents a bone in the ground truth of a dataset in green and the detected bone in black. The dashed lines show the distances $\protect l_{1}, l_{2}$, which are evaluated with the true length $\protect L$ of the bone}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Percentage of Detected Joints PDJ}{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:pdj}{{\mbox  {III-B}}{3}{Percentage of Detected Joints PDJ}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Percentage of Correct Key-points PCK}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Head-normalized Probability of Correct Key-points PCKh}{3}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-E}}Object Key-point Similarity OKS}{3}{subsection.3.5}\protected@file@percent }
\newlabel{eq:OKS}{{1}{3}{Object Key-point Similarity OKS}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-F}}Mean Per Joint Position Error (MPJPE)}{3}{subsection.3.6}\protected@file@percent }
\newlabel{criteria:mpjpe}{{\mbox  {III-F}}{3}{Mean Per Joint Position Error (MPJPE)}{subsection.3.6}{}}
\newlabel{eq:MPJPE}{{2}{3}{Mean Per Joint Position Error (MPJPE)}{equation.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}models in human estimation}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Skinned Multi-Person Linear model SMPL}{3}{subsection.4.1}\protected@file@percent }
\newlabel{sec:SMPL}{{\mbox  {IV-A}}{3}{Skinned Multi-Person Linear model SMPL}{subsection.4.1}{}}
\newlabel{eq:model}{{3}{3}{Skinned Multi-Person Linear model SMPL}{equation.4.3}{}}
\citation{zuffi_pictorial_2012}
\citation{rafi_efficient_2016}
\newlabel{eq:smpl}{{4}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.4}{}}
\newlabel{eq:bs}{{5}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.5}{}}
\newlabel{eq:Tf}{{6}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.6}{}}
\newlabel{eq:lbs}{{7}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.7}{}}
\newlabel{eq:lbs2}{{8}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}2D pose estimation}{4}{section.5}\protected@file@percent }
\citation{sun_deep_2019}
\citation{cheng_higherhrnet_2020}
\citation{ren_faster_2016,micilotta_real-time_2006}
\citation{carreira_human_2016,fan_combining_2015,fieraru_learning_2018,li_heterogeneous_2014,qiu_peeking_2020,sun_compositional_2017,sun_integral_2018,toshev_deeppose_2014,wang_graph-pcnn_2020,z}
\citation{li_pose_2021}
\citation{toshev_deeppose_2014}
\citation{krizhevsky_imagenet_2012}
\citation{tompson_joint_2014,feng_ning_toward_2005}
\citation{li_markov_1994}
\citation{carreira_human_2016}
\citation{szegedy_going_2014}
\citation{vaswani_attention_2017}
\citation{li_pose_2021}
\citation{carion_end--end_2020}
\citation{chen_articulated_2014,newell_stacked_2016,wei_convolutional_2016}
\citation{tompson_efficient_2015,tompson_joint_2014}
\citation{artacho_unipose_2020,bulat_human_2016,gkioxari_chained_2016,lifshitz_human_2016,newell_stacked_2016,tompson_efficient_2015}
\citation{carreira_human_2016,luo_lstm_2018,toshev_deeppose_2014,wei_convolutional_2016}
\citation{ramakrishna_pose_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Top-Down Framework}{5}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}1}Regression-based methods}{5}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}2}Heatmap-based Methods}{5}{subsubsection.5.1.2}\protected@file@percent }
\citation{wei_convolutional_2016}
\citation{he_deep_2016}
\citation{wei_convolutional_2016}
\citation{cai_learning_2020,chen_cascaded_2018,chu_multi-context_2017,ke_multi-scale_2018,liu_cascaded_2018,newell_stacked_2016,su_multi-person_2019,sun_deep_2019,xiao_simple_2018-1,yang_learning_2017}
\citation{newell_stacked_2016}
\citation{chu_multi-context_2017}
\citation{yang_learning_2017}
\citation{cai_learning_2020}
\citation{liu_cascaded_2018,eichner_human_2012,felzenszwalb_pictorial_2005,girdhar_detect-and-track_2018}
\citation{pfister_flowing_2015}
\citation{yixing_gao_user_2015}
\citation{micilotta_real-time_2006,cheng_higherhrnet_2020,fieraru_learning_2018,jin_differentiable_2020,kreiss_pifpaf_2019,nie_single-stage_2019,insafutdinov_arttrack_2017,insafutdinov_deepercut_2016,newell_associative_2017,pishchulin_deepcut_2016,tian_directpose_2019}
\citation{geng_bottom-up_2021,nie_single-stage_2019,nie_human_2018}
\citation{jin_differentiable_2020,kreiss_pifpaf_2019,martinez_single-network_2019,insafutdinov_deepercut_2016,li_simple_2019}
\citation{cheng_higherhrnet_2020,jin_multi-person_2019,luo_rethinking_2021,newell_associative_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Bottom-Up Framework}{6}{subsection.5.2}\protected@file@percent }
\citation{ferrari_pose_2018}
\citation{newell_stacked_2016}
\citation{nie_single-stage_2019}
\citation{geng_bottom-up_2021}
\citation{cao_realtime_2017}
\citation{wei_convolutional_2016}
\citation{zhu_multi-person_nodate}
\citation{kreiss_pifpaf_2019}
\citation{cao_realtime_2017}
\citation{newell_stacked_2016}
\citation{newell_associative_2017}
\citation{cheng_higherhrnet_2020}
\citation{jin_multi-person_2019}
\citation{jin_differentiable_2020}
\citation{luo_rethinking_2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}1}Human Center Regression}{7}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}2}Part Field}{7}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}3}Associate Embedding}{7}{subsubsection.5.2.3}\protected@file@percent }
\citation{toshev_deeppose_2014}
\citation{newell_stacked_2016}
\citation{cao_realtime_2017}
\citation{chen_cascaded_2018}
\citation{cao_realtime_2017}
\citation{sumi}
\citation{summary80s}
\citation{wang_deep_2021,Chen2016,Chen_2017_CVPR,Tome_2017_CVPR,Andrikula2010,Ye2011,Martinez_2017_ICCV}
\citation{Chen2016}
\citation{Rogez2016}
\citation{Tome_2017_CVPR,Martinez_2017_ICCV}
\citation{Chen_2017_CVPR}
\citation{Dong_2019_CVPR}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{park_3d_2016,zhou_hemlets_2019,habibie_wild_2019,tekin_learning_2017}
\citation{wang_deep_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}2D Human Pose Estimation Summary}{8}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}3D pose estimation}{8}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-A}}Lifting from 2D to 3D pose}{8}{subsection.6.1}\protected@file@percent }
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Newell2016}
\citation{srivastava2014}
\citation{HeZRS15}
\citation{H3.6M}
\citation{PavlakosZDD16}
\citation{andriluka14cvpr}
\citation{Deng2021}
\citation{Deng2021}
\citation{Deng2021}
\citation{wang_deep_2021}
\citation{Tekin2016}
\citation{Luvizon2018}
\citation{H3.6M}
\citation{andriluka14cvpr}
\citation{Martinez_2017_ICCV}
\newlabel{sec:simple-yet-effective-baseline}{{\mbox  {VI-A}1}{9}{A simple yet effective baseline for 3d human pose estimation}{subsubsection.6.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}1}A simple yet effective baseline for 3d human pose estimation}{9}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}2}SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistency}{9}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-B}}3d pose directly from 2d image}{9}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}1}Structured Prediction of 3D Human Pose with Deep Neural Networks}{9}{subsubsection.6.2.1}\protected@file@percent }
\citation{spin}
\citation{simplify}
\citation{hmr}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neural network structure from \cite  {Martinez_2017_ICCV}. It consists of two inner blocks containing a linear layer followed by batch normalization and dropout. A residual connection is added from the input of the first inner block to the output of the second. This structure is then repeated another time. The networks inputs are 2d joint positions, which it outputs uplifted to three dimensions.}}{10}{figure.2}\protected@file@percent }
\newlabel{fig:martinez-network}{{2}{10}{Neural network structure from \cite {Martinez_2017_ICCV}. It consists of two inner blocks containing a linear layer followed by batch normalization and dropout. A residual connection is added from the input of the first inner block to the output of the second. This structure is then repeated another time. The networks inputs are 2d joint positions, which it outputs uplifted to three dimensions}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Test cases from \cite  {Martinez_2017_ICCV}. The picture on the left is the original overlayed with the 2d pose estimation (shown in the middle), while the respective uplifted 3d pose is shown on the right. Most test cases yield good results, but there are some failures too as shown in the bottom row.}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:martinez-test-cases}{{3}{10}{Test cases from \cite {Martinez_2017_ICCV}. The picture on the left is the original overlayed with the 2d pose estimation (shown in the middle), while the respective uplifted 3d pose is shown on the right. Most test cases yield good results, but there are some failures too as shown in the bottom row}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}2}2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning}{10}{subsubsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-C}}model-based 3d pose from 2d image}{10}{subsection.6.3}\protected@file@percent }
\citation{frankmocap}
\citation{frankmocap}
\citation{frankmocap}
\citation{frankmocap}
\citation{moon}
\citation{moon}
\citation{expose}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representation of the GAN structure. The input 2d pose gets uplifted to a 3d pose estimation and an estimated camera position by the generator. This allows rotation of the model to feed the discriminator with a reprojection from various angles, who is comparing them with real 2d poses. The reprojections are also used to uplift and reproject them again to ensure Single-view-multi-angle Consistency. \cite  {Deng2021}}}{11}{figure.4}\protected@file@percent }
\newlabel{fig:deng-structure}{{4}{11}{Representation of the GAN structure. The input 2d pose gets uplifted to a 3d pose estimation and an estimated camera position by the generator. This allows rotation of the model to feed the discriminator with a reprojection from various angles, who is comparing them with real 2d poses. The reprojections are also used to uplift and reproject them again to ensure Single-view-multi-angle Consistency. \cite {Deng2021}}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-C}1}hand estimation}{11}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Encoder-decoder architecture that takes a hand image as input, extracts the features and decodes them as sufficient parameters to construct a hand model.\cite  {frankmocap}}}{11}{figure.5}\protected@file@percent }
\newlabel{fig:frankmocap_hand}{{5}{11}{Encoder-decoder architecture that takes a hand image as input, extracts the features and decodes them as sufficient parameters to construct a hand model.\cite {frankmocap}}{figure.5}{}}
\citation{moon}
\citation{softmax}
\citation{spin}
\citation{spin}
\citation{chen}
\citation{spin}
\citation{expose}
\citation{moon}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Documentation of the \emph  {Pose2Pose} structure applied to the body (b). First $ResNet$ calculates the features of an input, which were used to calculate the joint positions and features with 1-by-1 convolutional layers. The features and positions were merged as input to the final layer of $ResNet$.\cite  {moon}}}{12}{figure.6}\protected@file@percent }
\newlabel{fig:p2p}{{6}{12}{Documentation of the \emph {Pose2Pose} structure applied to the body (b). First $\protect ResNet$ calculates the features of an input, which were used to calculate the joint positions and features with 1-by-1 convolutional layers. The features and positions were merged as input to the final layer of $\protect ResNet$.\cite {moon}}{figure.6}{}}
\newlabel{eq:expose_face_hand}{{9}{12}{hand estimation}{equation.6.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Visualisation of the $SPIN$ loop, in which first the pose and shape parameters on the right side were regressed to initialize the $SMPLify$ algorithm on the left side. The optimized parameters from $SMPLify$ were used to improve the supervision of the regressor modul, resulting into better initial models for $SMPLify$. \cite  {spin}}}{12}{figure.7}\protected@file@percent }
\newlabel{fig:spin}{{7}{12}{Visualisation of the $\protect SPIN$ loop, in which first the pose and shape parameters on the right side were regressed to initialize the $\protect SMPLify$ algorithm on the left side. The optimized parameters from $\protect SMPLify$ were used to improve the supervision of the regressor modul, resulting into better initial models for $\protect SMPLify$. \cite {spin}}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-C}2}full-body pose estimation}{12}{figure.7}\protected@file@percent }
\citation{flame}
\citation{ringnet}
\citation{ringnet}
\citation{ringnet}
\citation{Ye2011}
\citation{Haralick98}
\citation{Martinez_2017_ICCV}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-C}3}face estimation}{13}{subsubsection.6.3.3}\protected@file@percent }
\newlabel{eq:flame}{{10}{13}{face estimation}{equation.6.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Ring-shaped network architecture of the $RingNet$ in training with 4 elements that are encoder-decoder structures. Training is performed with image sources consisting of 2 subjects, with only one image coming from the second subject. Ring instances regress 3d information from 2d images, which are projected into 2d space for evaluation in the loss function. This concept ensures the learning of consistency.\cite  {ringnet}}}{13}{figure.8}\protected@file@percent }
\newlabel{fig:ringnet}{{8}{13}{Ring-shaped network architecture of the $\protect RingNet$ in training with 4 elements that are encoder-decoder structures. Training is performed with image sources consisting of 2 subjects, with only one image coming from the second subject. Ring instances regress 3d information from 2d images, which are projected into 2d space for evaluation in the loss function. This concept ensures the learning of consistency.\cite {ringnet}}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-D}}Direct 3d pose estimation}{13}{subsection.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VII}conclusion}{13}{section.7}\protected@file@percent }
\bibstyle{plain}
\bibdata{references/benchmarks,references/criteria,references/2d_estimation,references/3d_estimation}
\bibcite{mpii}{1}
\bibcite{andriluka14cvpr}{2}
\bibcite{Andrikula2010}{3}
\bibcite{artacho_unipose_2020}{4}
\bibcite{simplify}{5}
\bibcite{bulat_human_2016}{6}
\bibcite{cai_learning_2020}{7}
\bibcite{cao_realtime_2017}{8}
\bibcite{carion_end--end_2020}{9}
\bibcite{carreira_human_2016}{10}
\bibcite{Chen_2017_CVPR}{11}
\bibcite{chen}{12}
\bibcite{Chen2016}{13}
\bibcite{chen_articulated_2014}{14}
\bibcite{chen_cascaded_2018}{15}
\bibcite{cheng_higherhrnet_2020}{16}
\bibcite{expose}{17}
\bibcite{chu_multi-context_2017}{18}
\@writefile{toc}{\contentsline {section}{References}{14}{section*.4}\protected@file@percent }
\bibcite{Deng2021}{19}
\bibcite{Dong_2019_CVPR}{20}
\bibcite{eichner_human_2012}{21}
\bibcite{pcp}{22}
\bibcite{fan_combining_2015}{23}
\bibcite{felzenszwalb_pictorial_2005}{24}
\bibcite{feng_ning_toward_2005}{25}
\bibcite{fieraru_learning_2018}{26}
\bibcite{geng_bottom-up_2021}{27}
\bibcite{girdhar_detect-and-track_2018}{28}
\bibcite{gkioxari_chained_2016}{29}
\bibcite{gower1975generalized}{30}
\bibcite{habibie_wild_2019}{31}
\bibcite{Haralick98}{32}
\bibcite{prox}{33}
\bibcite{obman}{34}
\bibcite{HeZRS15}{35}
\bibcite{he_deep_2016}{36}
\bibcite{ltsh}{37}
\bibcite{dip}{38}
\bibcite{insafutdinov_arttrack_2017}{39}
\bibcite{insafutdinov_deepercut_2016}{40}
\bibcite{H3.6M}{41}
\bibcite{jin_multi-person_2019}{42}
\bibcite{jin_differentiable_2020}{43}
\bibcite{hmr}{44}
\bibcite{ke_multi-scale_2018}{45}
\bibcite{spin}{46}
\bibcite{kreiss_pifpaf_2019}{47}
\bibcite{krizhevsky_imagenet_2012}{48}
\bibcite{li_simple_2019}{49}
\bibcite{li_pose_2021}{50}
\bibcite{li_markov_1994}{51}
\bibcite{li_heterogeneous_2014}{52}
\bibcite{flame}{53}
\bibcite{lifshitz_human_2016}{54}
\bibcite{coco}{55}
\bibcite{liu_cascaded_2018}{56}
\bibcite{smpl}{57}
\bibcite{luo_lstm_2018}{58}
\bibcite{luo_rethinking_2021}{59}
\bibcite{Luvizon2018}{60}
\bibcite{cape}{61}
\bibcite{resynth}{62}
\bibcite{amass}{63}
\bibcite{martinez_single-network_2019}{64}
\bibcite{Martinez_2017_ICCV}{65}
\bibcite{micilotta_real-time_2006}{66}
\bibcite{mocap}{67}
\bibcite{summary80s}{68}
\bibcite{moon}{69}
\bibcite{tuch}{70}
\bibcite{newell_associative_2017}{71}
\bibcite{newell_stacked_2016}{72}
\bibcite{Newell2016}{73}
\bibcite{ferrari_pose_2018}{74}
\bibcite{nie_single-stage_2019}{75}
\bibcite{nie_human_2018}{76}
\bibcite{park_3d_2016}{77}
\bibcite{PavlakosZDD16}{78}
\bibcite{pfister_flowing_2015}{79}
\bibcite{pishchulin_deepcut_2016}{80}
\bibcite{qiu_peeking_2020}{81}
\bibcite{rafi_efficient_2016}{82}
\bibcite{ramakrishna_pose_2014}{83}
\bibcite{ren_faster_2016}{84}
\bibcite{Rhodin2018}{85}
\bibcite{caesar}{86}
\bibcite{Rogez2016}{87}
\bibcite{mano}{88}
\bibcite{frankmocap}{89}
\bibcite{ringnet}{90}
\bibcite{srivastava2014}{91}
\bibcite{su_multi-person_2019}{92}
\bibcite{sumi}{93}
\bibcite{sun_deep_2019}{94}
\bibcite{sun_compositional_2017}{95}
\bibcite{softmax}{96}
\bibcite{sun_integral_2018}{97}
\bibcite{szegedy_going_2014}{98}
\bibcite{grab}{99}
\bibcite{Tekin2016}{100}
\bibcite{tekin_learning_2017}{101}
\bibcite{tian_directpose_2019}{102}
\bibcite{Tome_2017_CVPR}{103}
\bibcite{tompson_efficient_2015}{104}
\bibcite{tompson_joint_2014}{105}
\bibcite{pdj}{106}
\bibcite{toshev_deeppose_2014}{107}
\bibcite{surreal}{108}
\bibcite{vaswani_attention_2017}{109}
\bibcite{vip}{110}
\bibcite{wang_graph-pcnn_2020}{111}
\bibcite{wang_deep_2021}{112}
\bibcite{wei_convolutional_2016}{113}
\bibcite{yang_learning_2017}{114}
\bibcite{pck}{115}
\bibcite{Ye2011}{116}
\bibcite{yixing_gao_user_2015}{117}
\bibcite{Zheng2020}{118}
\bibcite{zhou_hemlets_2019}{119}
\bibcite{zhu_multi-person_nodate}{120}
\bibcite{zuffi_pictorial_2012}{121}
\gdef \@abspage@last{17}
