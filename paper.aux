\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mocap}
\citation{vip}
\citation{cape}
\citation{resynth}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Benchmark Datasets}{1}{section.2}\protected@file@percent }
\newlabel{sec:benchmarks}{{II}{1}{Benchmark Datasets}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Max Planck Institute datasets}{1}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}1}Clothing}{1}{subsubsection.2.1.1}\protected@file@percent }
\citation{vip}
\citation{dip}
\citation{prox}
\citation{grab}
\citation{tuch}
\citation{mano}
\citation{obman}
\citation{surreal}
\citation{ltsh}
\citation{amass}
\citation{coco}
\citation{caesar}
\citation{pcp}
\citation{pdj}
\citation{pck}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}2}Full-body}{2}{subsubsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}3}Hand scans}{2}{subsubsection.2.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}4}synthetic data}{2}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-A}5}Generalization of datasets}{2}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}COCO dataset}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Caesar dataset}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Benchmark metrics}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Percentage of Correct Parts PCP}{2}{subsection.3.1}\protected@file@percent }
\newlabel{sec:pcp}{{\mbox  {III-A}}{2}{Percentage of Correct Parts PCP}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Abstract visualisation of the values required for the PCP calculation. A solid line between two endpoints represents a bone in the ground truth of a dataset in green and the detected bone in black. The dashed lines show the distances $l_{1}, l_{2}$, which are evaluated with the true length $L$ of the bone.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:pcp}{{1}{2}{Abstract visualisation of the values required for the PCP calculation. A solid line between two endpoints represents a bone in the ground truth of a dataset in green and the detected bone in black. The dashed lines show the distances $\protect l_{1}, l_{2}$, which are evaluated with the true length $\protect L$ of the bone}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Percentage of Detected Joints PDJ}{2}{subsection.3.2}\protected@file@percent }
\newlabel{sec:pdj}{{\mbox  {III-B}}{2}{Percentage of Detected Joints PDJ}{subsection.3.2}{}}
\citation{mpii}
\citation{Zheng2020}
\citation{smpl}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Percentage of Correct Key-points PCK}{3}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Head-normalized Probability of Correct Key-points PCKh}{3}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-E}}Object Key-point Similarity OKS}{3}{subsection.3.5}\protected@file@percent }
\newlabel{eq:OKS}{{1}{3}{Object Key-point Similarity OKS}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-F}}Mean Per Joint Position Error (MPJPE)}{3}{subsection.3.6}\protected@file@percent }
\newlabel{criteria:mpjpe}{{\mbox  {III-F}}{3}{Mean Per Joint Position Error (MPJPE)}{subsection.3.6}{}}
\newlabel{eq:MPJPE}{{2}{3}{Mean Per Joint Position Error (MPJPE)}{equation.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}models in human estimation}{3}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Skinned Multi-Person Linear model SMPL}{3}{subsection.4.1}\protected@file@percent }
\newlabel{sec:SMPL}{{\mbox  {IV-A}}{3}{Skinned Multi-Person Linear model SMPL}{subsection.4.1}{}}
\newlabel{eq:model}{{3}{3}{Skinned Multi-Person Linear model SMPL}{equation.4.3}{}}
\newlabel{eq:lbs}{{4}{3}{Skinned Multi-Person Linear model SMPL}{equation.4.4}{}}
\newlabel{eq:smpl}{{5}{3}{Skinned Multi-Person Linear model SMPL}{equation.4.5}{}}
\citation{zuffi_pictorial_2012}
\citation{rafi_efficient_2016}
\citation{sun_deep_2019}
\citation{cheng_higherhrnet_2020}
\citation{ren_faster_2016,micilotta_real-time_2006}
\citation{carreira_human_2016,fan_combining_2015,fieraru_learning_2018,li_heterogeneous_2014,qiu_peeking_2020,sun_compositional_2017,sun_integral_2018,toshev_deeppose_2014,wang_graph-pcnn_2020,z}
\citation{li_pose_2021}
\citation{toshev_deeppose_2014}
\citation{krizhevsky_imagenet_2012}
\citation{carreira_human_2016}
\citation{szegedy_going_2014}
\citation{vaswani_attention_2017}
\citation{li_pose_2021}
\citation{carion_end--end_2020}
\newlabel{eq:Tf}{{6}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.6}{}}
\newlabel{eq:lbs2}{{7}{4}{Skinned Multi-Person Linear model SMPL}{equation.4.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}2D pose estimation}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-A}}Top-Down Framework}{4}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}1}Regression-based methods}{4}{subsubsection.5.1.1}\protected@file@percent }
\citation{chen_articulated_2014,newell_stacked_2016,wei_convolutional_2016}
\citation{tompson_efficient_2015,tompson_joint_2014}
\citation{artacho_unipose_2020,bulat_human_2016,gkioxari_chained_2016,lifshitz_human_2016,newell_stacked_2016,tompson_efficient_2015}
\citation{carreira_human_2016,luo_lstm_2018,toshev_deeppose_2014,wei_convolutional_2016}
\citation{ramakrishna_pose_2014}
\citation{wei_convolutional_2016}
\citation{he_deep_2016}
\citation{wei_convolutional_2016}
\citation{cai_learning_2020,chen_cascaded_2018,chu_multi-context_2017,ke_multi-scale_2018,liu_cascaded_2018,newell_stacked_2016,su_multi-person_2019,sun_deep_2019,xiao_simple_2018-1,yang_learning_2017}
\citation{newell_stacked_2016}
\citation{chu_multi-context_2017}
\citation{yang_learning_2017}
\citation{cai_learning_2020}
\citation{liu_cascaded_2018,eichner_human_2012,felzenszwalb_pictorial_2005,girdhar_detect-and-track_2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-A}2}Heatmap-based Methods}{5}{subsubsection.5.1.2}\protected@file@percent }
\citation{pfister_flowing_2015}
\citation{yixing_gao_user_2015}
\citation{micilotta_real-time_2006,cheng_higherhrnet_2020,fieraru_learning_2018,jin_differentiable_2020,kreiss_pifpaf_2019,nie_single-stage_2019,insafutdinov_arttrack_2017,insafutdinov_deepercut_2016,newell_associative_2017,pishchulin_deepcut_2016,tian_directpose_2019}
\citation{geng_bottom-up_2021,nie_single-stage_2019,nie_human_2018}
\citation{jin_differentiable_2020,kreiss_pifpaf_2019,martinez_single-network_2019,insafutdinov_deepercut_2016,li_simple_2019}
\citation{cheng_higherhrnet_2020,jin_multi-person_2019,luo_rethinking_2021,newell_associative_2017}
\citation{ferrari_pose_2018}
\citation{newell_stacked_2016}
\citation{nie_single-stage_2019}
\citation{geng_bottom-up_2021}
\citation{cao_realtime_2017}
\citation{wei_convolutional_2016}
\citation{zhu_multi-person_nodate}
\citation{kreiss_pifpaf_2019}
\citation{cao_realtime_2017}
\citation{newell_stacked_2016}
\citation{newell_associative_2017}
\citation{cheng_higherhrnet_2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-B}}Bottom-Up Framework}{6}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}1}Human Center Regression}{6}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}2}Part Field}{6}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {V-B}3}Associate Embedding}{6}{subsubsection.5.2.3}\protected@file@percent }
\citation{jin_multi-person_2019}
\citation{jin_differentiable_2020}
\citation{luo_rethinking_2021}
\citation{toshev_deeppose_2014}
\citation{newell_stacked_2016}
\citation{cao_realtime_2017}
\citation{chen_cascaded_2018}
\citation{cao_realtime_2017}
\citation{summary80s}
\citation{wang_deep_2021,Chen2016,Chen_2017_CVPR,Tome_2017_CVPR,Andrikula2010,Ye2011,Martinez_2017_ICCV}
\citation{Chen2016}
\citation{Rogez2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {V-C}}2D Human Pose Estimation Summary}{7}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}3D pose estimation}{7}{section.6}\protected@file@percent }
\citation{Tome_2017_CVPR,Martinez_2017_ICCV}
\citation{Chen_2017_CVPR}
\citation{Dong_2019_CVPR}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{park_3d_2016,wang_deep_2021,zhou_hemlets_2019,habibie_wild_2019,tekin_learning_2017}
\citation{wang_deep_2021}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Martinez_2017_ICCV}
\citation{Newell2016}
\citation{srivastava2014}
\citation{HeZRS15}
\citation{H3.6M}
\citation{PavlakosZDD16}
\citation{andriluka14cvpr}
\citation{Deng2021}
\citation{Deng2021}
\citation{gower1975generalized}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-A}}Lifting from 2D to 3D pose}{8}{subsection.6.1}\protected@file@percent }
\newlabel{sec:simple-yet-effective-baseline}{{\mbox  {VI-A}1}{8}{A simple yet effective baseline for 3d human pose estimation}{subsubsection.6.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}1}A simple yet effective baseline for 3d human pose estimation}{8}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-A}2}SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistency}{8}{subsubsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-B}}3d pose directly from 2d image}{8}{subsection.6.2}\protected@file@percent }
\citation{Luvizon2018}
\citation{H3.6M}
\citation{andriluka14cvpr}
\citation{Martinez_2017_ICCV}
\citation{Tekin2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neural network structure from \cite  {Martinez_2017_ICCV}}}{9}{figure.2}\protected@file@percent }
\newlabel{fig:martinez-network}{{2}{9}{Neural network structure from \cite {Martinez_2017_ICCV}}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Test cases from \cite  {Martinez_2017_ICCV}}}{9}{figure.3}\protected@file@percent }
\newlabel{fig:martinez-test-cases}{{3}{9}{Test cases from \cite {Martinez_2017_ICCV}}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}1}2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning}{9}{subsubsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-B}2}Structured Prediction of 3D Human Pose with Deep Neural Networks}{9}{subsubsection.6.2.2}\protected@file@percent }
\citation{Ye2011}
\citation{Haralick98}
\citation{simplify}
\citation{frankmocap}
\citation{frankmocap}
\citation{frankmocap}
\citation{hmr}
\citation{spin}
\citation{spin}
\citation{spin}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces GAN structure from \cite  {Deng2021}}}{10}{figure.4}\protected@file@percent }
\newlabel{fig:deng-structure}{{4}{10}{GAN structure from \cite {Deng2021}}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-C}}Direct 3d pose estimation}{10}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VI-D}}use of models in pose estimation}{10}{subsection.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Encoder-decoder architecture that takes a hand image as input, extracts the features and decodes them as sufficient parameters to construct a hand model.\cite  {frankmocap}}}{10}{figure.5}\protected@file@percent }
\newlabel{fig:frankmocap_hand}{{5}{10}{Encoder-decoder architecture that takes a hand image as input, extracts the features and decodes them as sufficient parameters to construct a hand model.\cite {frankmocap}}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-D}1}hand estimation}{10}{figure.5}\protected@file@percent }
\citation{flame}
\citation{ringnet}
\citation{ringnet}
\citation{ringnet}
\bibstyle{plain}
\bibdata{references/benchmarks,references/criteria,references/2d_estimation,references/3d_estimation}
\bibcite{mpii}{1}
\bibcite{andriluka14cvpr}{2}
\bibcite{Andrikula2010}{3}
\bibcite{artacho_unipose_2020}{4}
\bibcite{simplify}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Visualisation of the $SPIN$ loop, in which first the pose and shape parameters on the right side were regressed to optimize the initialization of the $SMPLify$ algorithm on the left side. The optimized parameters were used to improve the supervision in the loss function of the neural network. \cite  {spin}}}{11}{figure.6}\protected@file@percent }
\newlabel{fig:spin}{{6}{11}{Visualisation of the $\protect SPIN$ loop, in which first the pose and shape parameters on the right side were regressed to optimize the initialization of the $\protect SMPLify$ algorithm on the left side. The optimized parameters were used to improve the supervision in the loss function of the neural network. \cite {spin}}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-D}2}full-body pose estimation}{11}{figure.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VI-D}3}face estimation}{11}{subsubsection.6.4.3}\protected@file@percent }
\newlabel{eq:flame}{{8}{11}{face estimation}{equation.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Ring-shaped network architecture of the $RingNet$ in training with 4 elements that are encoder-decoder structures. Training is performed with image sources consisting of 2 subjects, with only one image coming from the second subject. Ring instances regress 3d information from 2d images, which are projected into 2d space for evaluation in the loss function. This concept ensures the learning of consistency.\cite  {ringnet}}}{11}{figure.7}\protected@file@percent }
\newlabel{fig:ringnet}{{7}{11}{Ring-shaped network architecture of the $\protect RingNet$ in training with 4 elements that are encoder-decoder structures. Training is performed with image sources consisting of 2 subjects, with only one image coming from the second subject. Ring instances regress 3d information from 2d images, which are projected into 2d space for evaluation in the loss function. This concept ensures the learning of consistency.\cite {ringnet}}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}conclusion}{11}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{11}{section*.4}\protected@file@percent }
\bibcite{bulat_human_2016}{6}
\bibcite{cai_learning_2020}{7}
\bibcite{cao_realtime_2017}{8}
\bibcite{carion_end--end_2020}{9}
\bibcite{carreira_human_2016}{10}
\bibcite{Chen_2017_CVPR}{11}
\bibcite{Chen2016}{12}
\bibcite{chen_articulated_2014}{13}
\bibcite{chen_cascaded_2018}{14}
\bibcite{cheng_higherhrnet_2020}{15}
\bibcite{chu_multi-context_2017}{16}
\bibcite{Deng2021}{17}
\bibcite{Dong_2019_CVPR}{18}
\bibcite{eichner_human_2012}{19}
\bibcite{pcp}{20}
\bibcite{fan_combining_2015}{21}
\bibcite{felzenszwalb_pictorial_2005}{22}
\bibcite{fieraru_learning_2018}{23}
\bibcite{geng_bottom-up_2021}{24}
\bibcite{girdhar_detect-and-track_2018}{25}
\bibcite{gkioxari_chained_2016}{26}
\bibcite{gower1975generalized}{27}
\bibcite{habibie_wild_2019}{28}
\bibcite{Haralick98}{29}
\bibcite{prox}{30}
\bibcite{obman}{31}
\bibcite{HeZRS15}{32}
\bibcite{he_deep_2016}{33}
\bibcite{ltsh}{34}
\bibcite{dip}{35}
\bibcite{insafutdinov_arttrack_2017}{36}
\bibcite{insafutdinov_deepercut_2016}{37}
\bibcite{H3.6M}{38}
\bibcite{jin_multi-person_2019}{39}
\bibcite{jin_differentiable_2020}{40}
\bibcite{hmr}{41}
\bibcite{ke_multi-scale_2018}{42}
\bibcite{spin}{43}
\bibcite{kreiss_pifpaf_2019}{44}
\bibcite{krizhevsky_imagenet_2012}{45}
\bibcite{li_simple_2019}{46}
\bibcite{li_pose_2021}{47}
\bibcite{li_heterogeneous_2014}{48}
\bibcite{flame}{49}
\bibcite{lifshitz_human_2016}{50}
\bibcite{coco}{51}
\bibcite{liu_cascaded_2018}{52}
\bibcite{smpl}{53}
\bibcite{luo_lstm_2018}{54}
\bibcite{luo_rethinking_2021}{55}
\bibcite{Luvizon2018}{56}
\bibcite{cape}{57}
\bibcite{resynth}{58}
\bibcite{amass}{59}
\bibcite{martinez_single-network_2019}{60}
\bibcite{Martinez_2017_ICCV}{61}
\bibcite{micilotta_real-time_2006}{62}
\bibcite{summary80s}{63}
\bibcite{tuch}{64}
\bibcite{newell_associative_2017}{65}
\bibcite{newell_stacked_2016}{66}
\bibcite{Newell2016}{67}
\bibcite{ferrari_pose_2018}{68}
\bibcite{nie_single-stage_2019}{69}
\bibcite{nie_human_2018}{70}
\bibcite{park_3d_2016}{71}
\bibcite{PavlakosZDD16}{72}
\bibcite{pfister_flowing_2015}{73}
\bibcite{pishchulin_deepcut_2016}{74}
\bibcite{qiu_peeking_2020}{75}
\bibcite{rafi_efficient_2016}{76}
\bibcite{ramakrishna_pose_2014}{77}
\bibcite{ren_faster_2016}{78}
\bibcite{caesar}{79}
\bibcite{Rogez2016}{80}
\bibcite{mano}{81}
\bibcite{frankmocap}{82}
\bibcite{ringnet}{83}
\bibcite{srivastava2014}{84}
\bibcite{su_multi-person_2019}{85}
\bibcite{sun_deep_2019}{86}
\bibcite{sun_compositional_2017}{87}
\bibcite{sun_integral_2018}{88}
\bibcite{szegedy_going_2014}{89}
\bibcite{grab}{90}
\bibcite{Tekin2016}{91}
\bibcite{tekin_learning_2017}{92}
\bibcite{tian_directpose_2019}{93}
\bibcite{Tome_2017_CVPR}{94}
\bibcite{tompson_efficient_2015}{95}
\bibcite{tompson_joint_2014}{96}
\bibcite{pdj}{97}
\bibcite{toshev_deeppose_2014}{98}
\bibcite{surreal}{99}
\bibcite{vaswani_attention_2017}{100}
\bibcite{vip}{101}
\bibcite{wang_graph-pcnn_2020}{102}
\bibcite{wang_deep_2021}{103}
\bibcite{wei_convolutional_2016}{104}
\bibcite{yang_learning_2017}{105}
\bibcite{pck}{106}
\bibcite{Ye2011}{107}
\bibcite{yixing_gao_user_2015}{108}
\bibcite{Zheng2020}{109}
\bibcite{zhou_hemlets_2019}{110}
\bibcite{zhu_multi-person_nodate}{111}
\bibcite{zuffi_pictorial_2012}{112}
\gdef \@abspage@last{14}
