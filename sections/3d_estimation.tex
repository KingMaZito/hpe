\section{3D pose estimation}
From a historical perspective, a 3d motion capture algorithm consists of 4 sequential processes: initialisation, tracking, pose estimation and recognition. Initialization involves both camera and model initialization, i.e. setting the camera calibration and finding a model that represents the subject and assigning its initial pose manually or automatically. Model-based approaches can be viewed iteratively, with each frame of the data source representing an iteration in which the initial pose is refined. Tracking is concerned with the relationship between the parts of the subject's body. This leads to segmentation of the subject from the background, representation changes, and establishing tracking in further images. The next phase, which is mainly covered in this section, is the estimation of the pose. A distinction is made between model-based and non-model-based methods, with the former requiring  \emph{a priori} a model. In that approaches, especially human pose estimation, a human model is used to benefit from its encoded information. This model can either be used indirectly, considering e.g. only general aspects such as size and structure, or it is a direct used model. Directly used models are both more detailed and offer broader benefits in regards to occlusion handling and embedded kinematic constraints. In an application, the observed object is approximated by the model, which is continuously refined with further images.\cite{summary80s}
\\
As with 2D Pose Estimation, neural networks, particularly convolutional networks, have successfully been used to achieve more accurate results than earlier methods \cite{wang_deep_2021, Chen2016, Chen_2017_CVPR, Tome_2017_CVPR, Andrikula2010, Ye2011, Martinez_2017_ICCV}. Since neural networks can be very resource intensive to compute and the architecture can be extremely complex when working with 3D data, many  of the presented approaches use 2D Pose Estimation followed by an uplifting process to 3 dimensions.  Also, a lot of 2D training data is available in comparison to 3D data which could be used to train a neural network, because the annotation process is way harder in higher dimensions. Especially the lack of in-the-wild 3D training data, which is not created under laboratory conditions, can be partly overcome by using 2D pose estimation methods (which work way better in-the-wild because of more training data available) first before adding an extra dimension.
 Therefore \cite{Chen2016} presents a process to synthesize training images and shows that neural networks training with data generated by their method are even more effective than neural networks which were trained using real images. \cite{Rogez2016} presents a similar approach.

\subsection{Lifting from 2D to 3D pose}
For uplifting, recent work has proven statistical models such as (deep) neural networks themselves \cite{Tome_2017_CVPR, Martinez_2017_ICCV}, matching the estimated 2D pose with a database \cite{Chen_2017_CVPR} or triangulation using multiple viewpoints \cite{Dong_2019_CVPR} useful. In particular \cite{Martinez_2017_ICCV} shows that even very simple deep neural networks can be extraordinarily effective for uplifting 2D to 3D pose estimations, considering both computational resources and failure rate.
\newline
Inspired by various 2D human pose estimation algorithms, many studies have employed the outputs of 2D pose estimate methods for 3D human pose estimation to improve in-the-wild generalization performance. For example, Martinez et al. \cite{Martinez_2017_ICCV} pioneered the research on lifting 2D poses to 3D space with a simple yet effective neural network. Other methods \cite{park_3d_2016, wang_deep_2021, zhou_hemlets_2019, habibie_wild_2019, tekin_learning_2017} focus on fusing 2D joint heat maps from the top-down 2D pose estimation methods with 3D image cues to reduce ambiguity. The uplifting approach also makes it possible to project the calculated 3d pose to 2d again to make sure results are consistent. \cite{wang_deep_2021} 

\begin{figure*}[!htb]
	\centering
	\includegraphics[scale=0.4]{3dpose/martinez_network.png}
	\caption{Neural network structure from \cite{Martinez_2017_ICCV}}
	\label{fig:martinez-network}
\end{figure*}

\begin{figure*}[!htb]
	\centering
	\includegraphics[scale=0.3]{3dpose/martinez_test_cases.png}
	\caption{Test cases from \cite{Martinez_2017_ICCV}}
	\label{fig:martinez-test-cases}
\end{figure*}

\subsubsection{A simple yet effective baseline for 3d human pose estimation}
\label{simple-yet-effective-baseline}
While trying to investigate common errors in the uplifting process, \cite{Martinez_2017_ICCV} created a method with state-of-the-art results for 3d pose estimation using a very basic neural network with recently proposed optimization methods, whose structure is shown in \autoref{fig:martinez-network}. Linear layers changing the input and output dimensions are not shown. 2d joint positions are used as input data determined by the so called stacked hourglass network as described in \cite{Newell2016}, while the output consists of 3d joint positions.\\
 Since the 2d joint positions are low-dimensional and therefore no highly complex computation is necessary, a simple linear layer with a RELU activation function is used first, followed by batch normalization and dropout as presented in \cite{srivastava2014} to prevent overfitting and improve result quality at the cost of a slight increase in computation time during training and testing. This structure is then repeated and a residual connection to the output added. These connections are proposed to help improve performance, reduce training time and lower error rates in \cite{HeZRS15}. The whole network established so far is doubled to complete the architecture which in sum consists of 4 to 5 million trainable parameters. It was then trained on Human3.6M, a dataset with 3.6 million 3d poses of humans during normal activities such as eating or walking \cite{H3.6M}.\\ Testing results show that this approach outperformed previous methods like \cite{PavlakosZDD16} in most cases, despite the simple architecture that was used. However, testing on the MPII Human Pose Dataset \cite{andriluka14cvpr} also revealed limitations of the network shown especially in the bottom row of \autoref{fig:martinez-test-cases}. Firstly (left and right picture), the 2d joint position must be detected properly. The middle picture also rendered a problem, which according to the original paper comes down to poses being not included in the Human3.6M dataset, such as upside-down poses or examples not showing a full human body.\\
 The authors conclude that basic neural networks today are already able to produce very good results in terms of accuracy for uplifting 2d to 3d human poses. Therefore one of the main error sources of this process remains 2d pose estimation and more complex models should be able to perform the task of uplifting even better.
 
\begin{figure*}[!htb]
 	\centering
 	\includegraphics[scale=0.35]{3dpose/deng_main_structure.png}
 	\caption{GAN structure from \cite{Deng2021}}
 	\label{fig:deng-structure}
\end{figure*}
 
\subsubsection{SVMAC: Unsupervised 3D Human Pose Estimation from a Single Image with Single-view-multi-angle Consistency}
\cite{Deng2021} proposes a generative adversarial network (GAN) method which works unsupervised with just 2d joint positions as inputs. The generator gets trained to lift the input to a 3d pose estimation and extract the camera position, which enables rotating the model to reproject it into two dimensions. The reprojected 2d joint positions aswell as the ground truth ones are used to train the corresponding discriminator of the GAN. Single-view-multi-angle Consistency (SVMAC) denotes the ability to mix different rotations of an estimate pose with those of the estimated camera and ensuring that reprojections into 2d from the same angle are consistent. This is used in the loss function of the generator.
The structure of the network is shown in \autoref{fig:deng-structure}. SVMAC constraints are applied using the generators on the right.
SVMAC resulted in heavily improved performance of the model when using just 2 angles. However using more than 2 did not yield a significantly better result but increased training time by a lot.
Another experiment done was including 5\% ground truth data (3d poses) for supervision (with 2 angles used for SVMAC), which led to impressive results being able to outperform many weakly- and even some fully supervised approaches in terms of P-MPJPE (mean per joint position error, see \autoref{criteria:mpjpe}, when aligning with procrustes analysis transformation \cite{gower1975generalized} for alignment first). When considering only unsupervised methods, this one outperformed any previous one.
 
\subsection{3d pose directly from 2d image}
Methods for 3d human pose estimation directly from an imagine (or many) show the lack of 3d training data not captured by special indoor motion capture systems. This often leads to worse performance when comparing to uplifting methods in a real scenario test. With more in-the-wils datasets coming up in the future, generating a 3d pose directly from a 2d image could be outperforming the uplifting approach. For now, generating artificial training data or using unsupervised methods yields the best results in this area.
\subsubsection{2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning}
In \cite{Luvizon2018} a convolutional deep neural network structure is proposed, which is able to handle 2d and 3d pose estimation from still images plus action recognition from video, since those problems are coupled. While not achieving groundbreakingly new state-of-the-art performance results in any of the solved tasks individually, this approach goes to show that a single architecture is able to produce at least equally as good results as dedicated methods for each task separately. Since the solved problems overlap, the network can work very efficient computationally. The authors claim that merging the tasks and training them together also increases robustness. Datasets used include Human3.6M \cite{H3.6M} and MPII Human Poses \cite{andriluka14cvpr}, which render comparison with the approach presented in \autoref{simple-yet-effective-baseline} possible. The multitask convolutional neural network outperforms \cite{Martinez_2017_ICCV} quite a lot, what in turn proves that complex network structures can produce even better results, as Martinez et al. stated.
% MAYBE describe softargmax + heatmaps block for pose esti
% TODO some unsupervised or training data generation


\subsection{Direct 3d pose estimation}
Methods working directly on 3D data, such as \cite{Ye2011} (working with depth maps) can avoid potential sources of error such as projections or lighting conditions. This leads to more robust and accurate results, however more complex (neural network) architectures and computational resources are required. Traditional methods like the least-squares-estimation presented in \cite{Haralick98} work without training data needed and are computationally inexpensive, but yield rather high error rates compared to newer methods.

\subsubsection{Skinned Multi-Person Linear model SMPL}
In \emph{SMPL} a model $M(\vec{\beta},\vec{\theta},\phi)$ is learned from the 3d scans explained in \autoref{sec:benchmarks}, that returns a mesh from the input. This formulation is also included in \autoref{eq:model}, where $\mathbb{R}^{3N}$ is a vector of \emph{N = 6890} vertices sculpturing the mesh. In this formula, $\vec{\beta}$ is a vector of blend shapes, while $\vec{\theta}$ are poses and $\phi$ describes the displacement of soft tissues.

\begin{equation}
\label{eq:model}
M(\vec{\beta},\vec{\theta},\phi) : \mathbb{R}^{\vert \vec{\theta} \vert \times \vert \vec{\beta} \vert} \mapsto \mathbb{R}^{3N}
\end{equation}

\emph{SMPL} is based on vertex skinning and blend shapes. A vertex changes its position depending on the motion of the associated joint. This displacement is controlled by assigned blend weights. A vector $T \in \mathbb{R}^{3N}$ of vertex positions describes a gender neutral initial human model, while a matrix $W \in \mathbb{R}^{N \times K}$ represents the blend weights per vertices and \emph{K = 23} joints. The joints that describe the human structure and form the skeleton are represented by rotation vectors. Moreover, T can be rearranged by the pose-blending function $B_{P}(\vec{\theta})$ according to the given poses, leaving T unaffected, while $B_{S}(\vec{\beta})$ reshapes the identity model by its given shape blends. A skinning function $W(T,J,\vec{\theta},\mathcal{W})$, on the other hand, transforms the initial model $T$ by the joint positions $J \in \mathbb{R}^{3K}$ and rotations $\vec{\theta}$ of the desired model. The simplest skinning approach is called \emph{linear blend skinning}, where each resulting vertex $\bar{t}_{i}'$ depends on the sum of its weights $w_{k,i}$ per joint multiplied by its previous position $\bar{t}_{i}$ and a target destination, as documented in \autoref{eq:lbs}. 

\begin{equation}
\label{eq:lbs}
\bar{t}_{i}' = \sum_{k=1}^{K} w_{k,i} \times G_{k}'(\vec{\theta},J) \times \bar{t}_{i}
\end{equation}

Unfortunately, this approach is error-prone and requires additional refinement by the artists to reduce artifacts. To overcome these problems, the skinning method in \emph{SMPL} introduces new functions $T_{F}(\vec{\beta},\vec{\theta})$ and $J(\vec{\beta})$ so that \autoref{eq:model} and $W(T,J,\vec{\theta},\mathcal{W})$ can be reformulated by \autoref{eq:smpl}.

\begin{equation}
\label{eq:smpl}
M(\vec{\beta},\vec{\theta}) = W(T_{F}(\vec{\beta},\vec{\theta}),J(\vec{\beta}),\vec{\theta},\mathcal{W}) \mapsto \mathbb{R}^{3N}
\end{equation}

In $T_{F}(\vec{\beta},\vec{\theta})$, the initial body $T$ is considered as a known parameter, while the pose rearrangements $B_{P}(\vec{\theta})$ and the reshapes $B_{S}(\vec{\beta})$ are added. Since the focus is on pose estimation, this section explains $B_{P}(\vec{\theta})$ in more detail. With respect to \autoref{eq:Tf}, $B_{P}(\vec{\theta})$ can be viewed as linear combinations of pose-blend shapes P learned from the datasets mentioned in \autoref{sec:benchmarks}. Each joint orientation $\theta$ is varied by rotating it 9 times relatively, therefore the iteration space is extended to the cardinality of $R_{n}(\vec{\theta})$ which is $\vert R_{n}(\vec{\theta}) \vert = K \times 9 = 207$. In \autoref{eq:Tf}, a second rotation set of the initial $T$ model is also initialised to define the vertex deviations and make them linear to P. Thus, the impact of an pose blend shape is neglected when a joint orientation is equal to the $T$ orientation.

\begin{equation}
\label{eq:Tf}
	\begin{split}
		T_{F}(\vec{\beta},\vec{\theta})&: \mathbb{R}^{\vert \vec{\theta} 			\vert \times \vert \vec{\beta} \vert} \mapsto \mathbb{R}^{3N} \\
		T_{F}(\vec{\beta},\vec{\theta}) &= T + B_{S}(\vec{\beta}) + B_{P}				(\vec{\theta}) \\
		&= T + B_{S}(\vec{\beta}) + \sum_{n=1}^{9K} (R_{n}(\vec{\theta}) - 				R_{n}(\vec{\theta}^{*}))P_{n};
	\end{split}
\end{equation}

Finally, the \emph{linear blend skinning} in \autoref{eq:lbs} can be rewritten by the formulations in \autoref{eq:smpl} and their application in \autoref{eq:Tf}. This \autoref{eq:lbs2} is formulated below, considering both $B_{P}$ and $B_{S}$ as matrices in which a vertex is indexed by i.

\begin{equation}
\label{eq:lbs2}
\bar{t}_{i}' = \sum_{k=1}^{K} w_{k,i} \times G_{k}'(\vec{\theta},J(\vec{\beta})) \times (\bar{t}_{i} + b_{S,i}(\vec{\beta}) +b_{P,i}(\vec{\theta}))
\end{equation}


\cite{smpl}

