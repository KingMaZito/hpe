\section{Benchmark Datasets}
\label{sec:benchmarks}
Huge data sources are part of the benchmarking process for human posture estimation approaches. The datasets used specifically for this purpose contain images showing one or more individuals in different poses, as well as other information about joint and limb positions. This information is acquired by motion capture markers on the body or using \emph{IMU} units as passive sensing devices, attached to the body to determine the position regardless of obstacles. Existing video footage can also be manually annotated. Some datasets focus on different features within its content to ensure the quality of a model in relation to that aspect. Commonly used datasets are explained in more detail in this section.

\subsection{Max Planck Institute datasets}
\emph{Perceiving Systems}\footnote{\url{https://ps.is.mpg.de/}} is a department of the \emph{Max Planck Institute for Intelligent Systems}\footnote{\url{https://is.mpg.de/}} that is specialized in computer vision and, in addition to scientific publications, also provides datasets\footnote{\url{https://ps.is.mpg.de/research_fields/datasets-and-code}} for e.g. pose estimation approaches. The listed datasets can be subdivided according to the following aspects:

\subsubsection{Clothing}
The individuality of a person is expressed by his clothing. To account for this feature, a model called \emph{CAPE} is built from 4d posture sequences of 8 men and 3 women. This dataset consists of about 80000 frames. In \emph{CAPE}, clothing is represented by additional offsets that depend on the pose $\theta$, the clothing type $c$ and a shape variation $z$ that shift the vertices of a human body model.To regress the parameters, displacement vertices are calculated from clothed minus unclothed body scans, which are passed to a neural encoder-decoder network.\cite{cape}
Further work by Ma et al. has generated a synthetic dataset on specific \emph{CAPE} subjects and published it as \emph{ReSynth} for researchers \cite{resynth}.


\subsubsection{Full-body}
Acquiring 3d scans and data from multiple people in an outdoor environment is challenging because the markers are difficult to track. Von Marcard et al. have shown in their publication that capturing sufficient data in a scene is possible with 6-17 \emph{IMU} units attached to each person, combined with a single hand-held camera. The recorded 51000 images are available for research.\cite{vip} A similar approach is followed by Huang et al. with 17 \emph{IMU} units for 10 subjects in 64 sequences, resulting in 330000 time instances \cite{dip}. Human-environment interaction is mainly covered in the datasets of Hassan et al. which consist of three parts in different scenes \cite{prox}. The \emph{GRAB} dataset, on the other hand, targets the relationship between full-body models and object manipulation. It contains motion data of 10 individuals interacting with 51 objects in 4 different contexts, e.g., lifting, transferring, hand-to-hand transfer, and using objects\cite{grab}. In contrast to \emph{GRAB}, the dataset from MÃ¼ller et al. includes all of a person's interactions with themselves \cite{tuch}.

\subsubsection{Hand scans}
The hand contributes to communication, e.g., the hand gesture is used to confirm a statement in conversation. To incorporate this expressiveness into existing full-body models, Romero et al. developed the \emph{MANO} model from approximately 1000 3d scans of 31 subjects in 51 poses. These scans showed female and male hands, both left and right, interacting with primitives. \cite{mano} 
Hasson et al. also published a synthetically generated hand dataset \emph{ObMan} that focuses on the manipulation of grasped primitives \cite{obman}.

\subsubsection{synthetic data}
A much more cost-effective approach is to create realistic body data from existing motion capture sources. An prominent example is \emph{SURREAL} by Varol et al. which consists of 6 million frames\cite{surreal}. In \cite{ltsh}, Hoffmann et al. investigate the impact of synthetic and synthetically augmented images on the capability of neural networks to generalize to real world data. They subsequently release their datasets, $\\mathbb{D}_{M}$ and $\mathbb{D}_{Style}$.

\subsubsection{Generalization of datasets}
Many different 3d scans are based on markers and motion capture software. Unfortunately, the number of markers varies from dataset to dataset, so their use as a data source for a body model leads to inaccuracies and further adjustments. A common solution to this problem is provided by the \emph{MoSh++} algorithm and its resulting \emph{AMASS} dataset, which unifies existing datasets in a common framework. It consists of 11265 motions from 344 subjects with 40 hours of content.\cite{amass}

\subsection{COCO dataset} 
According to Lin et al. the context of a given scene has an impact on the quality of the estimation. Therefore, images with many classifiable objects, such as animals, people, etc., are the content of the dataset to encode contextual information about their constellation and appearance in images. In addition, mainly non-iconic images of no centered objects are included. The dimension of \emph{COCO} is 328000 images, divided into 91 object categories with a total of 2500000 label instances. \cite{coco}

\subsection{Caesar dataset}
\cite{caesar}