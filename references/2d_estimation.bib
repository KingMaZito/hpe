
@inproceedings{bertasius_learning_2019,
	title = {Learning {Temporal} {Pose} {Estimation} from {Sparsely}-{Labeled} {Videos}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html},
	abstract = {Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames---a labeled Frame A and an unlabeled Frame B---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7\% mAP vs 83.8\% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows us to obtain state-of-the-art pose detection results on PoseTrack2017 and PoseTrack2018 datasets.},
	urldate = {2022-05-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bertasius, Gedas and Feichtenhofer, Christoph and Tran, Du and Shi, Jianbo and Torresani, Lorenzo},
	year = {2019},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/T94P9CHW/Bertasius et al. - 2019 - Learning Temporal Pose Estimation from Sparsely-La.pdf:application/pdf},
}

@inproceedings{wei_convolutional_2016,
	address = {Las Vegas, NV, USA},
	title = {Convolutional {Pose} {Machines}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780880/},
	doi = {10.1109/CVPR.2016.511},
	abstract = {Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly reﬁned estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difﬁculty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.},
	language = {en},
	urldate = {2022-05-18},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
	month = jun,
	year = {2016},
	pages = {4724--4732},
	file = {Wei et al. - 2016 - Convolutional Pose Machines.pdf:/Users/masoudtaghikhah/Zotero/storage/G9HXN9FI/Wei et al. - 2016 - Convolutional Pose Machines.pdf:application/pdf},
}

@inproceedings{liu_deep_2021,
	address = {Nashville, TN, USA},
	title = {Deep {Dual} {Consecutive} {Network} for {Human} {Pose} {Estimation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577714/},
	doi = {10.1109/CVPR46437.2021.00059},
	abstract = {Multi-frame human pose estimation in complicated situations is challenging. Although state-of-the-art human joints detectors have demonstrated remarkable results for static images, their performances come short when we apply these models to video sequences. Prevalent shortcomings include the failure to handle motion blur, video defocus, or pose occlusions, arising from the inability in capturing the temporal dependency among video frames. On the other hand, directly employing conventional recurrent neural networks incurs empirical difﬁculties in modeling spatial contexts, especially for dealing with pose occlusions. In this paper, we propose a novel multi-frame human pose estimation framework, leveraging abundant temporal cues between video frames to facilitate keypoint detection. Three modular components are designed in our framework. A Pose Temporal Merger encodes keypoint spatiotemporal context to generate effective searching scopes while a Pose Residual Fusion module computes weighted pose residuals in dual directions. These are then processed via our Pose Correction Network for efﬁcient reﬁning of pose estimations. Our method ranks No.1 in the Multi-frame Person Pose Estimation Challenge on the large-scale benchmark datasets PoseTrack2017 and PoseTrack2018. We have released our code, hoping to inspire future research.},
	language = {en},
	urldate = {2022-05-18},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Zhenguang and Chen, Haoming and Feng, Runyang and Wu, Shuang and Ji, Shouling and Yang, Bailin and Wang, Xun},
	month = jun,
	year = {2021},
	pages = {525--534},
	file = {Liu et al. - 2021 - Deep Dual Consecutive Network for Human Pose Estim.pdf:/Users/masoudtaghikhah/Zotero/storage/6EIWHW9Y/Liu et al. - 2021 - Deep Dual Consecutive Network for Human Pose Estim.pdf:application/pdf},
}

@inproceedings{luo_lstm_2018,
	title = {{LSTM} {Pose} {Machines}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_LSTM_Pose_Machines_CVPR_2018_paper.html},
	urldate = {2022-05-18},
	author = {Luo, Yue and Ren, Jimmy and Wang, Zhouxia and Sun, Wenxiu and Pan, Jinshan and Liu, Jianbo and Pang, Jiahao and Lin, Liang},
	year = {2018},
	pages = {5207--5215},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/FS2NT2W5/Luo et al. - 2018 - LSTM Pose Machines.pdf:application/pdf},
}

@inproceedings{yu_lite-hrnet_2021,
	address = {Nashville, TN, USA},
	title = {Lite-{HRNet}: {A} {Lightweight} {High}-{Resolution} {Network}},
	isbn = {978-1-66544-509-2},
	shorttitle = {Lite-{HRNet}},
	url = {https://ieeexplore.ieee.org/document/9578746/},
	doi = {10.1109/CVPR46437.2021.01030},
	abstract = {We present an efﬁcient high-resolution network, LiteHRNet, for human pose estimation. We start by simply applying the efﬁcient shufﬂe block in ShufﬂeNet to HRNet (high-resolution network), yielding stronger performance over popular lightweight networks, such as MobileNet, ShufﬂeNet, and Small HRNet.},
	language = {en},
	urldate = {2022-05-18},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yu, Changqian and Xiao, Bin and Gao, Changxin and Yuan, Lu and Zhang, Lei and Sang, Nong and Wang, Jingdong},
	month = jun,
	year = {2021},
	pages = {10435--10445},
	file = {Yu et al. - 2021 - Lite-HRNet A Lightweight High-Resolution Network.pdf:/Users/masoudtaghikhah/Zotero/storage/VL22JXLY/Yu et al. - 2021 - Lite-HRNet A Lightweight High-Resolution Network.pdf:application/pdf},
}

@inproceedings{jin_differentiable_2020,
	address = {Berlin, Heidelberg},
	title = {Differentiable {Hierarchical} {Graph} {Grouping} for {Multi}-person {Pose} {Estimation}},
	isbn = {978-3-030-58570-9},
	url = {https://doi.org/10.1007/978-3-030-58571-6_42},
	doi = {10.1007/978-3-030-58571-6_42},
	abstract = {Multi-person pose estimation is challenging because it localizes body keypoints for multiple persons simultaneously. Previous methods can be divided into two streams, i.e. top-down and bottom-up methods. The top-down methods localize keypoints after human detection, while the bottom-up methods localize keypoints directly and then cluster/group them for different persons, which are generally more efficient than top-down methods. However, in existing bottom-up methods, the keypoint grouping is usually solved independently from keypoint detection, making them not end-to-end trainable and have sub-optimal performance. In this paper, we investigate a new perspective of human part grouping and reformulate it as a graph clustering task. Especially, we propose a novel differentiable Hierarchical Graph Grouping (HGG) method to learn the graph grouping in bottom-up multi-person pose estimation task. Moreover, HGG is easily embedded into main-stream bottom-up methods. It takes human keypoint candidates as graph nodes and clusters keypoints in a multi-layer graph neural network model. The modules of HGG can be trained end-to-end with the keypoint detection network and is able to supervise the grouping process in a hierarchical manner. To improve the discrimination of the clustering, we add a set of edge discriminators and macro-node discriminators. Extensive experiments on both COCO and OCHuman datasets demonstrate that the proposed method improves the performance of bottom-up pose estimation methods.},
	urldate = {2022-05-18},
	booktitle = {Computer {Vision} – {ECCV} 2020: 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23–28, 2020, {Proceedings}, {Part} {VII}},
	publisher = {Springer-Verlag},
	author = {Jin, Sheng and Liu, Wentao and Xie, Enze and Wang, Wenhai and Qian, Chen and Ouyang, Wanli and Luo, Ping},
	month = aug,
	year = {2020},
	keywords = {Human pose estimation, Graph neural network, Grouping},
	pages = {718--734},
	file = {Submitted Version:/Users/masoudtaghikhah/Zotero/storage/H2V7X3LF/Jin et al. - 2020 - Differentiable Hierarchical Graph Grouping for Mul.pdf:application/pdf},
}

@inproceedings{geng_bottom-up_2021,
	title = {Bottom-{Up} {Human} {Pose} {Estimation} via {Disentangled} {Keypoint} {Regression}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Geng_Bottom-Up_Human_Pose_Estimation_via_Disentangled_Keypoint_Regression_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-05-18},
	author = {Geng, Zigang and Sun, Ke and Xiao, Bin and Zhang, Zhaoxiang and Wang, Jingdong},
	year = {2021},
	pages = {14676--14686},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/JSXS3427/Geng et al. - 2021 - Bottom-Up Human Pose Estimation via Disentangled K.pdf:application/pdf},
}

@inproceedings{nie_single-stage_2019,
	title = {Single-{Stage} {Multi}-{Person} {Pose} {Machines}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Single-Stage_Multi-Person_Pose_Machines_ICCV_2019_paper.html},
	urldate = {2022-05-18},
	author = {Nie, Xuecheng and Feng, Jiashi and Zhang, Jianfeng and Yan, Shuicheng},
	year = {2019},
	pages = {6951--6960},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/C9XPNLU5/Nie et al. - 2019 - Single-Stage Multi-Person Pose Machines.pdf:application/pdf},
}

@inproceedings{kreiss_pifpaf_2019,
	title = {{PifPaf}: {Composite} {Fields} for {Human} {Pose} {Estimation}},
	shorttitle = {{PifPaf}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Kreiss_PifPaf_Composite_Fields_for_Human_Pose_Estimation_CVPR_2019_paper.html},
	urldate = {2022-05-18},
	author = {Kreiss, Sven and Bertoni, Lorenzo and Alahi, Alexandre},
	year = {2019},
	pages = {11977--11986},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/QQ79RUQE/Kreiss et al. - 2019 - PifPaf Composite Fields for Human Pose Estimation.pdf:application/pdf;Snapshot:/Users/masoudtaghikhah/Zotero/storage/6SBJWHYA/Kreiss_PifPaf_Composite_Fields_for_Human_Pose_Estimation_CVPR_2019_paper.html:text/html},
}

@inproceedings{zhang_distribution-aware_2020,
	title = {Distribution-{Aware} {Coordinate} {Representation} for {Human} {Pose} {Estimation}},
	doi = {10.1109/CVPR42600.2020.00712},
	abstract = {While being the de facto standard coordinate representation for human pose estimation, heatmap has not been investigated in-depth. This work fills this gap. For the first time, we find that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for the performance. We further probe the design limitations of the standard coordinate decoding method, and propose a more principled distributionaware decoding method. Also, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating unbiased/accurate heatmaps. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoints (DARK) method. Serving as a model-agnostic plug-in, DARK brings about significant performance boost to existing human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO. Besides, DARK achieves the 2nd place entry in the ICCV 2019 COCO Keypoints Challenge. The code is available online.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Feng and Zhu, Xiatian and Dai, Hanbin and Ye, Mao and Zhu, Ce},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {Decoding, Encoding, Image resolution, Pose estimation, Space heating, Standards},
	pages = {7091--7100},
	annote = {leverage heatmap based approches, categorize in regres and heat map
},
	file = {IEEE Xplore Abstract Record:/Users/masoudtaghikhah/Zotero/storage/JA7XCUM4/9157744.html:text/html;IEEE Xplore Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/GMUF64Q3/Zhang et al. - 2020 - Distribution-Aware Coordinate Representation for H.pdf:application/pdf},
}

@inproceedings{rafi_efficient_2016,
	address = {York, UK},
	title = {An {Efficient} {Convolutional} {Network} for {Human} {Pose} {Estimation}},
	isbn = {978-1-901725-59-9},
	url = {http://www.bmva.org/bmvc/2016/papers/paper109/index.html},
	doi = {10.5244/C.30.109},
	abstract = {In recent years, human pose estimation has greatly beneﬁted from deep learning and huge gains in performance have been achieved. However, to push for maximum performance recent approaches exploit computationally expensive deep network architectures, train on multiple datasets, apply additional post-processing and provide limited details about used design choices. This makes it hard not only to compare different methods and but also to reproduce existing results .},
	language = {en},
	urldate = {2022-06-08},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2016},
	publisher = {British Machine Vision Association},
	author = {Rafi, Umer and Leibe, Bastian and Gall, Juergen and Kostrikov, Ilya},
	year = {2016},
	pages = {109.1--109.11},
	file = {Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:/Users/masoudtaghikhah/Zotero/storage/U4EXRS7T/Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:application/pdf},
}

@inproceedings{micilotta_real-time_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Real-{Time} {Upper} {Body} {Detection} and {3D} {Pose} {Estimation} in {Monoscopic} {Images}},
	isbn = {978-3-540-33837-6},
	doi = {10.1007/11744078_11},
	abstract = {This paper presents a novel solution to the difficult task of both detecting and estimating the 3D pose of humans in monoscopic images. The approach consists of two parts. Firstly the location of a human is identified by a probabalistic assembly of detected body parts. Detectors for the face, torso and hands are learnt using adaBoost. A pose likliehood is then obtained using an a priori mixture model on body configuration and possible configurations assembled from available evidence using RANSAC. Once a human has been detected, the location is used to initialise a matching algorithm which matches the silhouette and edge map of a subject with a 3D model. This is done efficiently using chamfer matching, integral images and pose estimation from the initial detection stage. We demonstrate the application of the approach to large, cluttered natural images and at near framerate operation (16fps) on lower resolution video streams.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2006},
	publisher = {Springer},
	author = {Micilotta, Antonio S. and Ong, Eng-Jon and Bowden, Richard},
	editor = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
	year = {2006},
	keywords = {Body Part, Face Detection, False Detection, Integral Image, Mixture Model},
	pages = {139--150},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/V39PEPN9/Micilotta et al. - 2006 - Real-Time Upper Body Detection and 3D Pose Estimat.pdf:application/pdf},
}

@article{drrksrra_note_nodate,
	title = {A {Note} on {Two} {Problemsin} {Connexionwith} {Graphs}},
	language = {en},
	author = {DrrKsrRA, E W},
	pages = {3},
	file = {DrrKsrRA - A Note on Two Problemsin Connexionwith Graphs.pdf:/Users/masoudtaghikhah/Zotero/storage/ZW9IKWA8/DrrKsrRA - A Note on Two Problemsin Connexionwith Graphs.pdf:application/pdf},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2022-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/LVKRB34Q/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@incollection{bulat_human_2016,
	title = {Human pose estimation via {Convolutional} {Part} {Heatmap} {Regression}},
	volume = {9911},
	url = {http://arxiv.org/abs/1609.01743},
	abstract = {This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from http://www.cs.nott.ac.uk/{\textasciitilde}psxab5/},
	urldate = {2022-06-09},
	author = {Bulat, Adrian and Tzimiropoulos, Georgios},
	year = {2016},
	doi = {10.1007/978-3-319-46478-7_44},
	note = {arXiv:1609.01743 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {717--732},
	annote = {Comment: accepted to ECCV 2016},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/9PAICWEW/Bulat and Tzimiropoulos - 2016 - Human pose estimation via Convolutional Part Heatm.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/SF9IW7E7/1609.html:text/html},
}

@inproceedings{toshev_deeppose_2014,
	title = {{DeepPose}: {Human} {Pose} {Estimation} via {Deep} {Neural} {Networks}},
	shorttitle = {{DeepPose}},
	url = {http://arxiv.org/abs/1312.4659},
	doi = {10.1109/CVPR.2014.214},
	abstract = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-of-art or better performance on four academic benchmarks of diverse real-world images.},
	urldate = {2022-06-09},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Toshev, Alexander and Szegedy, Christian},
	month = jun,
	year = {2014},
	note = {arXiv:1312.4659 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1653--1660},
	annote = {Comment: IEEE Conference on Computer Vision and Pattern Recognition, 2014},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/E2KS2PYP/Toshev and Szegedy - 2014 - DeepPose Human Pose Estimation via Deep Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/APF25BZY/1312.html:text/html},
}

@article{poppe_vision-based_2007,
	title = {Vision-based human motion analysis: {An} overview},
	volume = {108},
	shorttitle = {Vision-based human motion analysis},
	doi = {10.1016/j.cviu.2006.10.016},
	abstract = {Abstract Markerless vision-based human motion analysis has the potential to provide an inexpensive, non-obtrusive solution for the estimation of body poses. The significant research effort in this domain has been motivated by the fact that many application areas, including sur- veillance, Human–Computer Interaction and automatic annotation, will benefit from a robust solution. In this paper, we discuss the characteristics of human,motion,analysis. We divide the analysis into a modeling,and an estimation phase. Modeling is the construction of the likelihood function, estimation is concerned with finding the most likely pose given the likelihood surface. We discuss model-free approaches,separately. This taxonomy,allows us to highlight trends in the domain,and to point out limitations of the current state of the art. 2007 Elsevier Inc. All rights reserved. Keywords: Human,motion,analysis; Pose estimation; Computer,vision},
	journal = {Computer Vision and Image Understanding},
	author = {Poppe, Ronald},
	month = oct,
	year = {2007},
	pages = {4--18},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Complexity theory, Degradation, Image recognition, Image segmentation, Neural networks, Training, Visualization},
	pages = {770--778},
	file = {IEEE Xplore Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/BNEE6WQM/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{liu_cascaded_2018,
	title = {A {Cascaded} {Inception} of {Inception} {Network} {With} {Attention} {Modulated} {Feature} {Fusion} for {Human} {Pose} {Estimation}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12334},
	doi = {10.1609/aaai.v32i1.12334},
	abstract = {Accurate keypoint localization of human pose needs diversified features: the high level for contextual dependencies and the low level for detailed refinement of joints. However, the importance of the two factors varies from case to case, but how to efficiently use the features is still an open problem. Existing methods have limitations in preserving low level features, adaptively adjusting the importance of different levels of features, and modeling the human perception process. This paper presents three novel techniques step by step to efficiently utilize different levels of features for human pose estimation. Firstly, an inception of inception (IOI) block is designed to emphasize the low level features. Secondly, an attention mechanism is proposed to adjust the importance of individual levels according to the context. Thirdly, a cascaded network is proposed to sequentially localize the joints to enforce message passing from joints of stand-alone parts like head and torso to remote joints like wrist or ankle. Experimental results demonstrate that the proposed method achieves the state-of-the-art performance on both MPII and LSP benchmarks.},
	language = {en},
	number = {1},
	urldate = {2022-06-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Wentao and Chen, Jie and Li, Cheng and Qian, Chen and Chu, Xiao and Hu, Xiaolin},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Cascade Joint Network},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/RSXXVM44/Liu et al. - 2018 - A Cascaded Inception of Inception Network With Att.pdf:application/pdf},
}

@inproceedings{sun_deep_2019,
	address = {Long Beach, CA, USA},
	title = {Deep {High}-{Resolution} {Representation} {Learning} for {Human} {Pose} {Estimation}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953615/},
	doi = {10.1109/CVPR.2019.00584},
	abstract = {In this paper, we are interested in the human pose estimation problem with a focus on learning reliable highresolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
	month = jun,
	year = {2019},
	pages = {5686--5696},
	file = {Sun et al. - 2019 - Deep High-Resolution Representation Learning for H.pdf:/Users/masoudtaghikhah/Zotero/storage/ZTVNHD96/Sun et al. - 2019 - Deep High-Resolution Representation Learning for H.pdf:application/pdf},
}

@incollection{ferrari_deeply_2018,
	address = {Cham},
	title = {Deeply {Learned} {Compositional} {Models} for {Human} {Pose} {Estimation}},
	volume = {11207},
	isbn = {978-3-030-01218-2 978-3-030-01219-9},
	url = {http://link.springer.com/10.1007/978-3-030-01219-9_12},
	abstract = {Compositional models represent patterns with hierarchies of meaningful parts and subparts. Their ability to characterize high-order relationships among body parts helps resolve low-level ambiguities in human pose estimation (HPE). However, prior compositional models make unrealistic assumptions on subpart-part relationships, making them incapable to characterize complex compositional patterns. Moreover, state spaces of their higher-level parts can be exponentially large, complicating both inference and learning. To address these issues, this paper introduces a novel framework, termed as Deeply Learned Compositional Model (DLCM), for HPE. It exploits deep neural networks to learn the compositionality of human bodies. This results in a novel network with a hierarchical compositional architecture and bottom-up/top-down inference stages. In addition, we propose a novel bone-based part representation. It not only compactly encodes orientations, scales and shapes of parts, but also avoids their potentially large state spaces. With signiﬁcantly lower complexities, our approach outperforms state-of-the-art methods on three benchmark datasets.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Tang, Wei and Yu, Pei and Wu, Ying},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01219-9_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {197--214},
	file = {Tang et al. - 2018 - Deeply Learned Compositional Models for Human Pose.pdf:/Users/masoudtaghikhah/Zotero/storage/FGRUK2XB/Tang et al. - 2018 - Deeply Learned Compositional Models for Human Pose.pdf:application/pdf},
}

@article{eichner_human_2012,
	title = {Human {Pose} {Co}-{Estimation} and {Applications}},
	doi = {10.1109/TPAMI.2012.85},
	abstract = {It is shown that PCE improves pose estimation accuracy over estimating each person independently, and it learns meaningful prototypes which can be used as priors for pose estimation in novel images. Most existing techniques for articulated Human Pose Estimation (HPE) consider each person independently. Here we tackle the problem in a new setting, coined Human Pose Coestimation (PCE), where multiple people are in a common, but unknown pose. The task of PCE is to estimate their poses jointly and to produce prototypes characterizing the shared pose. Since the poses of the individual people should be similar to the prototype, PCE has less freedom compared to estimating each pose independently, which simplifies the problem. We demonstrate our PCE technique on two applications. The first is estimating the pose of people performing the same activity synchronously, such as during aerobics, cheerleading, and dancing in a group. We show that PCE improves pose estimation accuracy over estimating each person independently. The second application is learning prototype poses characterizing a pose class directly from an image search engine queried by the class name (e.g., “lotus pose”). We show that PCE leads to better pose estimation in such images, and it learns meaningful prototypes which can be used as priors for pose estimation in novel images.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Eichner, M. and Ferrari, V.},
	year = {2012},
	file = {Accepted Version:/Users/masoudtaghikhah/Zotero/storage/9QDAR8RM/Eichner and Ferrari - 2012 - Human Pose Co-Estimation and Applications.pdf:application/pdf},
}

@article{felzenszwalb_pictorial_2005,
	title = {Pictorial {Structures} for {Object} {Recognition}},
	volume = {61},
	issn = {0920-5691},
	url = {http://link.springer.com/10.1023/B:VISI.0000042934.15159.49},
	doi = {10.1023/B:VISI.0000042934.15159.49},
	abstract = {In this paper we present a computationally eﬃcient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable conﬁguration. The appearance of each part is modeled separately, and the deformable conﬁguration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to ﬁnd instances of an object in an image as well as the problem of learning an object model from training examples, presenting eﬃcient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.},
	language = {en},
	number = {1},
	urldate = {2022-06-09},
	journal = {International Journal of Computer Vision},
	author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	month = jan,
	year = {2005},
	pages = {55--79},
	file = {Felzenszwalb and Huttenlocher - 2005 - Pictorial Structures for Object Recognition.pdf:/Users/masoudtaghikhah/Zotero/storage/ZZ4LG9P3/Felzenszwalb and Huttenlocher - 2005 - Pictorial Structures for Object Recognition.pdf:application/pdf},
}

@inproceedings{yixing_gao_user_2015,
	address = {Hamburg, Germany},
	title = {User modelling for personalised dressing assistance by humanoid robots},
	isbn = {978-1-4799-9994-1},
	url = {http://ieeexplore.ieee.org/document/7353617/},
	doi = {10.1109/IROS.2015.7353617},
	abstract = {Assistive robots can improve the well-being of disabled or frail human users by reducing the burden that activities of daily living impose on them. To enable personalised assistance, such robots beneﬁt from building a user-speciﬁc model, so that the assistance is customised to the particular set of user abilities. In this paper, we present an end-to-end approach for home-environment assistive humanoid robots to provide personalised assistance through a dressing application for users who have upper-body movement limitations. We use randomised decision forests to estimate the upper-body pose of users captured by a top-view depth camera, and model the movement space of upper-body joints using Gaussian mixture models. The movement space of each upper-body joint consists of regions with different reaching capabilities. We propose a method which is based on real-time upper-body pose and user models to plan robot motions for assistive dressing. We validate each part of our approach and test the whole system, allowing a Baxter humanoid robot to assist human to wear a sleeveless jacket.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {{Yixing Gao} and {Hyung Jin Chang} and Demiris, Yiannis},
	month = sep,
	year = {2015},
	pages = {1840--1845},
	file = {Yixing Gao et al. - 2015 - User modelling for personalised dressing assistanc.pdf:/Users/masoudtaghikhah/Zotero/storage/K77WIS5W/Yixing Gao et al. - 2015 - User modelling for personalised dressing assistanc.pdf:application/pdf},
}

@misc{noauthor_cs231n_nodate,
	title = {{CS231n} {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {https://cs231n.github.io/convolutional-networks/},
	urldate = {2022-06-09},
	file = {CS231n Convolutional Neural Networks for Visual Recognition:/Users/masoudtaghikhah/Zotero/storage/KHAJNU6W/convolutional-networks.html:text/html},
}

@inproceedings{ramakrishna_pose_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pose {Machines}: {Articulated} {Pose} {Estimation} via {Inference} {Machines}},
	isbn = {978-3-319-10605-2},
	shorttitle = {Pose {Machines}},
	doi = {10.1007/978-3-319-10605-2_3},
	abstract = {State-of-the-art approaches for articulated human pose estimation are rooted in parts-based graphical models. These models are often restricted to tree-structured representations and simple parametric potentials in order to enable tractable inference. However, these simple dependencies fail to capture all the interactions between body parts. While models with more complex interactions can be defined, learning the parameters of these models remains challenging with intractable or approximate inference. In this paper, instead of performing inference on a learned graphical model, we build upon the inference machine framework and present a method for articulated human pose estimation. Our approach incorporates rich spatial interactions among multiple parts and information across parts of different scales. Additionally, the modular framework of our approach enables both ease of implementation without specialized optimization solvers, and efficient inference. We analyze our approach on two challenging datasets with large pose variation and outperform the state-of-the-art on these benchmarks.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Ramakrishna, Varun and Munoz, Daniel and Hebert, Martial and Andrew Bagnell, James and Sheikh, Yaser},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Approximate Inference, Composite Part, Context Feature, Pictorial Structure, Random Forest},
	pages = {33--47},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/T3MJIZQN/Ramakrishna et al. - 2014 - Pose Machines Articulated Pose Estimation via Inf.pdf:application/pdf},
}

@inproceedings{yixing_gao_user_2015-1,
	address = {Hamburg, Germany},
	title = {User modelling for personalised dressing assistance by humanoid robots},
	isbn = {978-1-4799-9994-1},
	url = {http://ieeexplore.ieee.org/document/7353617/},
	doi = {10.1109/IROS.2015.7353617},
	abstract = {Assistive robots can improve the well-being of disabled or frail human users by reducing the burden that activities of daily living impose on them. To enable personalised assistance, such robots beneﬁt from building a user-speciﬁc model, so that the assistance is customised to the particular set of user abilities. In this paper, we present an end-to-end approach for home-environment assistive humanoid robots to provide personalised assistance through a dressing application for users who have upper-body movement limitations. We use randomised decision forests to estimate the upper-body pose of users captured by a top-view depth camera, and model the movement space of upper-body joints using Gaussian mixture models. The movement space of each upper-body joint consists of regions with different reaching capabilities. We propose a method which is based on real-time upper-body pose and user models to plan robot motions for assistive dressing. We validate each part of our approach and test the whole system, allowing a Baxter humanoid robot to assist human to wear a sleeveless jacket.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {{Yixing Gao} and {Hyung Jin Chang} and Demiris, Yiannis},
	month = sep,
	year = {2015},
	pages = {1840--1845},
	file = {Yixing Gao et al. - 2015 - User modelling for personalised dressing assistanc.pdf:/Users/masoudtaghikhah/Zotero/storage/W477VRQY/Yixing Gao et al. - 2015 - User modelling for personalised dressing assistanc.pdf:application/pdf},
}

@inproceedings{luo_rethinking_2021,
	address = {Nashville, TN, USA},
	title = {Rethinking the {Heatmap} {Regression} for {Bottom}-up {Human} {Pose} {Estimation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578397/},
	doi = {10.1109/CVPR46437.2021.01306},
	abstract = {Heatmap regression has become the most prevalent choice for nowadays human pose estimation methods. The ground-truth heatmaps are usually constructed via covering all skeletal keypoints by 2D gaussian kernels. The standard deviations of these kernels are ﬁxed. However, for bottom-up methods, which need to handle a large variance of human scales and labeling ambiguities, the current practice seems unreasonable. To better cope with these problems, we propose the scale-adaptive heatmap regression (SAHR) method, which can adaptively adjust the standard deviation for each keypoint. In this way, SAHR is more tolerant of various human scales and labeling ambiguities. However, SAHR may aggravate the imbalance between fore-background samples, which potentially hurts the improvement of SAHR. Thus, we further introduce the weight-adaptive heatmap regression (WAHR) to help balance the fore-background samples. Extensive experiments show that SAHR together with WAHR largely improves the accuracy of bottom-up human pose estimation. As a result, we ﬁnally outperform the state-of-the-art model by +1.5AP and achieve 72.0AP on COCO test-dev2017, which is comparable with the performances of most top-down methods. Source codes are available at https://github.com/ greatlog/SWAHR-HumanPose.},
	language = {en},
	urldate = {2022-06-13},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Luo, Zhengxiong and Wang, Zhicheng and Huang, Yan and Wang, Liang and Tan, Tieniu and Zhou, Erjin},
	month = jun,
	year = {2021},
	pages = {13259--13268},
	file = {Luo et al. - 2021 - Rethinking the Heatmap Regression for Bottom-up Hu.pdf:/Users/masoudtaghikhah/Zotero/storage/CXJW48CU/Luo et al. - 2021 - Rethinking the Heatmap Regression for Bottom-up Hu.pdf:application/pdf},
}

@inproceedings{zuffi_pictorial_2012,
	title = {From {Pictorial} {Structures} to {Deformable} {Structures}},
	doi = {10.1109/CVPR.2012.6248098},
	abstract = {Pictorial Structures (PS) define a probabilistic model of 2D articulated objects in images. Typical PS models assume an object can be represented by a set of rigid parts connected with pairwise constraints that define the prior probability of part configurations. These models are widely used to represent non-rigid articulated objects such as humans and animals despite the fact that such objects have parts that deform non-rigidly. Here we define a new Deformable Structures (DS) model that is a natural extension of previous PS models and that captures the non-rigid shape deformation of the parts. Each part in a DS model is represented by a low-dimensional shape deformation space and pairwise potentials between parts capture how the shape varies with pose and the shape of neighboring parts. A key advantage of such a model is that it more accurately models object boundaries. This enables image likelihood models that are more discriminative than previous PS likelihoods. This likelihood is learned using training imagery annotated using a DS “puppet.” We focus on a human DS model learned from 2D projections of a realistic 3D human body model and use it to infer human poses in images using a form of non-parametric belief propagation.},
	author = {Zuffi, Silvia and Freifeld, O. and Black, Michael},
	month = jun,
	year = {2012},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/VRPSIPI7/Zuffi et al. - 2012 - From Pictorial Structures to Deformable Structures.pdf:application/pdf},
}

@article{chen_monocular_2020,
	title = {Monocular {Human} {Pose} {Estimation}: {A} {Survey} of {Deep} {Learning}-based {Methods}},
	volume = {192},
	issn = {10773142},
	shorttitle = {Monocular {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2006.01423},
	doi = {10.1016/j.cviu.2019.102897},
	abstract = {Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision, aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics, performance comparison, and discusses some promising future research directions.},
	urldate = {2022-06-14},
	journal = {Computer Vision and Image Understanding},
	author = {Chen, Yucheng and Tian, Yingli and He, Mingyi},
	month = mar,
	year = {2020},
	note = {arXiv:2006.01423 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {102897},
	annote = {Comment: This version corresponds to the pre-print of the paper accepted for Computer Vision and Image Understanding (CVIU)},
	annote = {pose images
},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/H6K8EBKS/Chen et al. - 2020 - Monocular Human Pose Estimation A Survey of Deep .pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/8UE8VQ6R/2006.html:text/html},
}

@inproceedings{insafutdinov_arttrack_2017,
	address = {Honolulu, HI},
	title = {{ArtTrack}: {Articulated} {Multi}-{Person} {Tracking} in the {Wild}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{ArtTrack}},
	url = {http://ieeexplore.ieee.org/document/8099625/},
	doi = {10.1109/CVPR.2017.142},
	abstract = {In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by ofﬂoading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public “MPII Human Pose” benchmark and on a new “MPII Video Pose” dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes1.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Insafutdinov, Eldar and Andriluka, Mykhaylo and Pishchulin, Leonid and Tang, Siyu and Levinkov, Evgeny and Andres, Bjoern and Schiele, Bernt},
	month = jul,
	year = {2017},
	pages = {1293--1301},
	file = {Insafutdinov et al. - 2017 - ArtTrack Articulated Multi-Person Tracking in the.pdf:/Users/masoudtaghikhah/Zotero/storage/PZWTUXY7/Insafutdinov et al. - 2017 - ArtTrack Articulated Multi-Person Tracking in the.pdf:application/pdf},
}

@inproceedings{insafutdinov_deepercut_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeeperCut}: {A} {Deeper}, {Stronger}, and {Faster} {Multi}-person {Pose} {Estimation} {Model}},
	isbn = {978-3-319-46466-4},
	shorttitle = {{DeeperCut}},
	doi = {10.1007/978-3-319-46466-4_3},
	abstract = {The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation (Models and code available at http://pose.mpi-inf.mpg.de).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Insafutdinov, Eldar and Pishchulin, Leonid and Andres, Bjoern and Andriluka, Mykhaylo and Schiele, Bernt},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Area Under Curve, Body Part, Conv4 Bank, Integer Linear Programming, Part Detector},
	pages = {34--50},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/JW88JK9U/Insafutdinov et al. - 2016 - DeeperCut A Deeper, Stronger, and Faster Multi-pe.pdf:application/pdf},
}

@inproceedings{nie_human_2018,
	title = {Human {Pose} {Estimation} {With} {Parsing} {Induced} {Learner}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Nie_Human_Pose_Estimation_CVPR_2018_paper.html},
	urldate = {2022-06-14},
	author = {Nie, Xuecheng and Feng, Jiashi and Zuo, Yiming and Yan, Shuicheng},
	year = {2018},
	pages = {2100--2108},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/MRMA69AN/Nie et al. - 2018 - Human Pose Estimation With Parsing Induced Learner.pdf:application/pdf;Snapshot:/Users/masoudtaghikhah/Zotero/storage/LLCDB8KV/Nie_Human_Pose_Estimation_CVPR_2018_paper.html:text/html},
}

@incollection{ferrari_pose_2018,
	address = {Cham},
	title = {Pose {Partition} {Networks} for {Multi}-person {Pose} {Estimation}},
	volume = {11209},
	isbn = {978-3-030-01227-4 978-3-030-01228-1},
	url = {http://link.springer.com/10.1007/978-3-030-01228-1_42},
	abstract = {This paper proposes a novel Pose Partition Network (PPN) to address the challenging multi-person pose estimation problem. The proposed PPN is favorably featured by low complexity and high accuracy of joint detection and partition. In particular, PPN performs dense regressions from global joint candidates within a speciﬁc embedding space, which is parameterized by centroids of persons, to eﬃciently generate robust person detection and joint partition. Then, PPN infers body joint conﬁgurations through conducting graph partition for each person detection locally, utilizing reliable global aﬃnity cues. In this way, PPN reduces computation complexity and improves multi-person pose estimation signiﬁcantly. We implement PPN with the Hourglass architecture as the backbone network to simultaneously learn joint detector and dense regressor. Extensive experiments on benchmarks MPII Human Pose Multi-Person, extended PASCAL-Person-Part, and WAF show the eﬃciency of PPN with new state-of-the-art performance.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Nie, Xuecheng and Feng, Jiashi and Xing, Junliang and Yan, Shuicheng},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01228-1_42},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {705--720},
	file = {Nie et al. - 2018 - Pose Partition Networks for Multi-person Pose Esti.pdf:/Users/masoudtaghikhah/Zotero/storage/UNJF8EX4/Nie et al. - 2018 - Pose Partition Networks for Multi-person Pose Esti.pdf:application/pdf},
}

@inproceedings{martinez_single-network_2019,
	address = {Seoul, Korea (South)},
	title = {Single-{Network} {Whole}-{Body} {Pose} {Estimation}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010752/},
	doi = {10.1109/ICCV.2019.00708},
	abstract = {We present the ﬁrst single-network approach for 2D whole-body pose estimation, which entails simultaneous localization of body, face, hands, and feet keypoints. Due to the bottom-up formulation, our method maintains constant real-time performance regardless of the number of people in the image. The network is trained in a single stage using multi-task learning, through an improved architecture which can handle scale differences between body/foot and face/hand keypoints. Our approach considerably improves upon OpenPose [9], the only work so far capable of wholebody pose estimation, both in terms of speed and global accuracy. Unlike [9], our method does not need to run an additional network for each hand and face candidate, making it substantially faster for multi-person scenarios. This work directly results in a reduction of computational complexity for applications that require 2D whole-body information (e.g., VR/AR, re-targeting). In addition, it yields higher accuracy, especially for occluded, blurry, and low resolution faces and hands. For code, trained models, and validation benchmarks, visit our project page1.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Martinez, Gines Hidalgo and Raaj, Yaadhav and Idrees, Haroon and Xiang, Donglai and Joo, Hanbyul and Simon, Tomas and Sheikh, Yaser},
	month = oct,
	year = {2019},
	pages = {6981--6990},
	file = {Martinez et al. - 2019 - Single-Network Whole-Body Pose Estimation.pdf:/Users/masoudtaghikhah/Zotero/storage/8PZJSKZZ/Martinez et al. - 2019 - Single-Network Whole-Body Pose Estimation.pdf:application/pdf},
}

@article{zhu_multi-person_nodate,
	title = {Multi-{Person} {Pose} {Estimation} for {PoseTrack} with {Enhanced} {Part} {Affinity} {Fields}},
	abstract = {This paper is a description for method we adopted in the competition of “PoseTrack, ICCV 2017 workshop” [1]. We presents an improved approach based on Part Affinity Fields (PAFs) [2]. To achieve a better performance on PoseTrack benchmark, several modifications are proposed, including pre-training model on COCO [3], rethinking the network structure and redundant PAFs. As a result, the framework obtains a significant improvement comparing to baseline methods. Moreover, inspired by semantic segmentation, we conduct some experiments using the hole algorithm and DenseNet, which achieves a desirable performance. Our submission achieves 72.5\% mAP on PoseTrack validation dataset and 68.3\% on Posetrack benchmark.},
	language = {en},
	author = {Zhu, Xiangyu and Jiang, Yingying and Luo, Zhenbo},
	pages = {4},
	file = {Zhu et al. - Multi-Person Pose Estimation for PoseTrack with En.pdf:/Users/masoudtaghikhah/Zotero/storage/Z4Y7TZWU/Zhu et al. - Multi-Person Pose Estimation for PoseTrack with En.pdf:application/pdf},
}

@article{feng_ning_toward_2005,
	title = {Toward automatic phenotyping of developing embryos from videos},
	volume = {14},
	issn = {1057-7149},
	url = {http://ieeexplore.ieee.org/document/1495508/},
	doi = {10.1109/TIP.2005.852470},
	abstract = {We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully-automated phenotyping system. The system contains three modules (1) a convolutional network trained to classify each pixel into ﬁve categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; (2) an Energy-Based Model which cleans up the output of the convolutional network by learning local consistency constraints that must be satisﬁed by label images; (3) A set of elastic models of the embryo at various stages of development that are matched to the label images.},
	language = {en},
	number = {9},
	urldate = {2022-06-15},
	journal = {IEEE Transactions on Image Processing},
	author = {{Feng Ning} and Delhomme, D. and LeCun, Y. and Piano, F. and Bottou, L. and Barbano, P.E.},
	month = sep,
	year = {2005},
	pages = {1360--1371},
	file = {Feng Ning et al. - 2005 - Toward automatic phenotyping of developing embryos.pdf:/Users/masoudtaghikhah/Zotero/storage/XX6IF79D/Feng Ning et al. - 2005 - Toward automatic phenotyping of developing embryos.pdf:application/pdf},
}

@misc{li_markov_1994,
	title = {Markov {Random} {Field} {Models} in {Computer} {Vision}},
	abstract = {. A variety of computer vision problems can be optimally posed as Bayesian labeling in which the solution of a problem is defined as the maximum a posteriori (MAP) probability estimate of the true labeling. The posterior probability is usually derived from a prior model and a likelihood model. The latter relates to how data is observed and is problem domain dependent. The former depends on how various prior constraints are expressed. Markov Random Field Models (MRF) theory is a tool to encode contextual constraints into the prior probability. This paper presents a unified approach for MRF modeling in low and high level computer vision. The unification is made possible due to a recent advance in MRF modeling for high level object recognition. Such unification provides a systematic approach for vision modeling based on sound mathematical principles. 1 Introduction  Since its beginning in early 1960's, computer vision research has been evolving from heuristic design of algorithms to syste...},
	author = {Li, S. Z.},
	year = {1994},
	file = {Citeseer - Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/XNH93W6F/Li - 1994 - Markov Random Field Models in Computer Vision.pdf:application/pdf;Citeseer - Snapshot:/Users/masoudtaghikhah/Zotero/storage/KSHWSSS5/summary.html:text/html},
}

@inproceedings{chen_articulated_2014,
	title = {Articulated {Pose} {Estimation} by a {Graphical} {Model} with {Image} {Dependent} {Pairwise} {Relations}},
	volume = {27},
	url = {https://proceedings.neurips.cc/paper/2014/hash/8b6dd7db9af49e67306feb59a8bdc52c-Abstract.html},
	abstract = {We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.},
	urldate = {2022-06-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Xianjie and Yuille, Alan L},
	year = {2014},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/Z8HLPAAG/Chen and Yuille - 2014 - Articulated Pose Estimation by a Graphical Model w.pdf:application/pdf},
}

@inproceedings{newell_associative_2017,
	title = {Associative {Embedding}: {End}-to-{End} {Learning} for {Joint} {Detection} and {Grouping}},
	volume = {30},
	shorttitle = {Associative {Embedding}},
	url = {https://papers.nips.cc/paper/2017/hash/8edd72158ccd2a879f79cb2538568fdc-Abstract.html},
	abstract = {We introduce associative embedding, a novel method for supervising convolutional neural networks for the task of detection and grouping. A number of computer vision problems can be framed in this manner including multi-person pose estimation, instance segmentation, and multi-object tracking. Usually the grouping of detections is achieved with multi-stage pipelines, instead we propose an approach that teaches a network to simultaneously output detections and group assignments. This technique can be easily integrated into any state-of-the-art network architecture that  produces pixel-wise predictions. We show how to apply this method to  multi-person pose estimation and report state-of-the-art performance  on the MPII and MS-COCO datasets.},
	urldate = {2022-06-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Newell, Alejandro and Huang, Zhiao and Deng, Jia},
	year = {2017},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/JKN6XKYS/Newell et al. - 2017 - Associative Embedding End-to-End Learning for Joi.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-06-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/NN45BLWS/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@inproceedings{chen_cascaded_2018,
	address = {Salt Lake City, UT},
	title = {Cascaded {Pyramid} {Network} for {Multi}-person {Pose} {Estimation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578840/},
	doi = {10.1109/CVPR.2018.00742},
	abstract = {The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these “hard” keypoints. More speciﬁcally, our algorithm includes two stages: GlobalNet and ReﬁneNet. GlobalNet is a feature pyramid network which can successfully localize the “simple” keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our ReﬁneNet tries explicitly handling the “hard” keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to ﬁrst generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-ofart results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19\% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code1 and the detection results for person used will be publicly available for further research.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Yilun and Wang, Zhicheng and Peng, Yuxiang and Zhang, Zhiqiang and Yu, Gang and Sun, Jian},
	month = jun,
	year = {2018},
	pages = {7103--7112},
	file = {Chen et al. - 2018 - Cascaded Pyramid Network for Multi-person Pose Est.pdf:/Users/masoudtaghikhah/Zotero/storage/XQJQET9L/Chen et al. - 2018 - Cascaded Pyramid Network for Multi-person Pose Est.pdf:application/pdf},
}

@inproceedings{gkioxari_chained_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Chained {Predictions} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-3-319-46493-0},
	doi = {10.1007/978-3-319-46493-0_44},
	abstract = {In this work, we present an adaptation of the sequence-to-sequence model for structured vision tasks. In this model, the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted at different steps. We show that chain models achieve top performing results on human pose estimation from images and videos.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Gkioxari, Georgia and Toshev, Alexander and Jaitly, Navdeep},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Chain model, Human pose estimation, Structured tasks},
	pages = {728--743},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/PHD5GFP8/Gkioxari et al. - 2016 - Chained Predictions Using Convolutional Neural Net.pdf:application/pdf},
}

@inproceedings{xiaochuan_fan_combining_2015,
	address = {Boston, MA, USA},
	title = {Combining local appearance and holistic view: {Dual}-{Source} {Deep} {Neural} {Networks} for human pose estimation},
	isbn = {978-1-4673-6964-0},
	shorttitle = {Combining local appearance and holistic view},
	url = {http://ieeexplore.ieee.org/document/7298740/},
	doi = {10.1109/CVPR.2015.7298740},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {{Xiaochuan Fan} and {Kang Zheng} and {Yuewei Lin} and Wang, Song},
	month = jun,
	year = {2015},
	pages = {1347--1355},
	file = {Xiaochuan Fan et al. - 2015 - Combining local appearance and holistic view Dual.pdf:/Users/masoudtaghikhah/Zotero/storage/P89W4U44/Xiaochuan Fan et al. - 2015 - Combining local appearance and holistic view Dual.pdf:application/pdf},
}

@article{sun_compositional_nodate,
	title = {Compositional {Human} {Pose} {Regression}},
	abstract = {Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to deﬁne a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a uniﬁed setting. Comprehensive evaluation validates the effectiveness of our approach. It signiﬁcantly advances the state-of-the-art on Human3.6M [20] and is competitive with state-of-the-art results on MPII [3].},
	language = {en},
	author = {Sun, Xiao and Shang, Jiaxiang and Liang, Shuang and Wei, Yichen},
	pages = {10},
	file = {Sun et al. - Compositional Human Pose Regression.pdf:/Users/masoudtaghikhah/Zotero/storage/7ACNL84H/Sun et al. - Compositional Human Pose Regression.pdf:application/pdf},
}

@book{zheng_deep_2020,
	title = {Deep {Learning}-{Based} {Human} {Pose} {Estimation}: {A} {Survey}},
	shorttitle = {Deep {Learning}-{Based} {Human} {Pose} {Estimation}},
	abstract = {Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusions. The goal of this survey paper is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 240 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. We also provide a regularly updated project page on: {\textbackslash}url\{https://github.com/zczcwh/DL-HPE\}},
	author = {Zheng, Ce and Wu, Wenhan and Yang, Taojiannan and Zhu, Sijie and Chen, Chen and Liu, Ruixu and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
	month = dec,
	year = {2020},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/UMV63FEI/Zheng et al. - 2020 - Deep Learning-Based Human Pose Estimation A Surve.pdf:application/pdf},
}

@inproceedings{pishchulin_deepcut_2016,
	address = {Las Vegas, NV, USA},
	title = {{DeepCut}: {Joint} {Subset} {Partition} and {Labeling} for {Multi} {Person} {Pose} {Estimation}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {{DeepCut}},
	url = {http://ieeexplore.ieee.org/document/7780902/},
	doi = {10.1109/CVPR.2016.533},
	abstract = {This paper considers the task of articulated human pose estimation of multiple people in real world images. We propose an approach that jointly solves the tasks of detection and pose estimation: it infers the number of persons in a scene, identiﬁes occluded body parts, and disambiguates body parts between people in close proximity of each other. This joint formulation is in contrast to previous strategies, that address the problem by ﬁrst detecting people and subsequently estimating their body pose. We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors. Our formulation, an instance of an integer linear program, implicitly performs non-maximum suppression on the set of part candidates and groups them to form conﬁgurations of body parts respecting geometric and appearance constraints. Experiments on four different datasets demonstrate state-of-the-art results for both single person and multi person pose estimation1.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pishchulin, Leonid and Insafutdinov, Eldar and Tang, Siyu and Andres, Bjoern and Andriluka, Mykhaylo and Gehler, Peter and Schiele, Bernt},
	month = jun,
	year = {2016},
	pages = {4929--4937},
	file = {Pishchulin et al. - 2016 - DeepCut Joint Subset Partition and Labeling for M.pdf:/Users/masoudtaghikhah/Zotero/storage/EBK3TH9D/Pishchulin et al. - 2016 - DeepCut Joint Subset Partition and Labeling for M.pdf:application/pdf},
}

@inproceedings{girdhar_detect-and-track_2018,
	address = {Salt Lake City, UT},
	title = {Detect-and-{Track}: {Efficient} {Pose} {Estimation} in {Videos}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Detect-and-{Track}},
	url = {https://ieeexplore.ieee.org/document/8578142/},
	doi = {10.1109/CVPR.2018.00044},
	abstract = {This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection [17] and video understanding [5]. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2\% on the validation and 51.8\% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge [1].},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Girdhar, Rohit and Gkioxari, Georgia and Torresani, Lorenzo and Paluri, Manohar and Tran, Du},
	month = jun,
	year = {2018},
	pages = {350--359},
	file = {Girdhar et al. - 2018 - Detect-and-Track Efficient Pose Estimation in Vid.pdf:/Users/masoudtaghikhah/Zotero/storage/MXD86QIG/Girdhar et al. - 2018 - Detect-and-Track Efficient Pose Estimation in Vid.pdf:application/pdf},
}

@book{tian_directpose_2019,
	title = {{DirectPose}: {Direct} {End}-to-{End} {Multi}-{Person} {Pose} {Estimation}},
	shorttitle = {{DirectPose}},
	abstract = {We propose the first direct end-to-end multi-person pose estimation framework, termed DirectPose. Inspired by recent anchor-free object detectors, which directly regress the two corners of target bounding-boxes, the proposed framework directly predicts instance-aware keypoints for all the instances from a raw input image, eliminating the need for heuristic grouping in bottom-up methods or bounding-box detection and RoI operations in top-down ones. We also propose a novel Keypoint Alignment (KPAlign) mechanism, which overcomes the main difficulty: lack of the alignment between the convolutional features and predictions in this end-to-end framework. KPAlign improves the framework's performance by a large margin while still keeping the framework end-to-end trainable. With the only postprocessing non-maximum suppression (NMS), our proposed framework can detect multi-person keypoints with or without bounding-boxes in a single shot. Experiments demonstrate that the end-to-end paradigm can achieve competitive or better performance than previous strong baselines, in both bottom-up and top-down methods. We hope that our end-to-end approach can provide a new perspective for the human pose estimation task.},
	author = {Tian, Zhi and Chen, Hao and Shen, Chunhua},
	month = nov,
	year = {2019},
}

@inproceedings{tompson_efficient_2015,
	address = {Boston, MA, USA},
	title = {Efficient object localization using {Convolutional} {Networks}},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298664/},
	doi = {10.1109/CVPR.2015.7298664},
	abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These beneﬁts of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efﬁcient ‘position reﬁnement’ model that is trained to estimate the joint offset location within a small region of the image. This reﬁnement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1].},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
	month = jun,
	year = {2015},
	pages = {648--656},
	file = {Tompson et al. - 2015 - Efficient object localization using Convolutional .pdf:/Users/masoudtaghikhah/Zotero/storage/KH9AP9S6/Tompson et al. - 2015 - Efficient object localization using Convolutional .pdf:application/pdf},
}

@inproceedings{carion_end--end_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	isbn = {978-3-030-58452-8},
	doi = {10.1007/978-3-030-58452-8_13},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {213--229},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/7NKIBJF2/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf},
}

@inproceedings{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {28},
	shorttitle = {Faster {R}-{CNN}},
	url = {https://papers.nips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
	urldate = {2022-06-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2015},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/BC6A5ZYN/Ren et al. - 2015 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf},
}

@book{zhang_fastpose_2019,
	title = {{FastPose}: {Towards} {Real}-time {Pose} {Estimation} and {Tracking} via {Scale}-normalized {Multi}-task {Networks}},
	shorttitle = {{FastPose}},
	abstract = {Both accuracy and efficiency are significant for pose estimation and tracking in videos. State-of-the-art performance is dominated by two-stages top-down methods. Despite the leading results, these methods are impractical for real-world applications due to their separated architectures and complicated calculation. This paper addresses the task of articulated multi-person pose estimation and tracking towards real-time speed. An end-to-end multi-task network (MTN) is designed to perform human detection, pose estimation, and person re-identification (Re-ID) tasks simultaneously. To alleviate the performance bottleneck caused by scale variation problem, a paradigm which exploits scale-normalized image and feature pyramids (SIFP) is proposed to boost both performance and speed. Given the results of MTN, we adopt an occlusion-aware Re-ID feature strategy in the pose tracking module, where pose information is utilized to infer the occlusion state to make better use of Re-ID feature. In experiments, we demonstrate that the pose estimation and tracking performance improves steadily utilizing SIFP through different backbones. Using ResNet-18 and ResNet-50 as backbones, the overall pose tracking framework achieves competitive performance with 29.4 FPS and 12.2 FPS, respectively. Additionally, occlusion-aware Re-ID feature decreases the identification switches by 37\% in the pose tracking process.},
	author = {Zhang, Jiabin and Zheng, Zhu and Zou, Wei and Li, Peng and Li, Yanwei and Su, Hu and Huang, Guan},
	month = aug,
	year = {2019},
}

@article{pfister_flowing_2015,
	title = {Flowing {ConvNets} for {Human} {Pose} {Estimation} in {Videos}},
	doi = {10.1109/ICCV.2015.222},
	abstract = {The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow.
To this end we propose a new network architecture that: (i) regresses a confidence heatmap of joint position predictions; (ii) incorporates optical flow at a mid-layer to align heatmap predictions from neighbouring frames; and (iii) includes a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map.
We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, and one that regresses joint coordinates directly.
The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset.},
	author = {Pfister, Tomas and Charles, James and Zisserman, Andrew},
	month = jun,
	year = {2015},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/KPRPGHXV/Pfister et al. - 2015 - Flowing ConvNets for Human Pose Estimation in Vide.pdf:application/pdf},
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Computer architecture, Computer vision, Convolutional codes, Neural networks, Object detection, Sparse matrices, Visualization},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:/Users/masoudtaghikhah/Zotero/storage/JY2TW9MQ/7298594.html:text/html;IEEE Xplore Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/SQL3X7HX/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:application/pdf},
}

@incollection{vedaldi_graph-pcnn_2020,
	address = {Cham},
	title = {Graph-{PCNN}: {Two} {Stage} {Human} {Pose} {Estimation} with {Graph} {Pose} {Refinement}},
	volume = {12356},
	isbn = {978-3-030-58620-1 978-3-030-58621-8},
	shorttitle = {Graph-{PCNN}},
	url = {https://link.springer.com/10.1007/978-3-030-58621-8_29},
	abstract = {Recently, most of the state-of-the-art human pose estimation methods are based on heatmap regression. The ﬁnal coordinates of keypoints are obtained by decoding heatmap directly. In this paper, we aim to ﬁnd a better approach to get more accurate localization results. We mainly put forward two suggestions for improvement: 1) diﬀerent features and methods should be applied for rough and accurate localization, 2) relationship between keypoints should be considered. Speciﬁcally, we propose a two-stage graph-based and model-agnostic framework, called Graph-PCNN, with a localization subnet and a graph pose reﬁnement module added onto the original heatmap regression network. In the ﬁrst stage, heatmap regression network is applied to obtain a rough localization result, and a set of proposal keypoints, called guided points, are sampled. In the second stage, for each guided point, diﬀerent visual feature is extracted by the localization subnet. The relationship between guided points is explored by the graph pose reﬁnement module to get more accurate localization results. Experiments show that Graph-PCNN can be used in various backbones to boost the performance by a large margin. Without bells and whistles, our best model can achieve a new state-of-the-art 76.8\% AP on COCO test-dev split.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Wang, Jian and Long, Xiang and Gao, Yuan and Ding, Errui and Wen, Shilei},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58621-8_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {492--508},
	file = {Wang et al. - 2020 - Graph-PCNN Two Stage Human Pose Estimation with G.pdf:/Users/masoudtaghikhah/Zotero/storage/NSSQ6DUB/Wang et al. - 2020 - Graph-PCNN Two Stage Human Pose Estimation with G.pdf:application/pdf},
}

@article{li_heterogeneous_2015,
	title = {Heterogeneous {Multi}-task {Learning} for {Human} {Pose} {Estimation} with {Deep} {Convolutional} {Neural} {Network}},
	volume = {113},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-014-0767-8},
	doi = {10.1007/s11263-014-0767-8},
	abstract = {We propose a heterogeneous multi-task learning framework for human pose estimation from monocular images using a deep convolutional neural network. In particular, we simultaneously learn a human pose regressor and sliding-window body-part and joint-point detectors in a deep network architecture. We show that including the detection tasks helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several datasets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.},
	language = {en},
	number = {1},
	urldate = {2022-06-21},
	journal = {International Journal of Computer Vision},
	author = {Li, Sijin and Liu, Zhi-Qiang and Chan, Antoni B.},
	month = may,
	year = {2015},
	keywords = {Deep Learning, Human Pose Estimation},
	pages = {19--36},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/5ILLWPS7/Li et al. - 2015 - Heterogeneous Multi-task Learning for Human Pose E.pdf:application/pdf},
}

@inproceedings{cheng_higherhrnet_2020,
	address = {Seattle, WA, USA},
	title = {{HigherHRNet}: {Scale}-{Aware} {Representation} {Learning} for {Bottom}-{Up} {Human} {Pose} {Estimation}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{HigherHRNet}},
	url = {https://ieeexplore.ieee.org/document/9156777/},
	doi = {10.1109/CVPR42600.2020.00543},
	abstract = {Bottom-up human pose estimation methods have difﬁculties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multiresolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5\% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5\% AP) without using reﬁnement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all topdown methods on CrowdPose test (67.6\% AP), suggesting its robustness in crowded scene. The code and models are available at https://github.com/HRNet/ Higher-HRNet-Human-Pose-Estimation.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cheng, Bowen and Xiao, Bin and Wang, Jingdong and Shi, Honghui and Huang, Thomas S. and Zhang, Lei},
	month = jun,
	year = {2020},
	pages = {5385--5394},
	file = {Cheng et al. - 2020 - HigherHRNet Scale-Aware Representation Learning f.pdf:/Users/masoudtaghikhah/Zotero/storage/2U32ZARL/Cheng et al. - 2020 - HigherHRNet Scale-Aware Representation Learning f.pdf:application/pdf},
}

@inproceedings{lifshitz_human_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Human {Pose} {Estimation} {Using} {Deep} {Consensus} {Voting}},
	isbn = {978-3-319-46475-6},
	doi = {10.1007/978-3-319-46475-6_16},
	abstract = {In this paper we consider the problem of human pose estimation from a single still image. We propose a novel approach where each location in the image votes for the position of each keypoint using a convolutional neural net. The voting scheme allows us to utilize information from the whole image, rather than rely on a sparse set of keypoint locations. Using dense, multi-target votes, not only produces good keypoint predictions, but also enables us to compute image-dependent joint keypoint probabilities by looking at consensus voting. This differs from most previous methods where joint probabilities are learned from relative keypoint locations and are independent of the image. We finally combine the keypoints votes and joint probabilities in order to identify the optimal pose configuration. We show our competitive performance on the MPII Human Pose and Leeds Sports Pose datasets.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Lifshitz, Ita and Fetaya, Ethan and Ullman, Shimon},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Convolutional Layer, Image Patch, Patch Center, Pictorial Structure, Vote Scheme},
	pages = {246--260},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/X94847MR/Lifshitz et al. - 2016 - Human Pose Estimation Using Deep Consensus Voting.pdf:application/pdf},
}

@inproceedings{carreira_human_2016,
	address = {Las Vegas, NV, USA},
	title = {Human {Pose} {Estimation} with {Iterative} {Error} {Feedback}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780881/},
	doi = {10.1109/CVPR.2016.512},
	abstract = {Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classiﬁcation tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Carreira, Joao and Agrawal, Pulkit and Fragkiadaki, Katerina and Malik, Jitendra},
	month = jun,
	year = {2016},
	pages = {4733--4742},
	file = {Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:/Users/masoudtaghikhah/Zotero/storage/TBA8GDG6/Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:application/pdf},
}

@incollection{ferrari_integral_2018,
	address = {Cham},
	title = {Integral {Human} {Pose} {Regression}},
	volume = {11210},
	isbn = {978-3-030-01230-4 978-3-030-01231-1},
	url = {http://link.springer.com/10.1007/978-3-030-01231-1_33},
	abstract = {State-of-the-art human pose estimation methods are based on heat map representation. In spite of the good performance, the representation has a few issues in nature, such as non-diﬀerentiable postprocessing and quantization error. This work shows that a simple integral operation relates and uniﬁes the heat map representation and joint regression, thus avoiding the above issues. It is diﬀerentiable, eﬃcient, and compatible with any heat map based methods. Its eﬀectiveness is convincingly validated via comprehensive ablation experiments under various settings, speciﬁcally on 3D pose estimation, for the ﬁrst time.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Sun, Xiao and Xiao, Bin and Wei, Fangyin and Liang, Shuang and Wei, Yichen},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01231-1_33},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {536--553},
	file = {Sun et al. - 2018 - Integral Human Pose Regression.pdf:/Users/masoudtaghikhah/Zotero/storage/2RN686YV/Sun et al. - 2018 - Integral Human Pose Regression.pdf:application/pdf},
}

@article{li_is_2021,
	title = {Is {2D} {Heatmap} {Representation} {Even} {Necessary} for {Human} {Pose} {Estimation}?},
	abstract = {A Simple yet promising Disentangled Representation for keypoint coordinate (SimDR) is proposed, reformulating human keypoint localization as a task of classification, leading to a more efficient scheme without extra upsampling and refinement. The 2D heatmap representation has dominated human pose estimation for years due to its high performance. However, heatmap-based approaches have some drawbacks: 1) The performance drops dramatically in the low-resolution images, which are frequently encountered in real-world scenarios. 2) To improve the localization precision, multiple upsample layers may be needed to recover the feature map resolution from low to high, which are computationally expensive. 3) Extra coordinate refinement is usually necessary to reduce the quantization error of downscaled heatmaps. To address these issues, we propose a Simple yet promising Disentangled Representation for keypoint coordinate (SimDR), reformulating human keypoint localization as a task of classification. In detail, we propose to disentangle the representation of horizontal and vertical coordinates for keypoint location, leading to a more efficient scheme without extra upsampling and refinement. Comprehensive experiments conducted over COCO dataset show that the proposed heatmap-free methods outperform heatmap-based counterparts in all tested input resolutions, especially in lower resolutions by a large margin. Code will be made publicly available at https://github.com/leeyegy/SimDR.},
	journal = {ArXiv},
	author = {Li, Yanjie and Yang, Sen and Zhang, Shoukui and Wang, Zhicheng and Yang, Wankou and Xia, Shutao and Zhou, Erjin},
	year = {2021},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/2XNRAF35/Li et al. - 2021 - Is 2D Heatmap Representation Even Necessary for Hu.pdf:application/pdf},
}

@article{tompson_joint_nodate,
	title = {Joint {Training} of a {Convolutional} {Network} and a {Graphical} {Model} for {Human} {Pose} {Estimation}},
	abstract = {This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to signiﬁcantly outperform existing state-of-the-art techniques.},
	language = {en},
	author = {Tompson, Jonathan and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
	pages = {9},
	file = {Tompson et al. - Joint Training of a Convolutional Network and a Gr.pdf:/Users/masoudtaghikhah/Zotero/storage/E7BMV49I/Tompson et al. - Joint Training of a Convolutional Network and a Gr.pdf:application/pdf},
}

@incollection{vedaldi_learning_2020,
	address = {Cham},
	title = {Learning {Delicate} {Local} {Representations} for {Multi}-person {Pose} {Estimation}},
	volume = {12348},
	isbn = {978-3-030-58579-2 978-3-030-58580-8},
	url = {https://link.springer.com/10.1007/978-3-030-58580-8_27},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Cai, Yuanhao and Wang, Zhicheng and Luo, Zhengxiong and Yin, Binyi and Du, Angang and Wang, Haoqian and Zhang, Xiangyu and Zhou, Xinyu and Zhou, Erjin and Sun, Jian},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58580-8_27},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {455--472},
	file = {Cai et al. - 2020 - Learning Delicate Local Representations for Multi-.pdf:/Users/masoudtaghikhah/Zotero/storage/TNA4ZVPD/Cai et al. - 2020 - Learning Delicate Local Representations for Multi-.pdf:application/pdf},
}

@inproceedings{yang_learning_2017,
	address = {Venice},
	title = {Learning {Feature} {Pyramids} for {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237406/},
	doi = {10.1109/ICCV.2017.144},
	abstract = {Articulated human pose estimation is a fundamental yet challenging task in computer vision. The difﬁculty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens. Although pyramid methods are widely used to handle scale changes at inference time, learning feature pyramids in deep convolutional neural networks (DCNNs) is still not well explored. In this work, we design a Pyramid Residual Module (PRMs) to enhance the invariance in scales of DCNNs. Given input features, the PRMs learn convolutional ﬁlters on various scales of input features, which are obtained with different subsampling ratios in a multibranch network. Moreover, we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks, which achieve superior performance than plain networks in many tasks recently. Therefore, we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures. We investigate our method on two standard benchmarks for human pose estimation. Our approach obtains state-of-the-art results on both benchmarks. Code is available at https://github.com/bearpaw/PyraNet.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Yang, Wei and Li, Shuang and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
	month = oct,
	year = {2017},
	pages = {1290--1299},
	file = {Yang et al. - 2017 - Learning Feature Pyramids for Human Pose Estimatio.pdf:/Users/masoudtaghikhah/Zotero/storage/9GJQXNC3/Yang et al. - 2017 - Learning Feature Pyramids for Human Pose Estimatio.pdf:application/pdf},
}

@article{fieraru_learning_2018,
	title = {Learning to {Refine} {Human} {Pose} {Estimation}},
	abstract = {Multi-person pose estimation in images and videos is an important yet challenging task with many applications. Despite the large improvements in human pose estimation enabled by the development of convolutional neural networks, there still exist a lot of difficult cases where even the state-of-the-art models fail to correctly localize all body joints. This motivates the need for an additional refinement step that addresses these challenging cases and can be easily applied on top of any existing method. In this work, we introduce a pose refinement network (PoseRefiner) which takes as input both the image and a given pose estimate and learns to directly predict a refined pose by jointly reasoning about the input-output space. In order for the network to learn to refine incorrect body joint predictions, we employ a novel data augmentation scheme for training, where we model "hard" human pose cases. We evaluate our approach on four popular large-scale pose estimation benchmarks such as MPII Single- and Multi-Person Pose Estimation, PoseTrack Pose Estimation, and PoseTrack Pose Tracking, and report systematic improvement over the state of the art.},
	author = {Fieraru, Mihai and Khoreva, Anna and Pishchulin, Leonid and Schiele, Bernt},
	month = apr,
	year = {2018},
}

@inproceedings{chu_multi-context_2017,
	address = {Honolulu, HI},
	title = {Multi-context {Attention} for {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100084/},
	doi = {10.1109/CVPR.2017.601},
	abstract = {In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on detailed descriptions for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semanticconsistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive ﬁeld of the network. These units are extensions of residual units with a side branch incorporating ﬁlters with larger receptive ﬁeld, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts. Code has been made publicly available.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chu, Xiao and Yang, Wei and Ouyang, Wanli and Ma, Cheng and Yuille, Alan L. and Wang, Xiaogang},
	month = jul,
	year = {2017},
	pages = {5669--5678},
	file = {Chu et al. - 2017 - Multi-context Attention for Human Pose Estimation.pdf:/Users/masoudtaghikhah/Zotero/storage/6NV9QEEX/Chu et al. - 2017 - Multi-context Attention for Human Pose Estimation.pdf:application/pdf},
}

@inproceedings{jin_multi-person_2019,
	address = {Long Beach, CA, USA},
	title = {Multi-{Person} {Articulated} {Tracking} {With} {Spatial} and {Temporal} {Embeddings}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953507/},
	doi = {10.1109/CVPR.2019.00581},
	abstract = {We propose a uniﬁed framework for multi-person pose estimation and tracking. Our framework consists of two main components, i.e. SpatialNet and TemporalNet. The SpatialNet accomplishes body part detection and part-level data association in a single frame, while the TemporalNet groups human instances in consecutive frames into trajectories. Speciﬁcally, besides body part detection heatmaps, SpatialNet also predicts the Keypoint Embedding (KE) and Spatial Instance Embedding (SIE) for body part association. We model the grouping procedure into a differentiable Pose-Guided Grouping (PGG) module to make the whole part detection and grouping pipeline fully end-toend trainable. TemporalNet extends the spatial grouping of keypoints to temporal grouping of human instances. Given human proposals from two consecutive frames, TemporalNet exploits both appearance features encoded in Human Embedding (HE) and temporally consistent geometric features embodied in Temporal Instance Embedding (TIE) for robust tracking. Extensive experiments demonstrate the effectiveness of our proposed model. Remarkably, we demonstrate substantial improvements over the state-ofthe-art pose tracking method from 65.4\% to 71.8\% MultiObject Tracking Accuracy (MOTA) on the ICCV’17 PoseTrack Dataset.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Jin, Sheng and Liu, Wentao and Ouyang, Wanli and Qian, Chen},
	month = jun,
	year = {2019},
	pages = {5657--5666},
	file = {Jin et al. - 2019 - Multi-Person Articulated Tracking With Spatial and.pdf:/Users/masoudtaghikhah/Zotero/storage/VVHA3C4K/Jin et al. - 2019 - Multi-Person Articulated Tracking With Spatial and.pdf:application/pdf},
}

@inproceedings{su_multi-person_2019,
	address = {Long Beach, CA, USA},
	title = {Multi-{Person} {Pose} {Estimation} {With} {Enhanced} {Channel}-{Wise} and {Spatial} {Information}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953968/},
	doi = {10.1109/CVPR.2019.00582},
	abstract = {Multi-person pose estimation is an important but challenging problem in computer vision. Although current approaches have achieved signiﬁcant progress by fusing the multi-scale feature maps, they pay little attention to enhancing the channel-wise and spatial information of the feature maps. In this paper, we propose two novel modules to perform the enhancement of the information for the multi-person pose estimation. First, a Channel Shufﬂe Module (CSM) is proposed to adopt the channel shufﬂe operation on the feature maps with different levels, promoting cross-channel information communication among the pyramid feature maps. Second, a Spatial, Channel-wise Attention Residual Bottleneck (SCARB) is designed to boost the original residual unit with attention mechanism, adaptively highlighting the information of the feature maps both in the spatial and channel-wise context. The effectiveness of our proposed modules is evaluated on the COCO keypoint benchmark, and experimental results show that our approach achieves the state-of-the-art results.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Su, Kai and Yu, Dongdong and Xu, Zhenqi and Geng, Xin and Wang, Changhu},
	month = jun,
	year = {2019},
	pages = {5667--5675},
	file = {Su et al. - 2019 - Multi-Person Pose Estimation With Enhanced Channel.pdf:/Users/masoudtaghikhah/Zotero/storage/LCF7ATNQ/Su et al. - 2019 - Multi-Person Pose Estimation With Enhanced Channel.pdf:application/pdf},
}

@book{chen_2d_2022,
	title = {{2D} {Human} {Pose} {Estimation}: {A} {Survey}},
	shorttitle = {{2D} {Human} {Pose} {Estimation}},
	abstract = {Human pose estimation aims at localizing human anatomical keypoints or body parts in the input data (e.g., images, videos, or signals). It forms a crucial component in enabling machines to have an insightful understanding of the behaviors of humans, and has become a salient problem in computer vision and related fields. Deep learning techniques allow learning feature representations directly from the data, significantly pushing the performance boundary of human pose estimation. In this paper, we reap the recent achievements of 2D human pose estimation methods and present a comprehensive survey. Briefly, existing approaches put their efforts in three directions, namely network architecture design, network training refinement, and post processing. Network architecture design looks at the architecture of human pose estimation models, extracting more robust features for keypoint recognition and localization. Network training refinement tap into the training of neural networks and aims to improve the representational ability of models. Post processing further incorporates model-agnostic polishing strategies to improve the performance of keypoint detection. More than 200 research contributions are involved in this survey, covering methodological frameworks, common benchmark datasets, evaluation metrics, and performance comparisons. We seek to provide researchers with a more comprehensive and systematic review on human pose estimation, allowing them to acquire a grand panorama and better identify future directions.},
	author = {Chen, Haoming and Feng, Runyang and Wu, Sifan and Xu, Hao and Zhou, Fengcheng and Liu, Zhenguang},
	month = apr,
	year = {2022},
}

@inproceedings{ke_multi-scale_2018,
	title = {Multi-{Scale} {Structure}-{Aware} {Network} for {Human} {Pose} {Estimation}},
	doi = {10.1007/978-3-030-01216-8_44},
	abstract = {A robust multi-scale structure-aware neural network for human pose estimation that effectively improves state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.},
	booktitle = {{ECCV}},
	author = {Ke, Lipeng and Chang, Ming-Ching and Qi, H. and Lyu, Siwei},
	year = {2018},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/TG3FX5BB/Ke et al. - 2018 - Multi-Scale Structure-Aware Network for Human Pose.pdf:application/pdf},
}

@incollection{ferrari_multiposenet_2018,
	address = {Cham},
	title = {{MultiPoseNet}: {Fast} {Multi}-{Person} {Pose} {Estimation} {Using} {Pose} {Residual} {Network}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	shorttitle = {{MultiPoseNet}},
	url = {http://link.springer.com/10.1007/978-3-030-01252-6_26},
	abstract = {In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose estimation architecture that combines a multi-task model with a novel assignment method. MultiPoseNet can jointly handle person detection, person segmentation and pose estimation problems. The novel assignment method is implemented by the Pose Residual Network (PRN) which receives keypoint and person detections, and produces accurate poses by assigning keypoints to person instances. On the COCO keypoints dataset, our pose estimation method outperforms all previous bottom-up methods both in accuracy (+4-point mAP over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster. Our method is the fastest real time system with ∼23 frames/sec.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Kocabas, Muhammed and Karagoz, Salih and Akbas, Emre},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01252-6_26},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {437--453},
	file = {Kocabas et al. - 2018 - MultiPoseNet Fast Multi-Person Pose Estimation Us.pdf:/Users/masoudtaghikhah/Zotero/storage/DSHTXZCX/Kocabas et al. - 2018 - MultiPoseNet Fast Multi-Person Pose Estimation Us.pdf:application/pdf},
}

@incollection{vedaldi_peeking_2020,
	address = {Cham},
	title = {Peeking into {Occluded} {Joints}: {A} {Novel} {Framework} for {Crowd} {Pose} {Estimation}},
	volume = {12364},
	isbn = {978-3-030-58528-0 978-3-030-58529-7},
	shorttitle = {Peeking into {Occluded} {Joints}},
	url = {https://link.springer.com/10.1007/978-3-030-58529-7_29},
	abstract = {Although occlusion widely exists in nature and remains a fundamental challenge for pose estimation, existing heatmap-based approaches suﬀer serious degradation on occlusions. Their intrinsic problem is that they directly localize the joints based on visual information; however, the invisible joints are lack of that. In contrast to localization, our framework estimates the invisible joints from an inference perspective by proposing an Image-Guided Progressive GCN module which provides a comprehensive understanding of both image context and pose structure. Moreover, existing benchmarks contain limited occlusions for evaluation. Therefore, we thoroughly pursue this problem and propose a novel OPEC-Net framework together with a new Occluded Pose (OCPose) dataset with 9k annotated images. Extensive quantitative and qualitative evaluations on benchmarks demonstrate that OPEC-Net achieves signiﬁcant improvements over recent leading works. Notably, our OCPose is the most complex occlusion dataset with respect to average IoU between adjacent instances. Source code and OCPose are publicly available at https://lingtengqiu.github.io/2020/03/22/OPEC-Net/ soon.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Qiu, Lingteng and Zhang, Xuanye and Li, Yanran and Li, Guanbin and Wu, Xiaojun and Xiong, Zixiang and Han, Xiaoguang and Cui, Shuguang},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58529-7_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {488--504},
	file = {Qiu et al. - 2020 - Peeking into Occluded Joints A Novel Framework fo.pdf:/Users/masoudtaghikhah/Zotero/storage/6DBQCSV6/Qiu et al. - 2020 - Peeking into Occluded Joints A Novel Framework fo.pdf:application/pdf},
}

@incollection{vedaldi_point-set_2020,
	address = {Cham},
	title = {Point-{Set} {Anchors} for {Object} {Detection}, {Instance} {Segmentation} and {Pose} {Estimation}},
	volume = {12355},
	isbn = {978-3-030-58606-5 978-3-030-58607-2},
	url = {https://link.springer.com/10.1007/978-3-030-58607-2_31},
	abstract = {A recent approach for object detection and human pose estimation is to regress bounding boxes or human keypoints from a central point on the object or person. While this center-point regression is simple and eﬃcient, we argue that the image features extracted at a central point contain limited information for predicting distant keypoints or bounding box boundaries, due to object deformation and scale/orientation variation. To facilitate inference, we propose to instead perform regression from a set of points placed at more advantageous positions. This point set is arranged to reﬂect a good initialization for the given task, such as modes in the training data for pose estimation, which lie closer to the ground truth than the central point and provide more informative features for regression. As the utility of a point set depends on how well its scale, aspect ratio and rotation matches the target, we adopt the anchor box technique of sampling these transformations to generate additional point-set candidates. We apply this proposed framework, called Point-Set Anchors, to object detection, instance segmentation, and human pose estimation. Our results show that this general-purpose approach can achieve performance competitive with state-of-the-art methods for each of these tasks.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Wei, Fangyun and Sun, Xiao and Li, Hongyang and Wang, Jingdong and Lin, Stephen},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58607-2_31},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {527--544},
	file = {Wei et al. - 2020 - Point-Set Anchors for Object Detection, Instance S.pdf:/Users/masoudtaghikhah/Zotero/storage/WX6DXYUQ/Wei et al. - 2020 - Point-Set Anchors for Object Detection, Instance S.pdf:application/pdf},
}

@inproceedings{li_pose_2021,
	address = {Nashville, TN, USA},
	title = {Pose {Recognition} with {Cascade} {Transformers}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578344/},
	doi = {10.1109/CVPR46437.2021.00198},
	abstract = {In this paper, we present a regression-based pose recognition method using cascade Transformers. One way to categorize the existing approaches in this domain is to separate them into 1). heatmap-based and 2). regressionbased. In general, heatmap-based methods achieve higher accuracy but are subject to various heuristic designs (not end-to-end mostly), whereas regression-based approaches attain relatively lower accuracy but they have less intermediate non-differentiable steps. Here we utilize the encoderdecoder structure in Transformers to perform regressionbased person and keypoint detection that is general-purpose and requires less heuristic design compared with the existing approaches. We demonstrate the keypoint hypothesis (query) reﬁnement process across different self-attention layers to reveal the recursive self-attention mechanism in Transformers. In the experiments, we report competitive results for pose recognition when compared with the competing regression-based methods.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Ke and Wang, Shijie and Zhang, Xiang and Xu, Yifan and Xu, Weijian and Tu, Zhuowen},
	month = jun,
	year = {2021},
	pages = {1944--1953},
	file = {Li et al. - 2021 - Pose Recognition with Cascade Transformers.pdf:/Users/masoudtaghikhah/Zotero/storage/SZILJXCF/Li et al. - 2021 - Pose Recognition with Cascade Transformers.pdf:application/pdf},
}

@inproceedings{cao_realtime_2017,
	address = {Honolulu, HI},
	title = {Realtime {Multi}-person {2D} {Pose} {Estimation} {Using} {Part} {Affinity} {Fields}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099626/},
	doi = {10.1109/CVPR.2017.143},
	abstract = {We present an approach to efﬁciently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Afﬁnity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed ﬁrst in the inaugural COCO 2016 keypoints challenge, and signiﬁcantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efﬁciency.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
	month = jul,
	year = {2017},
	pages = {1302--1310},
	file = {Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf:/Users/masoudtaghikhah/Zotero/storage/LAVSAYSR/Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf:application/pdf},
}

@inproceedings{fang_rmpe_2017,
	address = {Venice},
	title = {{RMPE}: {Regional} {Multi}-person {Pose} {Estimation}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {{RMPE}},
	url = {http://ieeexplore.ieee.org/document/8237518/},
	doi = {10.1109/ICCV.2017.256},
	abstract = {Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose NonMaximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve 76.7 mAP on the MPII (multi person) dataset[3]. Our model and source codes are made publicly available.†.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Fang, Hao-Shu and Xie, Shuqin and Tai, Yu-Wing and Lu, Cewu},
	month = oct,
	year = {2017},
	pages = {2353--2362},
	file = {Fang et al. - 2017 - RMPE Regional Multi-person Pose Estimation.pdf:/Users/masoudtaghikhah/Zotero/storage/4MH93TSR/Fang et al. - 2017 - RMPE Regional Multi-person Pose Estimation.pdf:application/pdf},
}

@article{kipf_semi-supervised_2017,
	title = {{SEMI}-{SUPERVISED} {CLASSIFICATION} {WITH} {GRAPH} {CONVOLUTIONAL} {NETWORKS}},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efﬁcient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized ﬁrst-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a signiﬁcant margin.},
	language = {en},
	author = {Kipf, Thomas N and Welling, Max},
	year = {2017},
	pages = {14},
	file = {Kipf and Welling - 2017 - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUT.pdf:/Users/masoudtaghikhah/Zotero/storage/X2UX88Z8/Kipf and Welling - 2017 - SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUT.pdf:application/pdf},
}

@incollection{ferrari_simple_2018,
	address = {Cham},
	title = {Simple {Baselines} for {Human} {Pose} {Estimation} and {Tracking}},
	volume = {11210},
	isbn = {978-3-030-01230-4 978-3-030-01231-1},
	url = {http://link.springer.com/10.1007/978-3-030-01231-1_29},
	abstract = {There has been signiﬁcant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more diﬃcult. This work provides simple and eﬀective baseline methods. They are helpful for inspiring and evaluating new ideas for the ﬁeld. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github. com/leoxiaobin/pose.pytorch.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Xiao, Bin and Wu, Haiping and Wei, Yichen},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01231-1_29},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {472--487},
	file = {Xiao et al. - 2018 - Simple Baselines for Human Pose Estimation and Tra.pdf:/Users/masoudtaghikhah/Zotero/storage/3R6Z8ITX/Xiao et al. - 2018 - Simple Baselines for Human Pose Estimation and Tra.pdf:application/pdf},
}

@inproceedings{li_simple_2020,
	title = {Simple {Pose}: {Rethinking} and {Improving} a {Bottom}-up {Approach} for {Multi}-{Person} {Pose} {Estimation}},
	shorttitle = {Simple {Pose}},
	doi = {10.1609/AAAI.V34I07.6797},
	abstract = {A well-known bottom-up approach for multi-person pose estimation is rethink and an improved one is proposed which outperforms the baseline by about 15\% in average precision and is comparable to the state of the art on the MS-COCO test-dev dataset. We rethink a well-known bottom-up approach for multi-person pose estimation and propose an improved one. The improved approach surpasses the baseline significantly thanks to (1) an intuitional yet more sensible representation, which we refer to as body parts to encode the connection information between keypoints, (2) an improved stacked hourglass network with attention mechanisms, (3) a novel focal L2 loss which is dedicated to “hard” keypoint and keypoint association (body part) mining, and (4) a robust greedy keypoint assignment algorithm for grouping the detected keypoints into individual poses. Our approach not only works straightforwardly but also outperforms the baseline by about 15\% in average precision and is comparable to the state of the art on the MS-COCO test-dev dataset. The code and pre-trained models are publicly available on our project page1.},
	booktitle = {{AAAI}},
	author = {Li, Jia and Su, Wen and Wang, Zengfu},
	year = {2020},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/A2SWD63M/Li et al. - 2020 - Simple Pose Rethinking and Improving a Bottom-up .pdf:application/pdf},
}

@inproceedings{newell_stacked_2016,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Stacked {Hourglass} {Networks} for {Human} {Pose} {Estimation}},
	isbn = {978-3-319-46484-8},
	doi = {10.1007/978-3-319-46484-8_29},
	abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Human pose estimation},
	pages = {483--499},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/MGWCLU7S/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:application/pdf},
}

@inproceedings{artacho_unipose_2020,
	address = {Seattle, WA, USA},
	title = {{UniPose}: {Unified} {Human} {Pose} {Estimation} in {Single} {Images} and {Videos}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{UniPose}},
	url = {https://ieeexplore.ieee.org/document/9157597/},
	doi = {10.1109/CVPR42600.2020.00706},
	abstract = {We propose UniPose, a uniﬁed framework for human pose estimation, based on our “Waterfall” Atrous Spatial Pooling architecture, that achieves state-of-art-results on several pose estimation metrics. UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage, with high accuracy, without relying on statistical postprocessing methods. The Waterfall module in UniPose leverages the efﬁciency of progressive ﬁltering in the cascade architecture, while maintaining multi-scale ﬁelds-of-view comparable to spatial pyramid conﬁgurations. Additionally, our method is extended to UniPose-LSTM for multi-frame processing and achieves state-of-the-art results for temporal pose estimation in Video. Our results on multiple datasets demonstrate that UniPose, with a ResNet backbone and Waterfall module, is a robust and efﬁcient architecture for pose estimation obtaining state-of-the-art results in single person pose detection for both single images and videos.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Artacho, Bruno and Savakis, Andreas},
	month = jun,
	year = {2020},
	pages = {7033--7042},
	file = {Artacho and Savakis - 2020 - UniPose Unified Human Pose Estimation in Single I.pdf:/Users/masoudtaghikhah/Zotero/storage/3S9BAX7Y/Artacho and Savakis - 2020 - UniPose Unified Human Pose Estimation in Single I.pdf:application/pdf},
}
