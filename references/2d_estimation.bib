
@inproceedings{bertasius_learning_2019,
	title = {Learning {Temporal} {Pose} {Estimation} from {Sparsely}-{Labeled} {Videos}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/7137debd45ae4d0ab9aa953017286b20-Abstract.html},
	abstract = {Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames---a labeled Frame A and an unlabeled Frame B---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7\% mAP vs 83.8\% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows us to obtain state-of-the-art pose detection results on PoseTrack2017 and PoseTrack2018 datasets.},
	urldate = {2022-05-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bertasius, Gedas and Feichtenhofer, Christoph and Tran, Du and Shi, Jianbo and Torresani, Lorenzo},
	year = {2019},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/T94P9CHW/Bertasius et al. - 2019 - Learning Temporal Pose Estimation from Sparsely-La.pdf:application/pdf},
}

@inproceedings{ramakrishna_pose_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pose {Machines}: {Articulated} {Pose} {Estimation} via {Inference} {Machines}},
	isbn = {978-3-319-10605-2},
	shorttitle = {Pose {Machines}},
	doi = {10.1007/978-3-319-10605-2_3},
	abstract = {State-of-the-art approaches for articulated human pose estimation are rooted in parts-based graphical models. These models are often restricted to tree-structured representations and simple parametric potentials in order to enable tractable inference. However, these simple dependencies fail to capture all the interactions between body parts. While models with more complex interactions can be defined, learning the parameters of these models remains challenging with intractable or approximate inference. In this paper, instead of performing inference on a learned graphical model, we build upon the inference machine framework and present a method for articulated human pose estimation. Our approach incorporates rich spatial interactions among multiple parts and information across parts of different scales. Additionally, the modular framework of our approach enables both ease of implementation without specialized optimization solvers, and efficient inference. We analyze our approach on two challenging datasets with large pose variation and outperform the state-of-the-art on these benchmarks.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Ramakrishna, Varun and Munoz, Daniel and Hebert, Martial and Andrew Bagnell, James and Sheikh, Yaser},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Approximate Inference, Composite Part, Context Feature, Pictorial Structure, Random Forest},
	pages = {33--47},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/T3MJIZQN/Ramakrishna et al. - 2014 - Pose Machines Articulated Pose Estimation via Inf.pdf:application/pdf},
}

@techreport{carion_end--end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	number = {arXiv:2005.12872},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	doi = {10.48550/arXiv.2005.12872},
	note = {arXiv:2005.12872 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/VVZHUTPV/Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/W73G3KJA/2005.html:text/html},
}

@techreport{fang_rmpe_2018,
	title = {{RMPE}: {Regional} {Multi}-person {Pose} {Estimation}},
	shorttitle = {{RMPE}},
	url = {http://arxiv.org/abs/1612.00137},
	abstract = {Multi-person pose estimation in the wild is challenging. Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable. These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results. In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes. Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG). Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve a 17\% increase in mAP over the state-of-the-art methods on the MPII (multi person) dataset.Our model and source codes are publicly available.},
	number = {arXiv:1612.00137},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Fang, Hao-Shu and Xie, Shuqin and Tai, Yu-Wing and Lu, Cewu},
	month = feb,
	year = {2018},
	note = {arXiv:1612.00137 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Models \& Codes available at https://github.com/MVIG-SJTU/RMPE or https://github.com/Fang-Haoshu/RMPE},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/DS33FFWE/Fang et al. - 2018 - RMPE Regional Multi-person Pose Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/IH73Y5RJ/1612.html:text/html},
}

@techreport{newell_stacked_2016,
	title = {Stacked {Hourglass} {Networks} for {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1603.06937},
	abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a "stacked hourglass" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
	number = {arXiv:1603.06937},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
	month = jul,
	year = {2016},
	note = {arXiv:1603.06937 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {resnet style, symmetric
},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/9P8WLUKF/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/J477KHSN/1603.html:text/html},
}

@inproceedings{wei_convolutional_2016,
	address = {Las Vegas, NV, USA},
	title = {Convolutional {Pose} {Machines}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780880/},
	doi = {10.1109/CVPR.2016.511},
	abstract = {Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly reﬁned estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difﬁculty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.},
	language = {en},
	urldate = {2022-05-18},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
	month = jun,
	year = {2016},
	pages = {4724--4732},
	file = {Wei et al. - 2016 - Convolutional Pose Machines.pdf:/Users/masoudtaghikhah/Zotero/storage/G9HXN9FI/Wei et al. - 2016 - Convolutional Pose Machines.pdf:application/pdf},
}

@techreport{carreira_human_2016,
	title = {Human {Pose} {Estimation} with {Iterative} {Error} {Feedback}},
	url = {http://arxiv.org/abs/1507.06550},
	abstract = {Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classification tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.},
	number = {arXiv:1507.06550},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Carreira, Joao and Agrawal, Pulkit and Fragkiadaki, Katerina and Malik, Jitendra},
	month = jun,
	year = {2016},
	note = {arXiv:1507.06550 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/HUQCU79T/Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/5GAUDQHB/1507.html:text/html},
}

@inproceedings{xiao_simple_2018,
	title = {Simple {Baselines} for {Human} {Pose} {Estimation} and {Tracking}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper.html},
	urldate = {2022-05-18},
	author = {Xiao, Bin and Wu, Haiping and Wei, Yichen},
	year = {2018},
	pages = {466--481},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/VZ4G2Q5D/Xiao et al. - 2018 - Simple Baselines for Human Pose Estimation and Tra.pdf:application/pdf},
}

@inproceedings{liu_deep_2021,
	address = {Nashville, TN, USA},
	title = {Deep {Dual} {Consecutive} {Network} for {Human} {Pose} {Estimation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577714/},
	doi = {10.1109/CVPR46437.2021.00059},
	abstract = {Multi-frame human pose estimation in complicated situations is challenging. Although state-of-the-art human joints detectors have demonstrated remarkable results for static images, their performances come short when we apply these models to video sequences. Prevalent shortcomings include the failure to handle motion blur, video defocus, or pose occlusions, arising from the inability in capturing the temporal dependency among video frames. On the other hand, directly employing conventional recurrent neural networks incurs empirical difﬁculties in modeling spatial contexts, especially for dealing with pose occlusions. In this paper, we propose a novel multi-frame human pose estimation framework, leveraging abundant temporal cues between video frames to facilitate keypoint detection. Three modular components are designed in our framework. A Pose Temporal Merger encodes keypoint spatiotemporal context to generate effective searching scopes while a Pose Residual Fusion module computes weighted pose residuals in dual directions. These are then processed via our Pose Correction Network for efﬁcient reﬁning of pose estimations. Our method ranks No.1 in the Multi-frame Person Pose Estimation Challenge on the large-scale benchmark datasets PoseTrack2017 and PoseTrack2018. We have released our code, hoping to inspire future research.},
	language = {en},
	urldate = {2022-05-18},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Zhenguang and Chen, Haoming and Feng, Runyang and Wu, Shuang and Ji, Shouling and Yang, Bailin and Wang, Xun},
	month = jun,
	year = {2021},
	pages = {525--534},
	file = {Liu et al. - 2021 - Deep Dual Consecutive Network for Human Pose Estim.pdf:/Users/masoudtaghikhah/Zotero/storage/6EIWHW9Y/Liu et al. - 2021 - Deep Dual Consecutive Network for Human Pose Estim.pdf:application/pdf},
}

@inproceedings{luo_lstm_2018,
	title = {{LSTM} {Pose} {Machines}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Luo_LSTM_Pose_Machines_CVPR_2018_paper.html},
	urldate = {2022-05-18},
	author = {Luo, Yue and Ren, Jimmy and Wang, Zhouxia and Sun, Wenxiu and Pan, Jinshan and Liu, Jianbo and Pang, Jiahao and Lin, Liang},
	year = {2018},
	pages = {5207--5215},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/FS2NT2W5/Luo et al. - 2018 - LSTM Pose Machines.pdf:application/pdf},
}

@inproceedings{yu_lite-hrnet_2021,
	address = {Nashville, TN, USA},
	title = {Lite-{HRNet}: {A} {Lightweight} {High}-{Resolution} {Network}},
	isbn = {978-1-66544-509-2},
	shorttitle = {Lite-{HRNet}},
	url = {https://ieeexplore.ieee.org/document/9578746/},
	doi = {10.1109/CVPR46437.2021.01030},
	abstract = {We present an efﬁcient high-resolution network, LiteHRNet, for human pose estimation. We start by simply applying the efﬁcient shufﬂe block in ShufﬂeNet to HRNet (high-resolution network), yielding stronger performance over popular lightweight networks, such as MobileNet, ShufﬂeNet, and Small HRNet.},
	language = {en},
	urldate = {2022-05-18},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yu, Changqian and Xiao, Bin and Gao, Changxin and Yuan, Lu and Zhang, Lei and Sang, Nong and Wang, Jingdong},
	month = jun,
	year = {2021},
	pages = {10435--10445},
	file = {Yu et al. - 2021 - Lite-HRNet A Lightweight High-Resolution Network.pdf:/Users/masoudtaghikhah/Zotero/storage/VL22JXLY/Yu et al. - 2021 - Lite-HRNet A Lightweight High-Resolution Network.pdf:application/pdf},
}

@techreport{zhang_fastpose_2019,
	title = {{FastPose}: {Towards} {Real}-time {Pose} {Estimation} and {Tracking} via {Scale}-normalized {Multi}-task {Networks}},
	shorttitle = {{FastPose}},
	url = {http://arxiv.org/abs/1908.05593},
	abstract = {Both accuracy and efficiency are significant for pose estimation and tracking in videos. State-of-the-art performance is dominated by two-stages top-down methods. Despite the leading results, these methods are impractical for real-world applications due to their separated architectures and complicated calculation. This paper addresses the task of articulated multi-person pose estimation and tracking towards real-time speed. An end-to-end multi-task network (MTN) is designed to perform human detection, pose estimation, and person re-identification (Re-ID) tasks simultaneously. To alleviate the performance bottleneck caused by scale variation problem, a paradigm which exploits scale-normalized image and feature pyramids (SIFP) is proposed to boost both performance and speed. Given the results of MTN, we adopt an occlusion-aware Re-ID feature strategy in the pose tracking module, where pose information is utilized to infer the occlusion state to make better use of Re-ID feature. In experiments, we demonstrate that the pose estimation and tracking performance improves steadily utilizing SIFP through different backbones. Using ResNet-18 and ResNet-50 as backbones, the overall pose tracking framework achieves competitive performance with 29.4 FPS and 12.2 FPS, respectively. Additionally, occlusion-aware Re-ID feature decreases the identification switches by 37\% in the pose tracking process.},
	number = {arXiv:1908.05593},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Zhang, Jiabin and Zhu, Zheng and Zou, Wei and Li, Peng and Li, Yanwei and Su, Hu and Huang, Guan},
	month = aug,
	year = {2019},
	note = {arXiv:1908.05593 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/WQ4U75H8/Zhang et al. - 2019 - FastPose Towards Real-time Pose Estimation and Tr.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/BSUE2LV8/1908.html:text/html},
}

@techreport{cheng_higherhrnet_2020,
	title = {{HigherHRNet}: {Scale}-{Aware} {Representation} {Learning} for {Bottom}-{Up} {Human} {Pose} {Estimation}},
	shorttitle = {{HigherHRNet}},
	url = {http://arxiv.org/abs/1908.10357},
	abstract = {Bottom-up human pose estimation methods have difficulties in predicting the correct pose for small persons due to challenges in scale variation. In this paper, we present HigherHRNet: a novel bottom-up human pose estimation method for learning scale-aware representations using high-resolution feature pyramids. Equipped with multi-resolution supervision for training and multi-resolution aggregation for inference, the proposed approach is able to solve the scale variation challenge in bottom-up multi-person pose estimation and localize keypoints more precisely, especially for small person. The feature pyramid in HigherHRNet consists of feature map outputs from HRNet and upsampled higher-resolution outputs through a transposed convolution. HigherHRNet outperforms the previous best bottom-up method by 2.5\% AP for medium person on COCO test-dev, showing its effectiveness in handling scale variation. Furthermore, HigherHRNet achieves new state-of-the-art result on COCO test-dev (70.5\% AP) without using refinement or other post-processing techniques, surpassing all existing bottom-up methods. HigherHRNet even surpasses all top-down methods on CrowdPose test (67.6\% AP), suggesting its robustness in crowded scene. The code and models are available at https://github.com/HRNet/Higher-HRNet-Human-Pose-Estimation.},
	number = {arXiv:1908.10357},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Cheng, Bowen and Xiao, Bin and Wang, Jingdong and Shi, Honghui and Huang, Thomas S. and Zhang, Lei},
	month = mar,
	year = {2020},
	doi = {10.48550/arXiv.1908.10357},
	note = {arXiv:1908.10357 [cs, eess]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: CVPR 2020},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/N4J87PJ3/Cheng et al. - 2020 - HigherHRNet Scale-Aware Representation Learning f.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/SZBYN5K9/1908.html:text/html},
}

@inproceedings{jin_differentiable_2020,
	address = {Berlin, Heidelberg},
	title = {Differentiable {Hierarchical} {Graph} {Grouping} for {Multi}-person {Pose} {Estimation}},
	isbn = {978-3-030-58570-9},
	url = {https://doi.org/10.1007/978-3-030-58571-6_42},
	doi = {10.1007/978-3-030-58571-6_42},
	abstract = {Multi-person pose estimation is challenging because it localizes body keypoints for multiple persons simultaneously. Previous methods can be divided into two streams, i.e. top-down and bottom-up methods. The top-down methods localize keypoints after human detection, while the bottom-up methods localize keypoints directly and then cluster/group them for different persons, which are generally more efficient than top-down methods. However, in existing bottom-up methods, the keypoint grouping is usually solved independently from keypoint detection, making them not end-to-end trainable and have sub-optimal performance. In this paper, we investigate a new perspective of human part grouping and reformulate it as a graph clustering task. Especially, we propose a novel differentiable Hierarchical Graph Grouping (HGG) method to learn the graph grouping in bottom-up multi-person pose estimation task. Moreover, HGG is easily embedded into main-stream bottom-up methods. It takes human keypoint candidates as graph nodes and clusters keypoints in a multi-layer graph neural network model. The modules of HGG can be trained end-to-end with the keypoint detection network and is able to supervise the grouping process in a hierarchical manner. To improve the discrimination of the clustering, we add a set of edge discriminators and macro-node discriminators. Extensive experiments on both COCO and OCHuman datasets demonstrate that the proposed method improves the performance of bottom-up pose estimation methods.},
	urldate = {2022-05-18},
	booktitle = {Computer {Vision} – {ECCV} 2020: 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23–28, 2020, {Proceedings}, {Part} {VII}},
	publisher = {Springer-Verlag},
	author = {Jin, Sheng and Liu, Wentao and Xie, Enze and Wang, Wenhai and Qian, Chen and Ouyang, Wanli and Luo, Ping},
	month = aug,
	year = {2020},
	keywords = {Human pose estimation, Graph neural network, Grouping},
	pages = {718--734},
	file = {Submitted Version:/Users/masoudtaghikhah/Zotero/storage/H2V7X3LF/Jin et al. - 2020 - Differentiable Hierarchical Graph Grouping for Mul.pdf:application/pdf},
}

@inproceedings{luo_rethinking_2021,
	title = {Rethinking the {Heatmap} {Regression} for {Bottom}-{Up} {Human} {Pose} {Estimation}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Rethinking_the_Heatmap_Regression_for_Bottom-Up_Human_Pose_Estimation_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-05-18},
	author = {Luo, Zhengxiong and Wang, Zhicheng and Huang, Yan and Wang, Liang and Tan, Tieniu and Zhou, Erjin},
	year = {2021},
	pages = {13264--13273},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/G4INP7S9/Luo et al. - 2021 - Rethinking the Heatmap Regression for Bottom-Up Hu.pdf:application/pdf},
}

@techreport{wei_point-set_2020,
	title = {Point-{Set} {Anchors} for {Object} {Detection}, {Instance} {Segmentation} and {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2007.02846},
	abstract = {A recent approach for object detection and human pose estimation is to regress bounding boxes or human keypoints from a central point on the object or person. While this center-point regression is simple and efficient, we argue that the image features extracted at a central point contain limited information for predicting distant keypoints or bounding box boundaries, due to object deformation and scale/orientation variation. To facilitate inference, we propose to instead perform regression from a set of points placed at more advantageous positions. This point set is arranged to reflect a good initialization for the given task, such as modes in the training data for pose estimation, which lie closer to the ground truth than the central point and provide more informative features for regression. As the utility of a point set depends on how well its scale, aspect ratio and rotation matches the target, we adopt the anchor box technique of sampling these transformations to generate additional point-set candidates. We apply this proposed framework, called Point-Set Anchors, to object detection, instance segmentation, and human pose estimation. Our results show that this general-purpose approach can achieve performance competitive with state-of-the-art methods for each of these tasks. Code is available at {\textbackslash}url\{https://github.com/FangyunWei/PointSetAnchor\}},
	number = {arXiv:2007.02846},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Wei, Fangyun and Sun, Xiao and Li, Hongyang and Wang, Jingdong and Lin, Stephen},
	month = aug,
	year = {2020},
	doi = {10.48550/arXiv.2007.02846},
	note = {arXiv:2007.02846 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ECCV 2020},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/JBP6XRUD/Wei et al. - 2020 - Point-Set Anchors for Object Detection, Instance S.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/AAUXBYKN/2007.html:text/html},
}

@inproceedings{geng_bottom-up_2021,
	title = {Bottom-{Up} {Human} {Pose} {Estimation} via {Disentangled} {Keypoint} {Regression}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Geng_Bottom-Up_Human_Pose_Estimation_via_Disentangled_Keypoint_Regression_CVPR_2021_paper.html},
	language = {en},
	urldate = {2022-05-18},
	author = {Geng, Zigang and Sun, Ke and Xiao, Bin and Zhang, Zhaoxiang and Wang, Jingdong},
	year = {2021},
	pages = {14676--14686},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/JSXS3427/Geng et al. - 2021 - Bottom-Up Human Pose Estimation via Disentangled K.pdf:application/pdf},
}

@inproceedings{nie_single-stage_2019,
	title = {Single-{Stage} {Multi}-{Person} {Pose} {Machines}},
	url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Single-Stage_Multi-Person_Pose_Machines_ICCV_2019_paper.html},
	urldate = {2022-05-18},
	author = {Nie, Xuecheng and Feng, Jiashi and Zhang, Jianfeng and Yan, Shuicheng},
	year = {2019},
	pages = {6951--6960},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/C9XPNLU5/Nie et al. - 2019 - Single-Stage Multi-Person Pose Machines.pdf:application/pdf},
}

@techreport{cao_realtime_2017,
	title = {Realtime {Multi}-{Person} {2D} {Pose} {Estimation} using {Part} {Affinity} {Fields}},
	url = {http://arxiv.org/abs/1611.08050},
	abstract = {We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.},
	number = {arXiv:1611.08050},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
	month = apr,
	year = {2017},
	doi = {10.48550/arXiv.1611.08050},
	note = {arXiv:1611.08050 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted as CVPR 2017 Oral. Video result: https://youtu.be/pW6nZXeWlGM},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/QIHV3UF6/Cao et al. - 2017 - Realtime Multi-Person 2D Pose Estimation using Par.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/5SYH5R5C/1611.html:text/html},
}

@inproceedings{kreiss_pifpaf_2019,
	title = {{PifPaf}: {Composite} {Fields} for {Human} {Pose} {Estimation}},
	shorttitle = {{PifPaf}},
	url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Kreiss_PifPaf_Composite_Fields_for_Human_Pose_Estimation_CVPR_2019_paper.html},
	urldate = {2022-05-18},
	author = {Kreiss, Sven and Bertoni, Lorenzo and Alahi, Alexandre},
	year = {2019},
	pages = {11977--11986},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/QQ79RUQE/Kreiss et al. - 2019 - PifPaf Composite Fields for Human Pose Estimation.pdf:application/pdf;Snapshot:/Users/masoudtaghikhah/Zotero/storage/6SBJWHYA/Kreiss_PifPaf_Composite_Fields_for_Human_Pose_Estimation_CVPR_2019_paper.html:text/html},
}

@techreport{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	number = {arXiv:1609.02907},
	urldate = {2022-05-28},
	institution = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {2d regression method based on graphs
},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/IK3UTQYV/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/9G8E6YWF/1609.html:text/html},
}

@inproceedings{zhang_distribution-aware_2020,
	title = {Distribution-{Aware} {Coordinate} {Representation} for {Human} {Pose} {Estimation}},
	doi = {10.1109/CVPR42600.2020.00712},
	abstract = {While being the de facto standard coordinate representation for human pose estimation, heatmap has not been investigated in-depth. This work fills this gap. For the first time, we find that the process of decoding the predicted heatmaps into the final joint coordinates in the original image space is surprisingly significant for the performance. We further probe the design limitations of the standard coordinate decoding method, and propose a more principled distributionaware decoding method. Also, we improve the standard coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) by generating unbiased/accurate heatmaps. Taking the two together, we formulate a novel Distribution-Aware coordinate Representation of Keypoints (DARK) method. Serving as a model-agnostic plug-in, DARK brings about significant performance boost to existing human pose estimation models. Extensive experiments show that DARK yields the best results on two common benchmarks, MPII and COCO. Besides, DARK achieves the 2nd place entry in the ICCV 2019 COCO Keypoints Challenge. The code is available online.},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zhang, Feng and Zhu, Xiatian and Dai, Hanbin and Ye, Mao and Zhu, Ce},
	month = jun,
	year = {2020},
	note = {ISSN: 2575-7075},
	keywords = {Decoding, Encoding, Image resolution, Pose estimation, Space heating, Standards},
	pages = {7091--7100},
	annote = {leverage heatmap based approches, categorize in regres and heat map
},
	file = {IEEE Xplore Abstract Record:/Users/masoudtaghikhah/Zotero/storage/JA7XCUM4/9157744.html:text/html;IEEE Xplore Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/GMUF64Q3/Zhang et al. - 2020 - Distribution-Aware Coordinate Representation for H.pdf:application/pdf},
}

@techreport{li_is_2021,
	title = {Is {2D} {Heatmap} {Representation} {Even} {Necessary} for {Human} {Pose} {Estimation}?},
	url = {http://arxiv.org/abs/2107.03332},
	abstract = {The 2D heatmap representation has dominated human pose estimation for years due to its high performance. However, heatmap-based approaches have some drawbacks: 1) The performance drops dramatically in the low-resolution images, which are frequently encountered in real-world scenarios. 2) To improve the localization precision, multiple upsample layers may be needed to recover the feature map resolution from low to high, which are computationally expensive. 3) Extra coordinate refinement is usually necessary to reduce the quantization error of downscaled heatmaps. To address these issues, we propose a {\textbackslash}textbf\{Sim\}ple yet promising {\textbackslash}textbf\{D\}isentangled {\textbackslash}textbf\{R\}epresentation for keypoint coordinate ({\textbackslash}emph\{SimDR\}), reformulating human keypoint localization as a task of classification. In detail, we propose to disentangle the representation of horizontal and vertical coordinates for keypoint location, leading to a more efficient scheme without extra upsampling and refinement. Comprehensive experiments conducted over COCO dataset show that the proposed {\textbackslash}emph\{heatmap-free\} methods outperform {\textbackslash}emph\{heatmap-based\} counterparts in all tested input resolutions, especially in lower resolutions by a large margin. Code will be made publicly available at {\textbackslash}url\{https://github.com/leeyegy/SimDR\}.},
	number = {arXiv:2107.03332},
	urldate = {2022-06-07},
	institution = {arXiv},
	author = {Li, Yanjie and Yang, Sen and Zhang, Shoukui and Wang, Zhicheng and Yang, Wankou and Xia, Shu-Tao and Zhou, Erjin},
	month = jul,
	year = {2021},
	note = {arXiv:2107.03332 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code will be made publicly available at https://github.com/leeyegy/SimDR},
	annote = {the shortcomings and downside of heatmap-based methods
},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/ZHNMA9IV/Li et al. - 2021 - Is 2D Heatmap Representation Even Necessary for Hu.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/TMXBIR3E/2107.html:text/html},
}

@techreport{cai_learning_2020,
	title = {Learning {Delicate} {Local} {Representations} for {Multi}-{Person} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2003.04030},
	abstract = {In this paper, we propose a novel method called Residual Steps Network (RSN). RSN aggregates features with the same spatial size (Intra-level features) efficiently to obtain delicate local representations, which retain rich low-level spatial information and result in precise keypoint localization. Additionally, we observe the output features contribute differently to final performance. To tackle this problem, we propose an efficient attention mechanism - Pose Refine Machine (PRM) to make a trade-off between local and global representations in output features and further refine the keypoint locations. Our approach won the 1st place of COCO Keypoint Challenge 2019 and achieves state-of-the-art results on both COCO and MPII benchmarks, without using extra training data and pretrained model. Our single model achieves 78.6 on COCO test-dev, 93.0 on MPII test dataset. Ensembled models achieve 79.2 on COCO test-dev, 77.1 on COCO test-challenge dataset. The source code is publicly available for further research at https://github.com/caiyuanhao1998/RSN/},
	number = {arXiv:2003.04030},
	urldate = {2022-06-07},
	institution = {arXiv},
	author = {Cai, Yuanhao and Wang, Zhicheng and Luo, Zhengxiong and Yin, Binyi and Du, Angang and Wang, Haoqian and Zhang, Xiangyu and Zhou, Xinyu and Zhou, Erjin and Sun, Jian},
	month = jul,
	year = {2020},
	note = {arXiv:2003.04030 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV2020 Spotlight},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/4CRN8S3T/Cai et al. - 2020 - Learning Delicate Local Representations for Multi-.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/P279XCF3/2003.html:text/html},
}

@inproceedings{rafi_efficient_2016,
	address = {York, UK},
	title = {An {Efficient} {Convolutional} {Network} for {Human} {Pose} {Estimation}},
	isbn = {978-1-901725-59-9},
	url = {http://www.bmva.org/bmvc/2016/papers/paper109/index.html},
	doi = {10.5244/C.30.109},
	abstract = {In recent years, human pose estimation has greatly beneﬁted from deep learning and huge gains in performance have been achieved. However, to push for maximum performance recent approaches exploit computationally expensive deep network architectures, train on multiple datasets, apply additional post-processing and provide limited details about used design choices. This makes it hard not only to compare different methods and but also to reproduce existing results .},
	language = {en},
	urldate = {2022-06-08},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2016},
	publisher = {British Machine Vision Association},
	author = {Rafi, Umer and Leibe, Bastian and Gall, Juergen and Kostrikov, Ilya},
	year = {2016},
	pages = {109.1--109.11},
	file = {Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:/Users/masoudtaghikhah/Zotero/storage/U4EXRS7T/Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:application/pdf},
}

@inproceedings{micilotta_real-time_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Real-{Time} {Upper} {Body} {Detection} and {3D} {Pose} {Estimation} in {Monoscopic} {Images}},
	isbn = {978-3-540-33837-6},
	doi = {10.1007/11744078_11},
	abstract = {This paper presents a novel solution to the difficult task of both detecting and estimating the 3D pose of humans in monoscopic images. The approach consists of two parts. Firstly the location of a human is identified by a probabalistic assembly of detected body parts. Detectors for the face, torso and hands are learnt using adaBoost. A pose likliehood is then obtained using an a priori mixture model on body configuration and possible configurations assembled from available evidence using RANSAC. Once a human has been detected, the location is used to initialise a matching algorithm which matches the silhouette and edge map of a subject with a 3D model. This is done efficiently using chamfer matching, integral images and pose estimation from the initial detection stage. We demonstrate the application of the approach to large, cluttered natural images and at near framerate operation (16fps) on lower resolution video streams.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2006},
	publisher = {Springer},
	author = {Micilotta, Antonio S. and Ong, Eng-Jon and Bowden, Richard},
	editor = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
	year = {2006},
	keywords = {Body Part, Face Detection, False Detection, Integral Image, Mixture Model},
	pages = {139--150},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/V39PEPN9/Micilotta et al. - 2006 - Real-Time Upper Body Detection and 3D Pose Estimat.pdf:application/pdf},
}

@techreport{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	number = {arXiv:1506.01497},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	doi = {10.48550/arXiv.1506.01497},
	note = {arXiv:1506.01497 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Extended tech report},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/I733FRYB/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/EVHZSGJD/1506.html:text/html},
}

@techreport{fan_combining_2015,
	title = {Combining {Local} {Appearance} and {Holistic} {View}: {Dual}-{Source} {Deep} {Neural} {Networks} for {Human} {Pose} {Estimation}},
	shorttitle = {Combining {Local} {Appearance} and {Holistic} {View}},
	url = {http://arxiv.org/abs/1504.07159},
	abstract = {We propose a new learning-based method for estimating 2D human pose from a single image, using Dual-Source Deep Convolutional Neural Networks (DS-CNN). Recently, many methods have been developed to estimate human pose by using pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective. In this paper, we propose to integrate both the local (body) part appearance and the holistic view of each local part for more accurate human pose estimation. Specifically, the proposed DS-CNN takes a set of image patches (category-independent object proposals for training and multi-scale sliding windows for testing) as the input and then learns the appearance of each local part by considering their holistic views in the full body. Using DS-CNN, we achieve both joint detection, which determines whether an image patch contains a body joint, and joint localization, which finds the exact location of the joint in the image patch. Finally, we develop an algorithm to combine these joint detection/localization results from all the image patches for estimating the human pose. The experimental results show the effectiveness of the proposed method by comparing to the state-of-the-art human-pose estimation methods based on pose priors that are estimated from physiologically inspired graphical models or learned from a holistic perspective.},
	number = {arXiv:1504.07159},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Fan, Xiaochuan and Zheng, Kang and Lin, Yuewei and Wang, Song},
	month = apr,
	year = {2015},
	doi = {10.48550/arXiv.1504.07159},
	note = {arXiv:1504.07159 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2015},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/H9MGSSNE/Fan et al. - 2015 - Combining Local Appearance and Holistic View Dual.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/IMR9J7SI/1504.html:text/html},
}

@techreport{fieraru_learning_2018,
	title = {Learning to {Refine} {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1804.07909},
	abstract = {Multi-person pose estimation in images and videos is an important yet challenging task with many applications. Despite the large improvements in human pose estimation enabled by the development of convolutional neural networks, there still exist a lot of difficult cases where even the state-of-the-art models fail to correctly localize all body joints. This motivates the need for an additional refinement step that addresses these challenging cases and can be easily applied on top of any existing method. In this work, we introduce a pose refinement network (PoseRefiner) which takes as input both the image and a given pose estimate and learns to directly predict a refined pose by jointly reasoning about the input-output space. In order for the network to learn to refine incorrect body joint predictions, we employ a novel data augmentation scheme for training, where we model "hard" human pose cases. We evaluate our approach on four popular large-scale pose estimation benchmarks such as MPII Single- and Multi-Person Pose Estimation, PoseTrack Pose Estimation, and PoseTrack Pose Tracking, and report systematic improvement over the state of the art.},
	number = {arXiv:1804.07909},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Fieraru, Mihai and Khoreva, Anna and Pishchulin, Leonid and Schiele, Bernt},
	month = apr,
	year = {2018},
	doi = {10.48550/arXiv.1804.07909},
	note = {arXiv:1804.07909 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in CVPRW (2018). Workshop: Visual Understanding of Humans in Crowd Scene and the 2nd Look Into Person Challenge (VUHCS-LIP)},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/4Q3LENKJ/Fieraru et al. - 2018 - Learning to Refine Human Pose Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/DQ4LA4CI/1804.html:text/html},
}

@techreport{li_heterogeneous_2014,
	title = {Heterogeneous {Multi}-task {Learning} for {Human} {Pose} {Estimation} with {Deep} {Convolutional} {Neural} {Network}},
	url = {http://arxiv.org/abs/1406.3474},
	abstract = {We propose an heterogeneous multi-task learning framework for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a sliding-window body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.},
	number = {arXiv:1406.3474},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Li, Sijin and Liu, Zhi-Qiang and Chan, Antoni B.},
	month = jun,
	year = {2014},
	doi = {10.48550/arXiv.1406.3474},
	note = {arXiv:1406.3474 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/ABRB4YAS/Li et al. - 2014 - Heterogeneous Multi-task Learning for Human Pose E.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/TVTIB5B3/1406.html:text/html},
}

@techreport{qiu_peeking_2020,
	title = {Peeking into occluded joints: {A} novel framework for crowd pose estimation},
	shorttitle = {Peeking into occluded joints},
	url = {http://arxiv.org/abs/2003.10506},
	abstract = {Although occlusion widely exists in nature and remains a fundamental challenge for pose estimation, existing heatmap-based approaches suffer serious degradation on occlusions. Their intrinsic problem is that they directly localize the joints based on visual information; however, the invisible joints are lack of that. In contrast to localization, our framework estimates the invisible joints from an inference perspective by proposing an Image-Guided Progressive GCN module which provides a comprehensive understanding of both image context and pose structure. Moreover, existing benchmarks contain limited occlusions for evaluation. Therefore, we thoroughly pursue this problem and propose a novel OPEC-Net framework together with a new Occluded Pose (OCPose) dataset with 9k annotated images. Extensive quantitative and qualitative evaluations on benchmarks demonstrate that OPEC-Net achieves significant improvements over recent leading works. Notably, our OCPose is the most complex occlusion dataset with respect to average IoU between adjacent instances. Source code and OCPose will be publicly available.},
	number = {arXiv:2003.10506},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Qiu, Lingteng and Zhang, Xuanye and Li, Yanran and Li, Guanbin and Wu, Xiaojun and Xiong, Zixiang and Han, Xiaoguang and Cui, Shuguang},
	month = mar,
	year = {2020},
	doi = {10.48550/arXiv.2003.10506},
	note = {arXiv:2003.10506 [cs]
type: article},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, I.2.10, I.4.8},
	annote = {Comment: The code of OPEC-Net is available at: https://lingtengqiu.github.io/2020/03/22/OPEC-Net/},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/6QYWG52C/Qiu et al. - 2020 - Peeking into occluded joints A novel framework fo.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/ER5CF3XW/2003.html:text/html},
}

@techreport{sun_compositional_2017,
	title = {Compositional {Human} {Pose} {Regression}},
	url = {http://arxiv.org/abs/1704.00159},
	abstract = {Regression based methods are not performing as well as detection based methods for human pose estimation. A central problem is that the structural information in the pose is not well exploited in the previous regression methods. In this work, we propose a structure-aware regression approach. It adopts a reparameterized pose representation using bones instead of joints. It exploits the joint connection structure to define a compositional loss function that encodes the long range interactions in the pose. It is simple, effective, and general for both 2D and 3D pose estimation in a unified setting. Comprehensive evaluation validates the effectiveness of our approach. It significantly advances the state-of-the-art on Human3.6M and is competitive with state-of-the-art results on MPII.},
	number = {arXiv:1704.00159},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Sun, Xiao and Shang, Jiaxiang and Liang, Shuang and Wei, Yichen},
	month = aug,
	year = {2017},
	doi = {10.48550/arXiv.1704.00159},
	note = {arXiv:1704.00159 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by International Conference on Computer Vision (ICCV) 2017},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/AMBC5JMF/Sun et al. - 2017 - Compositional Human Pose Regression.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/FK6Y8DV3/1704.html:text/html},
}

@techreport{sun_integral_2018,
	title = {Integral {Human} {Pose} {Regression}},
	url = {http://arxiv.org/abs/1711.08229},
	abstract = {State-of-the-art human pose estimation methods are based on heat map representation. In spite of the good performance, the representation has a few issues in nature, such as not differentiable and quantization error. This work shows that a simple integral operation relates and unifies the heat map representation and joint regression, thus avoiding the above issues. It is differentiable, efficient, and compatible with any heat map based methods. Its effectiveness is convincingly validated via comprehensive ablation experiments under various settings, specifically on 3D pose estimation, for the first time.},
	number = {arXiv:1711.08229},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Sun, Xiao and Xiao, Bin and Wei, Fangyin and Liang, Shuang and Wei, Yichen},
	month = sep,
	year = {2018},
	doi = {10.48550/arXiv.1711.08229},
	note = {arXiv:1711.08229 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/7CPJ9EGV/Sun et al. - 2018 - Integral Human Pose Regression.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/3JSSCT3M/1711.html:text/html},
}

@techreport{wang_graph-pcnn_2020,
	title = {Graph-{PCNN}: {Two} {Stage} {Human} {Pose} {Estimation} with {Graph} {Pose} {Refinement}},
	shorttitle = {Graph-{PCNN}},
	url = {http://arxiv.org/abs/2007.10599},
	abstract = {Recently, most of the state-of-the-art human pose estimation methods are based on heatmap regression. The final coordinates of keypoints are obtained by decoding heatmap directly. In this paper, we aim to find a better approach to get more accurate localization results. We mainly put forward two suggestions for improvement: 1) different features and methods should be applied for rough and accurate localization, 2) relationship between keypoints should be considered. Specifically, we propose a two-stage graph-based and model-agnostic framework, called Graph-PCNN, with a localization subnet and a graph pose refinement module added onto the original heatmap regression network. In the first stage, heatmap regression network is applied to obtain a rough localization result, and a set of proposal keypoints, called guided points, are sampled. In the second stage, for each guided point, different visual feature is extracted by the localization subnet. The relationship between guided points is explored by the graph pose refinement module to get more accurate localization results. Experiments show that Graph-PCNN can be used in various backbones to boost the performance by a large margin. Without bells and whistles, our best model can achieve a new state-of-the-art 76.8\% AP on COCO test-dev split.},
	number = {arXiv:2007.10599},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wang, Jian and Long, Xiang and Gao, Yuan and Ding, Errui and Wen, Shilei},
	month = jul,
	year = {2020},
	doi = {10.48550/arXiv.2007.10599},
	note = {arXiv:2007.10599 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ECCV2020},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/ZF97WTRP/Wang et al. - 2020 - Graph-PCNN Two Stage Human Pose Estimation with G.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/YJAKPR2V/2007.html:text/html},
}

@techreport{chen_2d_2022,
	title = {{2D} {Human} {Pose} {Estimation}: {A} {Survey}},
	shorttitle = {{2D} {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2204.07370},
	abstract = {Human pose estimation aims at localizing human anatomical keypoints or body parts in the input data (e.g., images, videos, or signals). It forms a crucial component in enabling machines to have an insightful understanding of the behaviors of humans, and has become a salient problem in computer vision and related fields. Deep learning techniques allow learning feature representations directly from the data, significantly pushing the performance boundary of human pose estimation. In this paper, we reap the recent achievements of 2D human pose estimation methods and present a comprehensive survey. Briefly, existing approaches put their efforts in three directions, namely network architecture design, network training refinement, and post processing. Network architecture design looks at the architecture of human pose estimation models, extracting more robust features for keypoint recognition and localization. Network training refinement tap into the training of neural networks and aims to improve the representational ability of models. Post processing further incorporates model-agnostic polishing strategies to improve the performance of keypoint detection. More than 200 research contributions are involved in this survey, covering methodological frameworks, common benchmark datasets, evaluation metrics, and performance comparisons. We seek to provide researchers with a more comprehensive and systematic review on human pose estimation, allowing them to acquire a grand panorama and better identify future directions.},
	number = {arXiv:2204.07370},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Chen, Haoming and Feng, Runyang and Wu, Sifan and Xu, Hao and Zhou, Fengcheng and Liu, Zhenguang},
	month = apr,
	year = {2022},
	note = {arXiv:2204.07370 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/EVUCD8ZB/Chen et al. - 2022 - 2D Human Pose Estimation A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/HC8H6QJ6/2204.html:text/html},
}

@techreport{li_pose_2021,
	title = {Pose {Recognition} with {Cascade} {Transformers}},
	url = {http://arxiv.org/abs/2104.06976},
	abstract = {In this paper, we present a regression-based pose recognition method using cascade Transformers. One way to categorize the existing approaches in this domain is to separate them into 1). heatmap-based and 2). regression-based. In general, heatmap-based methods achieve higher accuracy but are subject to various heuristic designs (not end-to-end mostly), whereas regression-based approaches attain relatively lower accuracy but they have less intermediate non-differentiable steps. Here we utilize the encoder-decoder structure in Transformers to perform regression-based person and keypoint detection that is general-purpose and requires less heuristic design compared with the existing approaches. We demonstrate the keypoint hypothesis (query) refinement process across different self-attention layers to reveal the recursive self-attention mechanism in Transformers. In the experiments, we report competitive results for pose recognition when compared with the competing regression-based methods.},
	number = {arXiv:2104.06976},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Li, Ke and Wang, Shijie and Zhang, Xiang and Xu, Yifan and Xu, Weijian and Tu, Zhuowen},
	month = apr,
	year = {2021},
	doi = {10.48550/arXiv.2104.06976},
	note = {arXiv:2104.06976 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2021},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/S6S77SQ6/Li et al. - 2021 - Pose Recognition with Cascade Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/3FWJ6S33/2104.html:text/html},
}

@article{drrksrra_note_nodate,
	title = {A {Note} on {Two} {Problemsin} {Connexionwith} {Graphs}},
	language = {en},
	author = {DrrKsrRA, E W},
	pages = {3},
	file = {DrrKsrRA - A Note on Two Problemsin Connexionwith Graphs.pdf:/Users/masoudtaghikhah/Zotero/storage/ZW9IKWA8/DrrKsrRA - A Note on Two Problemsin Connexionwith Graphs.pdf:application/pdf},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2022-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/LVKRB34Q/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@techreport{szegedy_going_2014,
	title = {Going {Deeper} with {Convolutions}},
	url = {http://arxiv.org/abs/1409.4842},
	abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	number = {arXiv:1409.4842},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = sep,
	year = {2014},
	doi = {10.48550/arXiv.1409.4842},
	note = {arXiv:1409.4842 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/KL98NENL/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/46B7VKZC/1409.html:text/html},
}

@techreport{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	number = {arXiv:1706.03762},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/F32JKN3B/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/4D4DQTBB/1706.html:text/html},
}

@techreport{chen_articulated_2014,
	title = {Articulated {Pose} {Estimation} by a {Graphical} {Model} with {Image} {Dependent} {Pairwise} {Relations}},
	url = {http://arxiv.org/abs/1407.3399},
	abstract = {We present a method for estimating articulated human pose from a single static image based on a graphical model with novel pairwise relations that make adaptive use of local image measurements. More precisely, we specify a graphical model for human pose which exploits the fact the local image measurements can be used both to detect parts (or joints) and also to predict the spatial relationships between them (Image Dependent Pairwise Relations). These spatial relationships are represented by a mixture model. We use Deep Convolutional Neural Networks (DCNNs) to learn conditional probabilities for the presence of parts and their spatial relationships within image patches. Hence our model combines the representational flexibility of graphical models with the efficiency and statistical power of DCNNs. Our method significantly outperforms the state of the art methods on the LSP and FLIC datasets and also performs very well on the Buffy dataset without any training.},
	number = {arXiv:1407.3399},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Chen, Xianjie and Yuille, Alan},
	month = nov,
	year = {2014},
	doi = {10.48550/arXiv.1407.3399},
	note = {arXiv:1407.3399 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: NIPS 2014 Camera Ready},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/39ZV5FAC/Chen and Yuille - 2014 - Articulated Pose Estimation by a Graphical Model w.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/CWGFJ3YB/1407.html:text/html},
}

@techreport{tompson_joint_2014,
	title = {Joint {Training} of a {Convolutional} {Network} and a {Graphical} {Model} for {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1406.2984},
	abstract = {This paper proposes a new hybrid architecture that consists of a deep Convolutional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques.},
	number = {arXiv:1406.2984},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Tompson, Jonathan and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
	month = sep,
	year = {2014},
	doi = {10.48550/arXiv.1406.2984},
	note = {arXiv:1406.2984 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/VJZXADJY/Tompson et al. - 2014 - Joint Training of a Convolutional Network and a Gr.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/BAF2UZ9Z/1406.html:text/html},
}

@techreport{artacho_unipose_2020,
	title = {{UniPose}: {Unified} {Human} {Pose} {Estimation} in {Single} {Images} and {Videos}},
	shorttitle = {{UniPose}},
	url = {http://arxiv.org/abs/2001.08095},
	abstract = {We propose UniPose, a unified framework for human pose estimation, based on our "Waterfall" Atrous Spatial Pooling architecture, that achieves state-of-art-results on several pose estimation metrics. Current pose estimation methods utilizing standard CNN architectures heavily rely on statistical postprocessing or predefined anchor poses for joint localization. UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage, with high accuracy, without relying on statistical postprocessing methods. The Waterfall module in UniPose leverages the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Additionally, our method is extended to UniPose-LSTM for multi-frame processing and achieves state-of-the-art results for temporal pose estimation in Video. Our results on multiple datasets demonstrate that UniPose, with a ResNet backbone and Waterfall module, is a robust and efficient architecture for pose estimation obtaining state-of-the-art results in single person pose detection for both single images and videos.},
	number = {arXiv:2001.08095},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Artacho, Bruno and Savakis, Andreas},
	month = jan,
	year = {2020},
	doi = {10.48550/arXiv.2001.08095},
	note = {arXiv:2001.08095 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/29C5IXI4/Artacho and Savakis - 2020 - UniPose Unified Human Pose Estimation in Single I.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/RKNIZBF8/2001.html:text/html},
}

@incollection{bulat_human_2016,
	title = {Human pose estimation via {Convolutional} {Part} {Heatmap} {Regression}},
	volume = {9911},
	url = {http://arxiv.org/abs/1609.01743},
	abstract = {This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from http://www.cs.nott.ac.uk/{\textasciitilde}psxab5/},
	urldate = {2022-06-09},
	author = {Bulat, Adrian and Tzimiropoulos, Georgios},
	year = {2016},
	doi = {10.1007/978-3-319-46478-7_44},
	note = {arXiv:1609.01743 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {717--732},
	annote = {Comment: accepted to ECCV 2016},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/9PAICWEW/Bulat and Tzimiropoulos - 2016 - Human pose estimation via Convolutional Part Heatm.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/SF9IW7E7/1609.html:text/html},
}

@techreport{zheng_deep_2022,
	title = {Deep {Learning}-{Based} {Human} {Pose} {Estimation}: {A} {Survey}},
	shorttitle = {Deep {Learning}-{Based} {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/2012.13392},
	abstract = {Human pose estimation aims to locate the human body parts and build human body representation (e.g., body skeleton) from input data such as images and videos. It has drawn increasing attention during the past decade and has been utilized in a wide range of applications including human-computer interaction, motion analysis, augmented reality, and virtual reality. Although the recently developed deep learning-based solutions have achieved high performance in human pose estimation, there still remain challenges due to insufficient training data, depth ambiguities, and occlusion. The goal of this survey paper is to provide a comprehensive review of recent deep learning-based solutions for both 2D and 3D pose estimation via a systematic analysis and comparison of these solutions based on their input data and inference procedures. More than 250 research papers since 2014 are covered in this survey. Furthermore, 2D and 3D human pose estimation datasets and evaluation metrics are included. Quantitative performance comparisons of the reviewed methods on popular datasets are summarized and discussed. Finally, the challenges involved, applications, and future research directions are concluded. A regularly updated project page is provided: {\textbackslash}url\{https://github.com/zczcwh/DL-HPE\}},
	number = {arXiv:2012.13392},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Zheng, Ce and Wu, Wenhan and Chen, Chen and Yang, Taojiannan and Zhu, Sijie and Shen, Ju and Kehtarnavaz, Nasser and Shah, Mubarak},
	month = jan,
	year = {2022},
	note = {arXiv:2012.13392 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Multimedia},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/ZM832ANT/Zheng et al. - 2022 - Deep Learning-Based Human Pose Estimation A Surve.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/SKPQMWJ6/2012.html:text/html},
}

@techreport{gkioxari_chained_2016,
	title = {Chained {Predictions} {Using} {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1605.02346},
	abstract = {In this paper, we present an adaptation of the sequence-to-sequence model for structured output prediction in vision tasks. In this model the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each time step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted in different steps. We show that chained predictions achieve top performing results on human pose estimation from single images and videos.},
	number = {arXiv:1605.02346},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Gkioxari, Georgia and Toshev, Alexander and Jaitly, Navdeep},
	month = oct,
	year = {2016},
	doi = {10.48550/arXiv.1605.02346},
	note = {arXiv:1605.02346 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: in submission to EECV 2016},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/Q4S29Z8V/Gkioxari et al. - 2016 - Chained Predictions Using Convolutional Neural Net.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/G7VPRV45/1605.html:text/html},
}

@techreport{lifshitz_human_2016,
	title = {Human {Pose} {Estimation} using {Deep} {Consensus} {Voting}},
	url = {http://arxiv.org/abs/1603.08212},
	abstract = {In this paper we consider the problem of human pose estimation from a single still image. We propose a novel approach where each location in the image votes for the position of each keypoint using a convolutional neural net. The voting scheme allows us to utilize information from the whole image, rather than rely on a sparse set of keypoint locations. Using dense, multi-target votes, not only produces good keypoint predictions, but also enables us to compute image-dependent joint keypoint probabilities by looking at consensus voting. This differs from most previous methods where joint probabilities are learned from relative keypoint locations and are independent of the image. We finally combine the keypoints votes and joint probabilities in order to identify the optimal pose configuration. We show our competitive performance on the MPII Human Pose and Leeds Sports Pose datasets.},
	number = {arXiv:1603.08212},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Lifshitz, Ita and Fetaya, Ethan and Ullman, Shimon},
	month = mar,
	year = {2016},
	doi = {10.48550/arXiv.1603.08212},
	note = {arXiv:1603.08212 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/G7DPBNAU/Lifshitz et al. - 2016 - Human Pose Estimation using Deep Consensus Voting.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/XYV44Z6U/1603.html:text/html},
}

@techreport{tompson_efficient_2015,
	title = {Efficient {Object} {Localization} {Using} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1411.4280},
	abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.},
	number = {arXiv:1411.4280},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christopher},
	month = jun,
	year = {2015},
	doi = {10.48550/arXiv.1411.4280},
	note = {arXiv:1411.4280 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 8 pages with 1 page of citations},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/6TRNPFYR/Tompson et al. - 2015 - Efficient Object Localization Using Convolutional .pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/WQF85DXW/1411.html:text/html},
}

@inproceedings{toshev_deeppose_2014,
	title = {{DeepPose}: {Human} {Pose} {Estimation} via {Deep} {Neural} {Networks}},
	shorttitle = {{DeepPose}},
	url = {http://arxiv.org/abs/1312.4659},
	doi = {10.1109/CVPR.2014.214},
	abstract = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-of-art or better performance on four academic benchmarks of diverse real-world images.},
	urldate = {2022-06-09},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Toshev, Alexander and Szegedy, Christian},
	month = jun,
	year = {2014},
	note = {arXiv:1312.4659 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1653--1660},
	annote = {Comment: IEEE Conference on Computer Vision and Pattern Recognition, 2014},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/E2KS2PYP/Toshev and Szegedy - 2014 - DeepPose Human Pose Estimation via Deep Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/APF25BZY/1312.html:text/html},
}

@techreport{chen_cascaded_2018,
	title = {Cascaded {Pyramid} {Network} for {Multi}-{Person} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1711.07319},
	abstract = {The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these "hard" keypoints. More specifically, our algorithm includes two stages: GlobalNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the "simple" keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the "hard" keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-of-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19\% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge.Code (https://github.com/chenyilun95/tf-cpn.git) and the detection results are publicly available for further research.},
	number = {arXiv:1711.07319},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Chen, Yilun and Wang, Zhicheng and Peng, Yuxiang and Zhang, Zhiqiang and Yu, Gang and Sun, Jian},
	month = apr,
	year = {2018},
	doi = {10.48550/arXiv.1711.07319},
	note = {arXiv:1711.07319 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages, accepted to CVPR 2018},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/KDQN59NW/Chen et al. - 2018 - Cascaded Pyramid Network for Multi-Person Pose Est.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/63QT42FG/1711.html:text/html},
}

@article{poppe_vision-based_2007,
	title = {Vision-based human motion analysis: {An} overview},
	volume = {108},
	shorttitle = {Vision-based human motion analysis},
	doi = {10.1016/j.cviu.2006.10.016},
	abstract = {Abstract Markerless vision-based human motion analysis has the potential to provide an inexpensive, non-obtrusive solution for the estimation of body poses. The significant research effort in this domain has been motivated by the fact that many application areas, including sur- veillance, Human–Computer Interaction and automatic annotation, will benefit from a robust solution. In this paper, we discuss the characteristics of human,motion,analysis. We divide the analysis into a modeling,and an estimation phase. Modeling is the construction of the likelihood function, estimation is concerned with finding the most likely pose given the likelihood surface. We discuss model-free approaches,separately. This taxonomy,allows us to highlight trends in the domain,and to point out limitations of the current state of the art. 2007 Elsevier Inc. All rights reserved. Keywords: Human,motion,analysis; Pose estimation; Computer,vision},
	journal = {Computer Vision and Image Understanding},
	author = {Poppe, Ronald},
	month = oct,
	year = {2007},
	pages = {4--18},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Complexity theory, Degradation, Image recognition, Image segmentation, Neural networks, Training, Visualization},
	pages = {770--778},
	file = {IEEE Xplore Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/BNEE6WQM/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@techreport{ke_multi-scale_2018,
	title = {Multi-{Scale} {Structure}-{Aware} {Network} for {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1803.09894},
	abstract = {We develop a robust multi-scale structure-aware neural network for human pose estimation. This method improves the recent deep conv-deconv hourglass models with four key improvements: (1) multi-scale supervision to strengthen contextual feature learning in matching body keypoints by combining feature heatmaps across scales, (2) multi-scale regression network at the end to globally optimize the structural matching of the multi-scale features, (3) structure-aware loss used in the intermediate supervision and at the regression to improve the matching of keypoints and respective neighbors to infer a higher-order matching configurations, and (4) a keypoint masking training scheme that can effectively fine-tune our network to robustly localize occluded keypoints via adjacent matches. Our method can effectively improve state-of-the-art pose estimation methods that suffer from difficulties in scale varieties, occlusions, and complex multi-person scenarios. This multi-scale supervision tightly integrates with the regression network to effectively (i) localize keypoints using the ensemble of multi-scale features, and (ii) infer global pose configuration by maximizing structural consistencies across multiple keypoints and scales. The keypoint masking training enhances these advantages to focus learning on hard occlusion samples. Our method achieves the leading position in the MPII challenge leaderboard among the state-of-the-art methods.},
	number = {arXiv:1803.09894},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Ke, Lipeng and Chang, Ming-Ching and Qi, Honggang and Lyu, Siwei},
	month = sep,
	year = {2018},
	doi = {10.48550/arXiv.1803.09894},
	note = {arXiv:1803.09894 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by ECCV2018},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/BAGQ2U8X/Ke et al. - 2018 - Multi-Scale Structure-Aware Network for Human Pose.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/6BG2JVIM/1803.html:text/html},
}

@article{liu_cascaded_2018,
	title = {A {Cascaded} {Inception} of {Inception} {Network} {With} {Attention} {Modulated} {Feature} {Fusion} for {Human} {Pose} {Estimation}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12334},
	doi = {10.1609/aaai.v32i1.12334},
	abstract = {Accurate keypoint localization of human pose needs diversified features: the high level for contextual dependencies and the low level for detailed refinement of joints. However, the importance of the two factors varies from case to case, but how to efficiently use the features is still an open problem. Existing methods have limitations in preserving low level features, adaptively adjusting the importance of different levels of features, and modeling the human perception process. This paper presents three novel techniques step by step to efficiently utilize different levels of features for human pose estimation. Firstly, an inception of inception (IOI) block is designed to emphasize the low level features. Secondly, an attention mechanism is proposed to adjust the importance of individual levels according to the context. Thirdly, a cascaded network is proposed to sequentially localize the joints to enforce message passing from joints of stand-alone parts like head and torso to remote joints like wrist or ankle. Experimental results demonstrate that the proposed method achieves the state-of-the-art performance on both MPII and LSP benchmarks.},
	language = {en},
	number = {1},
	urldate = {2022-06-09},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Liu, Wentao and Chen, Jie and Li, Cheng and Qian, Chen and Chu, Xiao and Hu, Xiaolin},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Cascade Joint Network},
	file = {Full Text PDF:/Users/masoudtaghikhah/Zotero/storage/RSXXVM44/Liu et al. - 2018 - A Cascaded Inception of Inception Network With Att.pdf:application/pdf},
}

@techreport{su_multi-person_2019,
	title = {Multi-{Person} {Pose} {Estimation} with {Enhanced} {Channel}-wise and {Spatial} {Information}},
	url = {http://arxiv.org/abs/1905.03466},
	abstract = {Multi-person pose estimation is an important but challenging problem in computer vision. Although current approaches have achieved significant progress by fusing the multi-scale feature maps, they pay little attention to enhancing the channel-wise and spatial information of the feature maps. In this paper, we propose two novel modules to perform the enhancement of the information for the multi-person pose estimation. First, a Channel Shuffle Module (CSM) is proposed to adopt the channel shuffle operation on the feature maps with different levels, promoting cross-channel information communication among the pyramid feature maps. Second, a Spatial, Channel-wise Attention Residual Bottleneck (SCARB) is designed to boost the original residual unit with attention mechanism, adaptively highlighting the information of the feature maps both in the spatial and channel-wise context. The effectiveness of our proposed modules is evaluated on the COCO keypoint benchmark, and experimental results show that our approach achieves the state-of-the-art results.},
	number = {arXiv:1905.03466},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Su, Kai and Yu, Dongdong and Xu, Zhenqi and Geng, Xin and Wang, Changhu},
	month = may,
	year = {2019},
	doi = {10.48550/arXiv.1905.03466},
	note = {arXiv:1905.03466 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR 2019},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/V8N678LS/Su et al. - 2019 - Multi-Person Pose Estimation with Enhanced Channel.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/SKUL9QTX/1905.html:text/html},
}

@inproceedings{sun_deep_2019,
	address = {Long Beach, CA, USA},
	title = {Deep {High}-{Resolution} {Representation} {Learning} for {Human} {Pose} {Estimation}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953615/},
	doi = {10.1109/CVPR.2019.00584},
	abstract = {In this paper, we are interested in the human pose estimation problem with a focus on learning reliable highresolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
	month = jun,
	year = {2019},
	pages = {5686--5696},
	file = {Sun et al. - 2019 - Deep High-Resolution Representation Learning for H.pdf:/Users/masoudtaghikhah/Zotero/storage/ZTVNHD96/Sun et al. - 2019 - Deep High-Resolution Representation Learning for H.pdf:application/pdf},
}

@incollection{ferrari_deeply_2018,
	address = {Cham},
	title = {Deeply {Learned} {Compositional} {Models} for {Human} {Pose} {Estimation}},
	volume = {11207},
	isbn = {978-3-030-01218-2 978-3-030-01219-9},
	url = {http://link.springer.com/10.1007/978-3-030-01219-9_12},
	abstract = {Compositional models represent patterns with hierarchies of meaningful parts and subparts. Their ability to characterize high-order relationships among body parts helps resolve low-level ambiguities in human pose estimation (HPE). However, prior compositional models make unrealistic assumptions on subpart-part relationships, making them incapable to characterize complex compositional patterns. Moreover, state spaces of their higher-level parts can be exponentially large, complicating both inference and learning. To address these issues, this paper introduces a novel framework, termed as Deeply Learned Compositional Model (DLCM), for HPE. It exploits deep neural networks to learn the compositionality of human bodies. This results in a novel network with a hierarchical compositional architecture and bottom-up/top-down inference stages. In addition, we propose a novel bone-based part representation. It not only compactly encodes orientations, scales and shapes of parts, but also avoids their potentially large state spaces. With signiﬁcantly lower complexities, our approach outperforms state-of-the-art methods on three benchmark datasets.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Tang, Wei and Yu, Pei and Wu, Ying},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01219-9_12},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {197--214},
	file = {Tang et al. - 2018 - Deeply Learned Compositional Models for Human Pose.pdf:/Users/masoudtaghikhah/Zotero/storage/FGRUK2XB/Tang et al. - 2018 - Deeply Learned Compositional Models for Human Pose.pdf:application/pdf},
}

@techreport{xiao_simple_2018-1,
	title = {Simple {Baselines} for {Human} {Pose} {Estimation} and {Tracking}},
	url = {http://arxiv.org/abs/1804.06208},
	abstract = {There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github.com/leoxiaobin/pose.pytorch.},
	number = {arXiv:1804.06208},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Xiao, Bin and Wu, Haiping and Wei, Yichen},
	month = aug,
	year = {2018},
	doi = {10.48550/arXiv.1804.06208},
	note = {arXiv:1804.06208 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by ECCV 2018},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/Y8W6ZERV/Xiao et al. - 2018 - Simple Baselines for Human Pose Estimation and Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/WJWF42WQ/1804.html:text/html},
}

@techreport{yang_learning_2017,
	title = {Learning {Feature} {Pyramids} for {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1708.01101},
	abstract = {Articulated human pose estimation is a fundamental yet challenging task in computer vision. The difficulty is particularly pronounced in scale variations of human body parts when camera view changes or severe foreshortening happens. Although pyramid methods are widely used to handle scale changes at inference time, learning feature pyramids in deep convolutional neural networks (DCNNs) is still not well explored. In this work, we design a Pyramid Residual Module (PRMs) to enhance the invariance in scales of DCNNs. Given input features, the PRMs learn convolutional filters on various scales of input features, which are obtained with different subsampling ratios in a multi-branch network. Moreover, we observe that it is inappropriate to adopt existing methods to initialize the weights of multi-branch networks, which achieve superior performance than plain networks in many tasks recently. Therefore, we provide theoretic derivation to extend the current weight initialization scheme to multi-branch network structures. We investigate our method on two standard benchmarks for human pose estimation. Our approach obtains state-of-the-art results on both benchmarks. Code is available at https://github.com/bearpaw/PyraNet.},
	number = {arXiv:1708.01101},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Yang, Wei and Li, Shuang and Ouyang, Wanli and Li, Hongsheng and Wang, Xiaogang},
	month = aug,
	year = {2017},
	doi = {10.48550/arXiv.1708.01101},
	note = {arXiv:1708.01101 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Submitted to ICCV 2017},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/MUSBKBX2/Yang et al. - 2017 - Learning Feature Pyramids for Human Pose Estimatio.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/HQLD234A/1708.html:text/html},
}

@techreport{chu_multi-context_2017,
	title = {Multi-{Context} {Attention} for {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1702.07432},
	abstract = {In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on the detailed description for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semantic-consistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive fields, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts.},
	number = {arXiv:1702.07432},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Chu, Xiao and Yang, Wei and Ouyang, Wanli and Ma, Cheng and Yuille, Alan L. and Wang, Xiaogang},
	month = feb,
	year = {2017},
	doi = {10.48550/arXiv.1702.07432},
	note = {arXiv:1702.07432 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: The first two authors contribute equally to this work},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/2GLZEQQJ/Chu et al. - 2017 - Multi-Context Attention for Human Pose Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/W3MD8PSJ/1702.html:text/html},
}

@article{eichner_human_2012,
	title = {Human {Pose} {Co}-{Estimation} and {Applications}},
	doi = {10.1109/TPAMI.2012.85},
	abstract = {It is shown that PCE improves pose estimation accuracy over estimating each person independently, and it learns meaningful prototypes which can be used as priors for pose estimation in novel images. Most existing techniques for articulated Human Pose Estimation (HPE) consider each person independently. Here we tackle the problem in a new setting, coined Human Pose Coestimation (PCE), where multiple people are in a common, but unknown pose. The task of PCE is to estimate their poses jointly and to produce prototypes characterizing the shared pose. Since the poses of the individual people should be similar to the prototype, PCE has less freedom compared to estimating each pose independently, which simplifies the problem. We demonstrate our PCE technique on two applications. The first is estimating the pose of people performing the same activity synchronously, such as during aerobics, cheerleading, and dancing in a group. We show that PCE improves pose estimation accuracy over estimating each person independently. The second application is learning prototype poses characterizing a pose class directly from an image search engine queried by the class name (e.g., “lotus pose”). We show that PCE leads to better pose estimation in such images, and it learns meaningful prototypes which can be used as priors for pose estimation in novel images.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Eichner, M. and Ferrari, V.},
	year = {2012},
	file = {Accepted Version:/Users/masoudtaghikhah/Zotero/storage/9QDAR8RM/Eichner and Ferrari - 2012 - Human Pose Co-Estimation and Applications.pdf:application/pdf},
}

@techreport{girdhar_detect-and-track_2018,
	title = {Detect-and-{Track}: {Efficient} {Pose} {Estimation} in {Videos}},
	shorttitle = {Detect-and-{Track}},
	url = {http://arxiv.org/abs/1712.09184},
	abstract = {This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection and video understanding. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2\% on the validation and 51.8\% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge.},
	number = {arXiv:1712.09184},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Girdhar, Rohit and Gkioxari, Georgia and Torresani, Lorenzo and Paluri, Manohar and Tran, Du},
	month = may,
	year = {2018},
	doi = {10.48550/arXiv.1712.09184},
	note = {arXiv:1712.09184 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: In CVPR 2018. Ranked first in ICCV 2017 PoseTrack challenge (keypoint tracking in videos). Code: https://github.com/facebookresearch/DetectAndTrack and webpage: https://rohitgirdhar.github.io/DetectAndTrack/},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/TU2J77TL/Girdhar et al. - 2018 - Detect-and-Track Efficient Pose Estimation in Vid.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/ELGC5HJ6/1712.html:text/html},
}

@article{felzenszwalb_pictorial_2005,
	title = {Pictorial {Structures} for {Object} {Recognition}},
	volume = {61},
	issn = {0920-5691},
	url = {http://link.springer.com/10.1023/B:VISI.0000042934.15159.49},
	doi = {10.1023/B:VISI.0000042934.15159.49},
	abstract = {In this paper we present a computationally eﬃcient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable conﬁguration. The appearance of each part is modeled separately, and the deformable conﬁguration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to ﬁnd instances of an object in an image as well as the problem of learning an object model from training examples, presenting eﬃcient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.},
	language = {en},
	number = {1},
	urldate = {2022-06-09},
	journal = {International Journal of Computer Vision},
	author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	month = jan,
	year = {2005},
	pages = {55--79},
	file = {Felzenszwalb and Huttenlocher - 2005 - Pictorial Structures for Object Recognition.pdf:/Users/masoudtaghikhah/Zotero/storage/ZZ4LG9P3/Felzenszwalb and Huttenlocher - 2005 - Pictorial Structures for Object Recognition.pdf:application/pdf},
}

@inproceedings{yixing_gao_user_2015,
	address = {Hamburg, Germany},
	title = {User modelling for personalised dressing assistance by humanoid robots},
	isbn = {978-1-4799-9994-1},
	url = {http://ieeexplore.ieee.org/document/7353617/},
	doi = {10.1109/IROS.2015.7353617},
	abstract = {Assistive robots can improve the well-being of disabled or frail human users by reducing the burden that activities of daily living impose on them. To enable personalised assistance, such robots beneﬁt from building a user-speciﬁc model, so that the assistance is customised to the particular set of user abilities. In this paper, we present an end-to-end approach for home-environment assistive humanoid robots to provide personalised assistance through a dressing application for users who have upper-body movement limitations. We use randomised decision forests to estimate the upper-body pose of users captured by a top-view depth camera, and model the movement space of upper-body joints using Gaussian mixture models. The movement space of each upper-body joint consists of regions with different reaching capabilities. We propose a method which is based on real-time upper-body pose and user models to plan robot motions for assistive dressing. We validate each part of our approach and test the whole system, allowing a Baxter humanoid robot to assist human to wear a sleeveless jacket.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {{Yixing Gao} and {Hyung Jin Chang} and Demiris, Yiannis},
	month = sep,
	year = {2015},
	pages = {1840--1845},
	file = {Yixing Gao et al. - 2015 - User modelling for personalised dressing assistanc.pdf:/Users/masoudtaghikhah/Zotero/storage/K77WIS5W/Yixing Gao et al. - 2015 - User modelling for personalised dressing assistanc.pdf:application/pdf},
}

@techreport{pfister_flowing_2015,
	title = {Flowing {ConvNets} for {Human} {Pose} {Estimation} in {Videos}},
	url = {http://arxiv.org/abs/1506.02897},
	abstract = {The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps; (ii) spatial fusion layers that learn an implicit spatial model; (iii) optical flow is used to align heatmap predictions from neighbouring frames; and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also Chen \& Yuille and Tompson et al. in the high precision region).},
	number = {arXiv:1506.02897},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Pfister, Tomas and Charles, James and Zisserman, Andrew},
	month = nov,
	year = {2015},
	doi = {10.48550/arXiv.1506.02897},
	note = {arXiv:1506.02897 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV'15},
	file = {arXiv Fulltext PDF:/Users/masoudtaghikhah/Zotero/storage/X7LIS623/Pfister et al. - 2015 - Flowing ConvNets for Human Pose Estimation in Vide.pdf:application/pdf;arXiv.org Snapshot:/Users/masoudtaghikhah/Zotero/storage/ZG9DPA8B/1506.html:text/html},
}
